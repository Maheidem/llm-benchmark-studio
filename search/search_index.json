{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLM Benchmark Studio","text":"<p>Measure token throughput, latency, and tool calling accuracy across LLM providers -- all in one place.</p> <p>LLM Benchmark Studio is a multi-user SaaS platform for evaluating Large Language Models. It combines speed benchmarking (tokens/sec, time to first token) with a tool calling evaluation framework, parameter tuning, prompt tuning, and LLM-as-judge -- giving you a complete picture of model performance.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#speed-benchmarking","title":"Speed Benchmarking","text":"<ul> <li>Real-time streaming measurements of tokens per second and time to first token (TTFT)</li> <li>Multi-provider parallel execution with per-provider sequential model runs</li> <li>Context window scaling tests across configurable token tiers (1K to 150K+)</li> <li>Cost tracking with LiteLLM pricing and custom per-model pricing</li> <li>Statistical analysis with std dev, min/max, p50/p95, IQR outlier detection</li> </ul>"},{"location":"#tool-calling-evaluation","title":"Tool Calling Evaluation","text":"<ul> <li>Define tool suites using OpenAI function calling JSON schema</li> <li>Build test cases with expected tools and parameters</li> <li>Score tool selection accuracy and parameter correctness (exact, fuzzy, contains, semantic)</li> <li>Multi-turn chain evaluation with mock tool responses</li> <li>Import tools directly from MCP servers</li> </ul>"},{"location":"#llm-as-judge","title":"LLM-as-Judge","text":"<ul> <li>Use one model to evaluate another model's tool calling quality</li> <li>Live inline scoring during eval runs or post-eval batch analysis</li> <li>Comparative judging between two eval runs</li> <li>Cross-case analysis with overall grades and recommendations</li> </ul>"},{"location":"#parameter-tuner","title":"Parameter Tuner","text":"<ul> <li>Grid search across temperature, top_p, top_k, and other parameters</li> <li>Provider-aware validation and clamping for 10 providers</li> <li>Per-model search spaces with automatic deduplication</li> <li>Search space presets (save, load, delete)</li> <li>Built-in vendor presets for specific model families</li> </ul>"},{"location":"#prompt-tuner","title":"Prompt Tuner","text":"<ul> <li>Quick mode (single generation) and evolutionary mode (multi-generation with selection)</li> <li>Meta-model generates prompt variations, target models evaluate them</li> <li>Automatic best-result promotion to experiments</li> </ul>"},{"location":"#experiments","title":"Experiments","text":"<ul> <li>Group related eval, param tune, prompt tune, and judge runs</li> <li>Timeline view of all runs within an experiment</li> <li>Automatic baseline and best-score tracking</li> </ul>"},{"location":"#analytics-and-history","title":"Analytics and History","text":"<ul> <li>Benchmark history browser with search and filtering</li> <li>Leaderboard rankings by speed, cost, and quality</li> <li>Trend analysis over configurable time periods</li> <li>CSV and JSON export/import for all data</li> </ul>"},{"location":"#scheduling","title":"Scheduling","text":"<ul> <li>Automated recurring benchmark runs</li> <li>Configurable intervals and model selection</li> <li>Results saved to history with schedule metadata</li> </ul>"},{"location":"#multi-user-platform","title":"Multi-User Platform","text":"<ul> <li>JWT authentication with 24-hour access tokens and 7-day refresh tokens</li> <li>Per-user API key management with Fernet encryption</li> <li>Per-user configuration (providers, models, prompts)</li> <li>Admin dashboard with user management, audit logs, and rate limits</li> <li>Real-time WebSocket updates for all job types (multi-tab support)</li> <li>Centralized job management with per-user concurrency limits and queuing</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation -- Get up and running</li> <li>Quick Start -- Run your first benchmark</li> <li>API Reference -- Full REST API documentation</li> <li>Docker Setup -- Self-host with Docker</li> <li>Architecture -- How it all works</li> </ul>"},{"location":"#technology-stack","title":"Technology Stack","text":"Component Technology Backend Python 3.13, FastAPI, Uvicorn Database SQLite with aiosqlite (WAL mode), 16 tables LLM Integration LiteLLM (unified API for all providers) Frontend Vue 3, Vite, Pinia, Vue Router, Tailwind CSS Real-time WebSocket (ConnectionManager, multi-tab, auto-reconnect) Job Management JobRegistry (asyncio tasks, per-user concurrency, queuing) Authentication JWT (python-jose), bcrypt, refresh tokens Encryption Fernet (cryptography) for API key storage Validation Pydantic v2 for all request/response schemas CLI Rich for terminal output Testing pytest + pytest-asyncio, ~405 tests, ~4s runtime Containerization Docker with multi-stage builds (Node + Python) CI/CD GitHub Actions, GHCR, Portainer CE"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>LLM Benchmark Studio is a multi-user SaaS platform built with a FastAPI backend serving a Vue 3 single-page application. The backend uses a modular router architecture with centralized job management and WebSocket-based real-time updates.</p> <pre><code>                    Vue 3 SPA (Vite + Pinia + Tailwind CSS)\n                         |\n                    HTTP + WebSocket\n                         |\n                  FastAPI (app.py orchestrator)\n                   /     |      \\        \\\n            routers/  auth.py  db.py  job_registry.py\n          (20 modules)   |       |         |\n               |      JWT/bcrypt SQLite  ws_manager.py\n          job_handlers.py        |         |\n               |         data/benchmark_studio.db\n           LiteLLM              (16 tables)\n               |\n         LLM Providers (10+ supported)\n</code></pre>"},{"location":"architecture/#core-files","title":"Core Files","text":"File Purpose Lines <code>app.py</code> FastAPI orchestrator -- lifespan, middleware, logging, router registration ~580 <code>benchmark.py</code> Core benchmark engine -- Target, RunResult, run_single, CLI ~980 <code>db.py</code> Database layer -- schema (16 tables), CRUD, DatabaseManager ~1660 <code>auth.py</code> Authentication -- JWT (24h access, 7d refresh), bcrypt, login rate limiting ~390 <code>keyvault.py</code> Fernet encryption for API key storage ~66 <code>provider_params.py</code> 3-tier parameter registry (10 providers), validation, clamping ~720 <code>job_registry.py</code> JobRegistry singleton -- background jobs, queuing, concurrency limits ~400 <code>job_handlers.py</code> Handler functions for 6 job types (benchmark, tool_eval, param_tune, prompt_tune, judge, judge_compare) ~1890 <code>ws_manager.py</code> WebSocket ConnectionManager -- per-user connections, multi-tab, admin broadcast ~90 <code>schemas.py</code> Pydantic request/response models for all API endpoints ~216 <code>config.yaml</code> Default provider/model configuration ~130 <code>routers/</code> 20 FastAPI router modules (6,480 lines total) see below <code>frontend/src/</code> Vue 3 SPA -- 8 stores, 7 composables, 50+ components see below <code>tests/</code> 20 test files, ~6,700 lines, ~405 tests see below"},{"location":"architecture/#router-modules","title":"Router Modules","text":"Router Endpoints Lines <code>helpers.py</code> Shared utilities (scoring, target selection, rate limiting) ~1080 <code>tool_eval.py</code> <code>/api/tool-eval/*</code> -- eval execution, run management ~1046 <code>judge.py</code> <code>/api/judge/*</code> -- LLM-as-judge, comparative analysis ~460 <code>experiments.py</code> <code>/api/experiments/*</code> -- experiment CRUD, timeline ~420 <code>prompt_tune.py</code> <code>/api/prompt-tune/*</code> -- prompt tuning orchestration ~387 <code>export_import.py</code> <code>/api/export/*</code>, <code>/api/import/*</code> -- data portability ~357 <code>discovery.py</code> <code>/api/discovery/*</code> -- model discovery, LM Studio detection ~330 <code>config.py</code> <code>/api/config/*</code> -- provider/model CRUD, custom pricing ~328 <code>admin.py</code> <code>/api/admin/*</code> -- users, audit log, rate limits, logs ~319 <code>mcp.py</code> <code>/api/mcp/*</code> -- MCP server tool import ~250 <code>analytics.py</code> <code>/api/analytics/*</code> -- leaderboard, trends, comparison ~227 <code>websocket.py</code> <code>/ws</code> -- WebSocket connection lifecycle ~199 <code>benchmark.py</code> <code>/api/benchmark</code> -- benchmark submission ~195 <code>jobs.py</code> <code>/api/jobs/*</code> -- job listing, cancellation ~173 <code>param_tune.py</code> <code>/api/param-tune/*</code> -- parameter tuning submission ~160 <code>schedules.py</code> <code>/api/schedules/*</code> -- scheduled benchmark CRUD ~149 <code>env.py</code> <code>/api/env</code> -- environment variable management ~130 <code>keys.py</code> <code>/api/keys/*</code> -- API key vault management ~93 <code>settings.py</code> <code>/api/settings/*</code> -- per-user settings storage ~75 <code>auth.py</code> <code>/api/auth/*</code> -- login, register, refresh, logout ~30 <code>onboarding.py</code> <code>/api/onboarding/*</code> -- onboarding completion ~27"},{"location":"architecture/#frontend-architecture","title":"Frontend Architecture","text":"Directory Contents <code>stores/</code> 8 Pinia stores: <code>auth</code>, <code>benchmark</code>, <code>config</code>, <code>judge</code>, <code>notifications</code>, <code>paramTuner</code>, <code>promptTuner</code>, <code>toolEval</code> <code>composables/</code> 7 composables: <code>useWebSocket</code>, <code>useToast</code>, <code>useModal</code>, <code>useChartTheme</code>, <code>useProviderColors</code>, <code>useActiveSession</code>, <code>useSharedContext</code> <code>views/</code> Page components: <code>BenchmarkPage</code>, <code>ToolEvalPage</code>, <code>AnalyticsPage</code>, <code>HistoryPage</code>, <code>SchedulesPage</code>, <code>SettingsPage</code>, <code>AdminPage</code>, <code>LandingPage</code> <code>components/</code> 40+ reusable components organized by domain: <code>benchmark/</code>, <code>tool-eval/</code>, <code>analytics/</code>, <code>admin/</code>, <code>auth/</code>, <code>layout/</code>, <code>schedules/</code>, <code>settings/</code>, <code>ui/</code> <code>router/</code> Vue Router with history mode, route guards for auth"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#benchmark-execution","title":"Benchmark Execution","text":"<pre><code>User selects models + params in Vue SPA\n        |\n        v\nPOST /api/benchmark (validated by Pydantic BenchmarkRequest)\n        |\n        v\nrouters/benchmark.py: validate, build params\n        |\n        v\njob_registry.submit(\"benchmark\", user_id, params)\n        |\n        v\nJobRegistry checks per-user concurrency limit\n        |\n  [under limit]                [at limit]\n        |                          |\n   Start immediately          Queue (status: \"queued\")\n   (status: \"running\")        Wait for slot to open\n        |\n        v\njob_handlers.benchmark_handler()\n        |\n        v\nBuild targets from user config, inject per-user API keys\n        |\n        v\nGroup targets by provider\n        |\n        v\nasyncio.create_task() per provider group (parallel)\n    |\n    |-- Provider A: model1 -&gt; model2 -&gt; model3 (sequential)\n    |-- Provider B: model4 -&gt; model5 (sequential)\n    |-- Provider C: model6 (sequential)\n    |\n    v\nResults flow through asyncio.Queue\n        |\n        v\nWebSocket broadcast to all user tabs (interleaved results)\n        |\n        v\nResults saved to DB (benchmark_runs) + JSON file\n</code></pre>"},{"location":"architecture/#tool-eval-execution","title":"Tool Eval Execution","text":"<pre><code>User selects suite + models in Vue SPA\n        |\n        v\nPOST /api/tool-eval (validated by Pydantic ToolEvalRequest)\n        |\n        v\njob_registry.submit(\"tool_eval\", user_id, params)\n        |\n        v\njob_handlers.tool_eval_handler()\n        |\n        v\nLoad suite tools + test cases from DB\n        |\n        v\nGroup targets by provider (parallel across providers)\n    |\n    |-- For each model: run all test cases (sequential)\n    |   |\n    |   |-- litellm.acompletion() with tools (non-streaming)\n    |   |-- Extract tool_calls from response\n    |   |-- Score: tool_selection + param_accuracy -&gt; overall\n    |\n    v\nResults via asyncio.Queue -&gt; WebSocket broadcast\n        |\n        v\nPer-model summaries computed\n        |\n        v\nOptional: Judge evaluation (live inline or post-eval)\n        |\n        v\nSaved to tool_eval_runs table\n        |\n        v\nIf experiment_id: auto-update baseline/best score\n</code></pre>"},{"location":"architecture/#websocket-event-flow","title":"WebSocket Event Flow","text":"<pre><code>Vue SPA                                    FastAPI\n  |                                          |\n  |--- WS connect (/ws?token=JWT) ---------&gt;|\n  |                                          |--- Validate JWT\n  |                                          |--- ConnectionManager.connect()\n  |&lt;-- { type: \"connected\" } ---------------|\n  |                                          |\n  |--- Submit benchmark via REST ----------&gt;|\n  |                                          |--- JobRegistry.submit()\n  |&lt;-- { type: \"job_created\" } -------------|\n  |&lt;-- { type: \"job_started\" } -------------|\n  |&lt;-- { type: \"benchmark_init\" } ---------|\n  |&lt;-- { type: \"benchmark_progress\" } -----|    (repeated per model/run)\n  |&lt;-- { type: \"benchmark_result\" } -------|\n  |&lt;-- { type: \"job_progress\" } ------------|    (% complete)\n  |&lt;-- { type: \"job_completed\" } ----------|\n  |                                          |\n  |--- ping ------&gt;|                         |\n  |&lt;-- pong -------|                         |\n</code></pre>"},{"location":"architecture/#job-types","title":"Job Types","text":"Type Handler WebSocket Events <code>benchmark</code> <code>benchmark_handler</code> <code>benchmark_init</code>, <code>benchmark_progress</code>, <code>benchmark_result</code> <code>tool_eval</code> <code>tool_eval_handler</code> <code>tool_eval_init</code>, <code>tool_eval_progress</code>, <code>tool_eval_result</code>, <code>tool_eval_summary</code>, <code>tool_eval_complete</code> <code>param_tune</code> <code>param_tune_handler</code> <code>tune_start</code>, <code>combo_result</code>, <code>tune_complete</code> <code>prompt_tune</code> <code>prompt_tune_handler</code> <code>tune_start</code>, <code>generation_start</code>, <code>prompt_generated</code>, <code>prompt_eval_start</code>, <code>prompt_eval_result</code>, <code>generation_complete</code>, <code>tune_complete</code> <code>judge</code> <code>judge_handler</code> <code>judge_start</code>, <code>judge_verdict</code>, <code>judge_report</code>, <code>judge_complete</code> <code>judge_compare</code> <code>judge_compare_handler</code> <code>compare_start</code>, <code>compare_case</code>, <code>compare_complete</code>"},{"location":"architecture/#concurrency-model","title":"Concurrency Model","text":""},{"location":"architecture/#job-registry","title":"Job Registry","text":"<ul> <li>Per-user concurrency limits: Configurable via <code>rate_limits</code> table (default: 1 concurrent job)</li> <li>Queuing: Excess jobs queued with FIFO ordering; auto-started when slots free up</li> <li>State machine: <code>pending -&gt; queued -&gt; running -&gt; done/failed/cancelled/interrupted</code></li> <li>Validated transitions: Invalid state transitions are logged but don't block execution</li> <li>Timeout watchdog: Background task checks every 60s for timed-out jobs (default: 2h)</li> <li>Startup recovery: On server restart, all running/pending/queued jobs marked as <code>interrupted</code></li> <li>Cancellation: Via <code>asyncio.Event</code> -- handlers check <code>cancel_event.is_set()</code> between operations</li> </ul>"},{"location":"architecture/#websocket-manager","title":"WebSocket Manager","text":"<ul> <li>Per-user connections: Up to 5 concurrent WebSocket connections per user (multi-tab)</li> <li>Message delivery: Broadcast to ALL tabs for a user via <code>send_to_user()</code></li> <li>Admin broadcast: <code>broadcast_to_admins()</code> for system-wide notifications</li> <li>Dead connection cleanup: Automatic removal on send failure</li> <li>Connection limit: Rejects connections beyond limit with code 4008</li> </ul>"},{"location":"architecture/#benchmark-execution_1","title":"Benchmark Execution","text":"<ul> <li>Cross-provider: Fully parallel via <code>asyncio.create_task()</code> per provider group</li> <li>Within provider: Sequential model execution (avoids API self-contention)</li> <li>Result collection: <code>asyncio.Queue</code> for non-blocking result aggregation</li> <li>Cancellation: <code>cancel_event</code> checked between runs; tasks cancelled on cancel</li> </ul>"},{"location":"architecture/#background-scheduler","title":"Background Scheduler","text":"<ul> <li>Single background task (<code>_run_scheduler</code>) checks for due schedules every 60 seconds</li> <li>Scheduled benchmarks run sequentially, one at a time</li> <li>Uses per-user API keys and configuration</li> </ul>"},{"location":"architecture/#authentication","title":"Authentication","text":"<pre><code>Register -&gt; bcrypt hash -&gt; store in users table\n         -&gt; issue access token (24h JWT)\n         -&gt; issue refresh token (7-day JWT, HttpOnly cookie)\n         -&gt; first user auto-promoted to admin\n\nLogin -&gt; verify bcrypt hash\n      -&gt; check login rate limiter (5 attempts / 5 min / IP, 15 min lockout)\n      -&gt; issue new tokens\n      -&gt; audit log entry\n\nAPI Request -&gt; extract Bearer token from Authorization header\n            -&gt; decode JWT (access or cli type)\n            -&gt; load user from DB\n            -&gt; inject into route handler via Depends()\n\nToken Refresh -&gt; validate refresh token from HttpOnly cookie\n              -&gt; verify token exists in DB (not revoked)\n              -&gt; issue new access token (refresh token stays the same)\n\nAdmin routes -&gt; get_current_user + check role == \"admin\"\n\nADMIN_EMAIL env var -&gt; auto-promote matching user on startup\n</code></pre>"},{"location":"architecture/#database","title":"Database","text":"<p>SQLite with aiosqlite for async access and WAL mode for concurrent reads.</p>"},{"location":"architecture/#16-tables","title":"16 Tables","text":"<pre><code>users\n  |-- refresh_tokens (CASCADE)\n  |-- user_api_keys (CASCADE)\n  |-- user_configs (CASCADE)\n  |-- benchmark_runs (CASCADE)\n  |-- rate_limits (CASCADE)\n  |-- audit_log (user_id nullable, survives user deletion)\n  |-- tool_suites (CASCADE)\n  |     |-- tool_test_cases (CASCADE)\n  |     |-- experiments (CASCADE)\n  |-- tool_eval_runs (CASCADE, FK to tool_suites + experiments)\n  |-- param_tune_runs (CASCADE, FK to tool_suites + experiments)\n  |-- prompt_tune_runs (CASCADE, FK to tool_suites + experiments)\n  |-- judge_reports (FK to tool_eval_runs + experiments)\n  |-- schedules\n  |-- jobs (CASCADE)\n</code></pre> Table Purpose <code>users</code> User accounts with email, bcrypt hash, role (user/admin) <code>refresh_tokens</code> Hashed refresh tokens with expiry <code>user_api_keys</code> Fernet-encrypted API keys per user per provider <code>user_configs</code> Per-user YAML config (providers, models) <code>benchmark_runs</code> Benchmark results with prompt, tiers, results JSON <code>rate_limits</code> Per-user rate limits (benchmarks/hour, max concurrent, max runs) <code>audit_log</code> All significant user actions with timestamps, IPs <code>tool_suites</code> Tool definitions (OpenAI function calling schema) <code>tool_test_cases</code> Test cases with expected tool/params, scoring config <code>tool_eval_runs</code> Eval results with per-model summaries <code>param_tune_runs</code> Parameter tuning results with search space and best config <code>prompt_tune_runs</code> Prompt tuning results with generations and best prompt <code>judge_reports</code> Judge verdicts and cross-case analysis <code>experiments</code> Experiment metadata with baseline/best tracking <code>schedules</code> Recurring benchmark schedules <code>jobs</code> Universal job tracker (7 types, 7 statuses)"},{"location":"architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>DatabaseManager singleton: Centralized connection management with <code>fetch_one</code>, <code>fetch_all</code>, <code>execute</code> methods -- eliminates repeated connect/row_factory/commit patterns</li> <li>Per-user config in DB: Each user's provider/model configuration stored as YAML in <code>user_configs</code></li> <li>Fernet encryption: User API keys encrypted at rest, decrypted only when needed for LLM calls</li> <li>Audit log preservation: Audit entries survive user deletion (user_id nullable)</li> <li>UUID primary keys: All tables use randomly generated hex IDs</li> <li>Schema migrations: <code>try/except</code> with <code>ALTER TABLE</code> for backward-compatible additions</li> </ul>"},{"location":"architecture/#litellm-integration","title":"LiteLLM Integration","text":"<p>All LLM calls go through LiteLLM, which provides a unified API across providers:</p> <ul> <li>Benchmarks: <code>litellm.completion()</code> with <code>stream=True</code>, 120s timeout, 0 retries</li> <li>Tool evals: <code>litellm.acompletion()</code> (non-streaming, to capture tool_calls)</li> <li>Model IDs: Follow LiteLLM conventions with provider prefixes (<code>anthropic/</code>, <code>gemini/</code>, etc.)</li> <li>API keys: Passed per-call via the <code>api_key</code> parameter (decrypted from vault)</li> </ul>"},{"location":"architecture/#provider-parameter-handling","title":"Provider Parameter Handling","text":"<p>The <code>provider_params.py</code> module implements a three-tier parameter system:</p> <ol> <li>Tier 1 (Universal): <code>temperature</code>, <code>max_tokens</code>, <code>stop</code> -- supported by all providers</li> <li>Tier 2 (Common): <code>top_p</code>, <code>top_k</code>, <code>frequency_penalty</code>, <code>presence_penalty</code>, <code>seed</code>, <code>reasoning_effort</code> -- with provider-specific ranges, support flags, and conflict rules</li> <li>Tier 3 (Passthrough): Provider-specific parameters that bypass validation (e.g., <code>repetition_penalty</code>, <code>min_p</code>, <code>mirostat</code>)</li> </ol> <p>Parameters are validated, clamped to valid ranges, and conflict-resolved before being passed to LiteLLM. The philosophy is \"warn, don't drop\" -- user-requested params pass through unless there is a hard conflict (e.g., Anthropic's mutual exclusion of temperature + top_p).</p>"},{"location":"architecture/#security","title":"Security","text":"<ul> <li>Content Security Policy: Restricts script/style/font/connect sources</li> <li>CORS: Disabled by default, configurable via <code>CORS_ORIGINS</code> env var</li> <li>Error sanitization: API keys, Bearer tokens, and sensitive patterns stripped from error messages</li> <li>Non-root Docker: Application runs as dedicated <code>bench</code> user</li> <li>HttpOnly cookies: Refresh tokens stored in HttpOnly, SameSite=Strict cookies</li> <li>Rate limiting: Per-user benchmark rate limits + login attempt rate limiting with lockout</li> <li>Security headers: X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy</li> <li>Request logging: JSON-formatted structured logs with request IDs, user IDs, durations</li> <li>In-memory log buffer: Ring buffer (2000 entries) accessible via admin API endpoint</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/maheidem/llm-benchmark-studio.git\ncd llm-benchmark-studio\n</code></pre> </li> <li> <p>Install backend dependencies:</p> <pre><code>uv sync\n</code></pre> </li> <li> <p>Install frontend dependencies:</p> <pre><code>cd frontend &amp;&amp; npm install &amp;&amp; cd ..\n</code></pre> </li> <li> <p>Set up environment:</p> <pre><code>cp .env.example .env\n# Edit .env with at least one provider API key\n</code></pre> </li> <li> <p>Run the application:</p> <pre><code># Backend (serves the built frontend from static/)\npython app.py\n\n# Frontend dev server (hot reload, proxies API to backend)\ncd frontend &amp;&amp; npm run dev\n</code></pre> </li> </ol>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>llm-benchmark-studio/\n\u251c\u2500\u2500 app.py                  # FastAPI orchestrator (lifespan, middleware, logging)\n\u251c\u2500\u2500 benchmark.py            # Core benchmark engine (Target, RunResult, CLI)\n\u251c\u2500\u2500 auth.py                 # JWT + bcrypt authentication\n\u251c\u2500\u2500 db.py                   # SQLite database layer (16 tables)\n\u251c\u2500\u2500 keyvault.py             # Fernet encryption for API keys\n\u251c\u2500\u2500 provider_params.py      # Provider parameter registry (10 providers)\n\u251c\u2500\u2500 job_registry.py         # JobRegistry singleton (background jobs, queuing)\n\u251c\u2500\u2500 job_handlers.py         # Handler functions for 6 job types\n\u251c\u2500\u2500 ws_manager.py           # WebSocket ConnectionManager\n\u251c\u2500\u2500 schemas.py              # Pydantic request/response models\n\u251c\u2500\u2500 config.yaml             # Default provider/model configuration\n\u251c\u2500\u2500 routers/                # 20 FastAPI router modules\n\u2502   \u251c\u2500\u2500 __init__.py         # Router registration (all_routers list)\n\u2502   \u251c\u2500\u2500 helpers.py          # Shared utilities (scoring, target selection)\n\u2502   \u251c\u2500\u2500 auth.py             # /api/auth/* endpoints\n\u2502   \u251c\u2500\u2500 benchmark.py        # /api/benchmark\n\u2502   \u251c\u2500\u2500 tool_eval.py        # /api/tool-eval/*\n\u2502   \u251c\u2500\u2500 param_tune.py       # /api/param-tune/*\n\u2502   \u251c\u2500\u2500 prompt_tune.py      # /api/prompt-tune/*\n\u2502   \u251c\u2500\u2500 judge.py            # /api/judge/*\n\u2502   \u251c\u2500\u2500 experiments.py      # /api/experiments/*\n\u2502   \u251c\u2500\u2500 analytics.py        # /api/analytics/*\n\u2502   \u251c\u2500\u2500 config.py           # /api/config/*\n\u2502   \u251c\u2500\u2500 admin.py            # /api/admin/*\n\u2502   \u251c\u2500\u2500 jobs.py             # /api/jobs/*\n\u2502   \u251c\u2500\u2500 schedules.py        # /api/schedules/*\n\u2502   \u251c\u2500\u2500 keys.py             # /api/keys/*\n\u2502   \u251c\u2500\u2500 settings.py         # /api/settings/*\n\u2502   \u251c\u2500\u2500 discovery.py        # /api/discovery/*\n\u2502   \u251c\u2500\u2500 mcp.py              # /api/mcp/*\n\u2502   \u251c\u2500\u2500 export_import.py    # /api/export/*, /api/import/*\n\u2502   \u251c\u2500\u2500 websocket.py        # /ws\n\u2502   \u251c\u2500\u2500 env.py              # /api/env\n\u2502   \u2514\u2500\u2500 onboarding.py       # /api/onboarding/*\n\u251c\u2500\u2500 frontend/               # Vue 3 SPA\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u251c\u2500\u2500 vite.config.js\n\u2502   \u2514\u2500\u2500 src/\n\u2502       \u251c\u2500\u2500 App.vue\n\u2502       \u251c\u2500\u2500 main.js\n\u2502       \u251c\u2500\u2500 router/         # Vue Router (history mode)\n\u2502       \u2502   \u2514\u2500\u2500 index.js\n\u2502       \u251c\u2500\u2500 stores/         # 8 Pinia stores\n\u2502       \u2502   \u251c\u2500\u2500 auth.js\n\u2502       \u2502   \u251c\u2500\u2500 benchmark.js\n\u2502       \u2502   \u251c\u2500\u2500 config.js\n\u2502       \u2502   \u251c\u2500\u2500 judge.js\n\u2502       \u2502   \u251c\u2500\u2500 notifications.js\n\u2502       \u2502   \u251c\u2500\u2500 paramTuner.js\n\u2502       \u2502   \u251c\u2500\u2500 promptTuner.js\n\u2502       \u2502   \u2514\u2500\u2500 toolEval.js\n\u2502       \u251c\u2500\u2500 composables/    # 7 composables\n\u2502       \u2502   \u251c\u2500\u2500 useWebSocket.js\n\u2502       \u2502   \u251c\u2500\u2500 useToast.js\n\u2502       \u2502   \u251c\u2500\u2500 useModal.js\n\u2502       \u2502   \u251c\u2500\u2500 useChartTheme.js\n\u2502       \u2502   \u251c\u2500\u2500 useProviderColors.js\n\u2502       \u2502   \u251c\u2500\u2500 useActiveSession.js\n\u2502       \u2502   \u2514\u2500\u2500 useSharedContext.js\n\u2502       \u251c\u2500\u2500 views/          # Page components\n\u2502       \u251c\u2500\u2500 components/     # Reusable UI components (40+)\n\u2502       \u251c\u2500\u2500 utils/          # Utility functions\n\u2502       \u2514\u2500\u2500 assets/         # CSS and static assets\n\u251c\u2500\u2500 tests/                  # 20 test files, ~6,700 lines\n\u2502   \u251c\u2500\u2500 conftest.py         # Shared fixtures (test DB, auth tokens)\n\u2502   \u251c\u2500\u2500 test_api_contracts.py\n\u2502   \u251c\u2500\u2500 test_scoring.py\n\u2502   \u251c\u2500\u2500 test_provider_params.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 Dockerfile              # Multi-stage build (Node frontend + Python backend)\n\u251c\u2500\u2500 docker-compose.yml      # Docker Compose for local development\n\u251c\u2500\u2500 pyproject.toml          # Python dependencies (uv)\n\u251c\u2500\u2500 uv.lock                 # Lockfile\n\u251c\u2500\u2500 .env.example            # Example environment variables\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 ci.yml          # CI/CD pipeline (test -&gt; build -&gt; deploy)\n\u251c\u2500\u2500 docs/                   # MkDocs documentation (this site)\n\u251c\u2500\u2500 data/                   # SQLite database and Fernet key (gitignored)\n\u2514\u2500\u2500 results/                # Benchmark result JSON files (gitignored)\n</code></pre>"},{"location":"contributing/#code-conventions","title":"Code Conventions","text":""},{"location":"contributing/#backend-python","title":"Backend (Python)","text":"<ul> <li>Async-first: All route handlers and database operations use <code>async/await</code></li> <li>Framework: FastAPI with modular routers in <code>routers/</code></li> <li>Database: <code>aiosqlite</code> with WAL mode, raw SQL, <code>DatabaseManager</code> singleton</li> <li>Auth: Dependency injection via <code>Depends(auth.get_current_user)</code> and <code>Depends(auth.require_admin)</code></li> <li>Validation: Pydantic v2 models in <code>schemas.py</code> for all request/response schemas</li> <li>Error handling: Return <code>JSONResponse</code> with appropriate HTTP status codes</li> <li>Job execution: Long-running operations go through <code>job_registry.submit()</code>, not direct execution</li> <li>WebSocket events: Follow pattern <code>{\"type\": \"event_type\", \"job_id\": \"...\", \"data\": {...}}</code></li> <li>Naming: <code>snake_case</code> for functions and variables, private helpers prefixed with <code>_</code></li> <li>Logging: Structured JSON logging via <code>logging</code> stdlib, no third-party log libraries</li> </ul>"},{"location":"contributing/#frontend-vue-3","title":"Frontend (Vue 3)","text":"<ul> <li>Framework: Vue 3 with Composition API (<code>&lt;script setup&gt;</code>)</li> <li>State management: Pinia stores (one per domain)</li> <li>Routing: Vue Router with history mode</li> <li>Styling: Tailwind CSS utility classes</li> <li>Real-time: <code>useWebSocket</code> composable for all WebSocket communication</li> <li>Notifications: <code>useToast</code> composable for user feedback</li> <li>API calls: <code>fetch()</code> with JWT Authorization header (managed by auth store)</li> <li>Components: Domain-organized directories under <code>components/</code></li> <li>Charts: Chart.js integrated via <code>useChartTheme</code> composable</li> </ul>"},{"location":"contributing/#configuration","title":"Configuration","text":"<ul> <li>YAML: Provider and model definitions (<code>config.yaml</code>, per-user <code>user_configs</code>)</li> <li>JSON: Database storage for results, tool definitions, search spaces</li> <li>Environment: Secrets and deployment settings via <code>.env</code></li> </ul>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#adding-a-new-router","title":"Adding a New Router","text":"<ol> <li>Create a new file in <code>routers/</code> (e.g., <code>routers/my_feature.py</code>)</li> <li>Define an <code>APIRouter</code> with a prefix: <code>router = APIRouter(prefix=\"/api/my-feature\", tags=[\"my-feature\"])</code></li> <li>Use <code>Depends(auth.get_current_user)</code> for authenticated routes</li> <li>Import and register in <code>routers/__init__.py</code> (add to <code>all_routers</code> list)</li> <li>Add Pydantic request/response models to <code>schemas.py</code> if needed</li> </ol>"},{"location":"contributing/#adding-a-new-job-type","title":"Adding a New Job Type","text":"<ol> <li>Add the handler function in <code>job_handlers.py</code> following the signature:     <pre><code>async def my_handler(job_id, params, cancel_event, progress_cb) -&gt; str | None:\n</code></pre></li> <li>Register it in <code>register_all_handlers()</code> at the bottom of <code>job_handlers.py</code></li> <li>Add the job type to the <code>jobs</code> table CHECK constraint in <code>db.py</code></li> <li>Create a router endpoint that calls <code>job_registry.submit(\"my_type\", user_id, params)</code></li> </ol>"},{"location":"contributing/#backend-changes","title":"Backend Changes","text":"<ol> <li>API routes go in the appropriate router module under <code>routers/</code></li> <li>Follow existing patterns:<ul> <li>Use <code>Depends(auth.get_current_user)</code> for authenticated routes</li> <li>Use <code>Depends(auth.require_admin)</code> for admin-only routes</li> <li>Return <code>JSONResponse</code> for errors with appropriate status codes</li> <li>Log significant actions to the audit log via <code>db.log_audit()</code></li> </ul> </li> <li>Database schema changes go in <code>db.py</code> in the <code>init_db()</code> function</li> <li>Use <code>try/except</code> with <code>ALTER TABLE</code> for backward-compatible schema migrations</li> <li>Add Pydantic models to <code>schemas.py</code> for request validation</li> </ol>"},{"location":"contributing/#frontend-changes","title":"Frontend Changes","text":"<ol> <li>Page components go in <code>frontend/src/views/</code></li> <li>Reusable components go in <code>frontend/src/components/&lt;domain&gt;/</code></li> <li>State management goes in <code>frontend/src/stores/</code></li> <li>Follow existing patterns:<ul> <li>Use Composition API with <code>&lt;script setup&gt;</code></li> <li>Use Tailwind CSS for styling</li> <li>Test in dark mode (the default theme)</li> <li>WebSocket event handlers should match backend event types</li> </ul> </li> <li>Build for production: <code>cd frontend &amp;&amp; npm run build</code> (outputs to <code>static/</code>)</li> </ol>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code># Run all tests (~405 tests, ~4 seconds)\nuv run pytest\n\n# Run a specific test file\nuv run pytest tests/test_scoring.py -v\n\n# Run tests matching a name pattern\nuv run pytest -k \"test_benchmark\" -v\n\n# Run with verbose output\nuv run pytest -v\n</code></pre> <p>Test conventions:</p> <ul> <li>Framework: <code>pytest</code> with <code>pytest-asyncio</code> for async tests</li> <li>HTTP testing: <code>httpx.ASGITransport</code> with FastAPI <code>TestClient</code></li> <li>No external API calls: All LiteLLM calls are mocked</li> <li>Fixtures in <code>conftest.py</code>: test database, auth tokens, sample data</li> <li>Test categories: unit (pure functions), API contracts, integration, E2E smoke</li> <li>E2E smoke tests (<code>test_e2e_smoke.py</code>) require a real API key and run only in CI on <code>main</code></li> </ul>"},{"location":"contributing/#validation-before-submitting","title":"Validation Before Submitting","text":"<pre><code># Run the test suite\nuv run pytest\n\n# Verify the app starts\npython app.py\n\n# Verify the frontend builds\ncd frontend &amp;&amp; npm run build\n\n# Verify Docker build\ndocker compose up --build\n\n# Test the health endpoint\ncurl http://localhost:8501/healthz\n</code></pre>"},{"location":"contributing/#cicd","title":"CI/CD","text":"<ul> <li>Push to <code>main</code> triggers: test -&gt; build -&gt; smoke test -&gt; auto-deploy to staging</li> <li>Create a version tag (<code>v1.2.0</code>) for production deploy</li> <li>PRs trigger: test -&gt; build -&gt; smoke test (no deploy)</li> <li>Registry: <code>ghcr.io/maheidem/llm-benchmark-studio</code></li> <li>Deploy method: Portainer CE REST API (pull image -&gt; stop stack -&gt; remove containers -&gt; start stack)</li> </ul>"},{"location":"feature-specs-v1.4/","title":"LLM Benchmark Studio -- Feature Specifications v1.4","text":"<p>Date: 2026-02-16 Status: Complete Target Version: v1.4.0 Current Version: v1.3.0 (Multi-Turn Tool Eval)</p>"},{"location":"feature-specs-v1.4/#executive-summary","title":"Executive Summary","text":"<p>This document specifies four new features for LLM Benchmark Studio that extend the Tool Eval system with automated optimization, AI-powered quality assessment, and intelligent per-provider parameter handling. All four features are designed for parallel development on separate <code>feat/</code> branches using git worktrees.</p>"},{"location":"feature-specs-v1.4/#the-four-features","title":"The Four Features","text":"# Feature Purpose Effort Est. 1 Parameter Tuner GridSearchCV-style deterministic sweep of parameter combinations for optimal tool calling config 22.5h 2 Prompt Tuner AI-generated system prompt variations with Quick and Evolutionary (genetic algorithm) modes 30h 3 LLM Judge AI evaluator layer with post-eval reports, live inline scoring, and comparative model judging 34.5h 4 Provider Parameters Per-provider parameter UI with 3-tier architecture, clamping, conflict resolution, JSON passthrough 35h <p>Total estimated effort: ~122 hours across backend + frontend + QA</p>"},{"location":"feature-specs-v1.4/#individual-spec-documents","title":"Individual Spec Documents","text":"<p>Each feature has a complete specification in <code>.scratchpad/handoffs/</code>:</p> <ol> <li>Parameter Tuner: <code>architect-parameter-tuner-spec-SUCCESS.md</code></li> <li>Prompt Tuner: <code>architect-prompt-tuner-spec-SUCCESS.md</code></li> <li>LLM Judge: <code>architect-llm-judge-spec-SUCCESS.md</code></li> <li>Provider Parameters: <code>architect-provider-params-spec-SUCCESS.md</code></li> </ol> <p>Supporting research: - LiteLLM Research: <code>research-litellm-findings-SUCCESS.md</code> - Codebase Patterns: <code>librarian-codebase-patterns-SUCCESS.md</code></p> <p>Each spec contains: Feature Description, User Stories, Acceptance Criteria, DoR, DoD, API Design, Database Schema, UI Wireframe Description, Task Breakdown, and Test Plan.</p>"},{"location":"feature-specs-v1.4/#cross-feature-dependencies","title":"Cross-Feature Dependencies","text":""},{"location":"feature-specs-v1.4/#shared-infrastructure","title":"Shared Infrastructure","text":"<p>All four features share common infrastructure that should be built once:</p> <ol> <li> <p>User Lock (<code>_get_user_lock</code>): Parameter Tuner, Prompt Tuner, and LLM Judge all acquire the existing per-user lock. Only one eval-family operation runs per user at a time.</p> </li> <li> <p>SSE Streaming Pattern: All features use the same <code>POST + ReadableStream</code> SSE pattern as existing tool eval (not EventSource). Each feature adds new SSE event types.</p> </li> <li> <p>Cancellation Pattern: All features reuse <code>_get_user_cancel()</code> / <code>asyncio.Event</code>. Cancel endpoints follow the same pattern.</p> </li> <li> <p>DB Patterns: All new tables follow: UUID hex PKs, user_id FK with CASCADE, datetime defaults, JSON columns for structured data, indexes on (user_id, timestamp DESC).</p> </li> <li> <p>Frontend Patterns: All features follow <code>te</code> naming convention (prefix per feature: <code>pt</code> for Parameter Tuner, <code>prt</code> for Prompt Tuner, <code>jg</code> for Judge, <code>pp</code> for Provider Params). Sub-views within the Tool Eval tab using <code>showToolEvalView()</code> pattern.</p> </li> </ol>"},{"location":"feature-specs-v1.4/#dependency-graph","title":"Dependency Graph","text":"<pre><code>Provider Parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                          \u2502\nParameter Tuner \u2500\u2500 uses \u2500\u2192 run_single_eval() \u2190\u2500\u2500 uses \u2500\u2500 \u2502\n                                                          \u2502\nPrompt Tuner \u2500\u2500\u2500\u2500\u2500 uses \u2500\u2192 run_single_eval() \u2190\u2500\u2500 uses \u2500\u2500 \u2502\n                   uses \u2500\u2192 litellm.acompletion() (meta)   \u2502\n                                                          \u2502\nLLM Judge \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 uses \u2500\u2192 litellm.acompletion() (judge)  \u2502\n                   reads \u2192 tool_eval_runs (existing)       \u2502\n</code></pre> <p>Key dependencies: - Provider Parameters affects all other features. When Provider Params is complete, Parameter Tuner and Prompt Tuner should respect provider param ranges in their search spaces. - Parameter Tuner is self-contained. Wraps <code>run_single_eval()</code> with parameter sweeps. No dependency on other new features. - Prompt Tuner is self-contained. Similar to Parameter Tuner but sweeps prompts instead of params. - LLM Judge integrates with existing eval runs. Post-eval and comparative modes read from <code>tool_eval_runs</code>. Live inline mode integrates into the eval SSE stream.</p>"},{"location":"feature-specs-v1.4/#recommended-implementation-order","title":"Recommended implementation order:","text":"<ol> <li>Provider Parameters (foundation -- affects parameter handling for everything)</li> <li>Parameter Tuner (simplest new feature, establishes tuning patterns)</li> <li>Prompt Tuner (builds on tuning patterns, adds AI generation)</li> <li>LLM Judge (most complex integration, touches eval SSE stream)</li> </ol> <p>However, since all features go on separate branches, they CAN be developed in parallel. Provider Parameters is the only one that affects how the others handle parameters.</p>"},{"location":"feature-specs-v1.4/#new-database-tables-summary","title":"New Database Tables Summary","text":"Table Feature Key Columns <code>param_tune_runs</code> Parameter Tuner user_id, suite_id, search_space_json, results_json, best_config_json, status <code>prompt_tune_runs</code> Prompt Tuner user_id, suite_id, mode, generations_json, best_prompt, best_score, status <code>judge_reports</code> LLM Judge user_id, eval_run_id, judge_model, mode, verdicts_json, report_json, overall_grade <p>Provider Parameters adds NO new tables (uses existing <code>user_configs</code> + static Python module).</p>"},{"location":"feature-specs-v1.4/#new-api-endpoints-summary","title":"New API Endpoints Summary","text":""},{"location":"feature-specs-v1.4/#parameter-tuner","title":"Parameter Tuner","text":"Method Path Purpose POST <code>/api/tool-eval/param-tune</code> Start tuning run (SSE) POST <code>/api/tool-eval/param-tune/cancel</code> Cancel run GET <code>/api/tool-eval/param-tune/history</code> List runs GET <code>/api/tool-eval/param-tune/history/{id}</code> Get full run DELETE <code>/api/tool-eval/param-tune/history/{id}</code> Delete run"},{"location":"feature-specs-v1.4/#prompt-tuner","title":"Prompt Tuner","text":"Method Path Purpose POST <code>/api/tool-eval/prompt-tune</code> Start tuning run (SSE) POST <code>/api/tool-eval/prompt-tune/cancel</code> Cancel run GET <code>/api/tool-eval/prompt-tune/estimate</code> Get cost estimate GET <code>/api/tool-eval/prompt-tune/history</code> List runs GET <code>/api/tool-eval/prompt-tune/history/{id}</code> Get full run DELETE <code>/api/tool-eval/prompt-tune/history/{id}</code> Delete run"},{"location":"feature-specs-v1.4/#llm-judge","title":"LLM Judge","text":"Method Path Purpose POST <code>/api/tool-eval/judge</code> Post-eval judge (SSE) POST <code>/api/tool-eval/judge/compare</code> Comparative judge (SSE) GET <code>/api/tool-eval/judge/reports</code> List reports GET <code>/api/tool-eval/judge/reports/{id}</code> Get full report DELETE <code>/api/tool-eval/judge/reports/{id}</code> Delete report"},{"location":"feature-specs-v1.4/#provider-parameters","title":"Provider Parameters","text":"Method Path Purpose GET <code>/api/provider-params/registry</code> Get full param registry POST <code>/api/provider-params/validate</code> Validate params for provider"},{"location":"feature-specs-v1.4/#new-sse-event-types-summary","title":"New SSE Event Types Summary","text":""},{"location":"feature-specs-v1.4/#parameter-tuner_1","title":"Parameter Tuner","text":"<p><code>tune_start</code>, <code>combo_start</code>, <code>combo_result</code>, <code>tune_progress</code>, <code>tune_complete</code></p>"},{"location":"feature-specs-v1.4/#prompt-tuner_1","title":"Prompt Tuner","text":"<p><code>tune_start</code>, <code>generation_start</code>, <code>prompt_generated</code>, <code>prompt_eval_start</code>, <code>prompt_eval_result</code>, <code>generation_complete</code>, <code>tune_complete</code></p>"},{"location":"feature-specs-v1.4/#llm-judge_1","title":"LLM Judge","text":"<p><code>judge_start</code>, <code>judge_verdict</code>, <code>judge_report</code>, <code>judge_complete</code>, <code>compare_start</code>, <code>compare_case</code>, <code>compare_complete</code></p>"},{"location":"feature-specs-v1.4/#new-files","title":"New Files","text":"File Purpose <code>provider_params.py</code> Provider parameter registry, validation, clamping, conflict resolution <p>All other code goes into existing files (<code>app.py</code>, <code>db.py</code>, <code>index.html</code>).</p>"},{"location":"feature-specs-v1.4/#test-strategy","title":"Test Strategy","text":"<ul> <li>Test Provider: Zai (GLM-4.7, GLM-5, GLM-4.5-Air) -- free tokens</li> <li>Environment: Local Docker</li> <li>Quality Gate: All features must pass browser testing with screen captures before going to staging</li> <li>Bundle Release: Ships as v1.4.0 when all features pass QA</li> </ul>"},{"location":"feature-specs-v1.4/#risk-assessment","title":"Risk Assessment","text":"Risk Mitigation AI-generated prompts may be low quality (Prompt Tuner) Test meta-prompts manually before integration. Include retry + fallback parsing. Judge model self-evaluation bias Warn when judge model = target model. Default to using a different model. Provider parameter registry becomes stale Tie registry updates to LiteLLM version bumps. Add version field to registry. Large parameter sweeps overwhelm rate limits Grid preview warns at &gt;50 combos. Each combo counts as 1 rate-limit event. SSE event types proliferate Prefix all new events per feature. Document event schemas."},{"location":"feature-specs-v1.4/#open-questions","title":"Open Questions","text":"<ol> <li>Should Parameter Tuner results feed into Provider Parameters defaults? e.g., \"Apply best config\" sets per-provider defaults automatically.</li> <li>Should Prompt Tuner support multi-model evolution? Currently each target model is scored independently. Could have \"best prompt across all models.\"</li> <li>Should Judge reports be shareable? (Ties into future sharing/public suites roadmap item)</li> <li>Should we add a \"Run All\" mode? Parameter Tuner + Prompt Tuner + Judge in one pipeline.</li> </ol>"},{"location":"admin/settings/","title":"System Settings","text":""},{"location":"admin/settings/#global-environment-keys","title":"Global Environment Keys","text":"<p>Admins can manage global API keys that serve as fallbacks for all users. These are stored in the <code>.env</code> file on the server.</p> <pre><code># List all environment keys (values are masked)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/env\n\n# Set or update a key\ncurl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"OPENAI_API_KEY\", \"value\": \"sk-...\"}' \\\n  http://localhost:8501/api/env\n\n# Remove a key\ncurl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"OPENAI_API_KEY\"}' \\\n  http://localhost:8501/api/env\n</code></pre> <p>Key Priority</p> <p>Per-user encrypted keys (set via <code>/api/keys</code>) take priority over global environment keys. Global keys serve as fallbacks for users who have not set their own.</p>"},{"location":"admin/settings/#api-key-encryption","title":"API Key Encryption","text":"<p>User API keys are encrypted using Fernet symmetric encryption before storage:</p> <ul> <li>The master key is resolved from (in order):<ol> <li><code>FERNET_KEY</code> environment variable</li> <li><code>data/.fernet_key</code> file (auto-generated on first run)</li> </ol> </li> <li>Auto-generated keys are written with <code>0600</code> permissions (owner-only)</li> </ul> <p>Back Up Your Fernet Key</p> <p>If the Fernet key is lost, all stored user API keys become unrecoverable. In production, set <code>FERNET_KEY</code> as an environment variable and back it up securely.</p>"},{"location":"admin/settings/#security-headers","title":"Security Headers","text":"<p>The application adds security headers to all responses:</p> Header Value Content-Security-Policy Restricts script/style/font/image sources X-Content-Type-Options <code>nosniff</code> X-Frame-Options <code>DENY</code> Referrer-Policy <code>strict-origin-when-cross-origin</code> Permissions-Policy Disables camera, microphone, geolocation, payment"},{"location":"admin/settings/#cors-configuration","title":"CORS Configuration","text":"<p>CORS is disabled by default. To enable it, set <code>CORS_ORIGINS</code>:</p> <pre><code>CORS_ORIGINS=https://example.com,https://staging.example.com\n</code></pre> <p>When set, the following CORS configuration is applied:</p> <ul> <li>Allowed origins: from the <code>CORS_ORIGINS</code> list</li> <li>Credentials: allowed</li> <li>Methods: GET, POST, PUT, DELETE</li> <li>Headers: Authorization, Content-Type</li> </ul>"},{"location":"admin/settings/#login-rate-limiting","title":"Login Rate Limiting","text":"<p>The login endpoint is protected by IP-based rate limiting:</p> Setting Value Max attempts per window 5 Window duration 5 minutes (300 seconds) Lockout duration 15 minutes (900 seconds) <p>After 5 failed login attempts from the same IP, that IP is locked out for 15 minutes.</p>"},{"location":"admin/settings/#jwt-configuration","title":"JWT Configuration","text":"Setting Default Description <code>JWT_SECRET</code> Auto-generated Secret key for signing tokens Access token expiry 24 hours (1440 minutes) Access tokens returned in JSON response body Refresh token expiry 7 days Refresh tokens stored in HttpOnly cookies CLI token expiry 30 days Long-lived tokens for CLI usage <code>COOKIE_SECURE</code> <code>false</code> Set to <code>true</code> for HTTPS deployments <p>The frontend proactively refreshes access tokens before they expire. Refresh tokens are stored as SHA-256 hashes in the database and can be revoked via the logout endpoint.</p> <p>Production JWT Secret</p> <p>Always set a strong <code>JWT_SECRET</code> in production. The auto-generated secret changes on each restart, invalidating all existing tokens.</p>"},{"location":"admin/settings/#feature-settings-phase-10","title":"Feature Settings (Phase 10)","text":"<p>Per-user settings for advanced features are managed via the Settings page in the web UI. The Settings page has four tabs:</p> <ul> <li>API Keys: Manage per-user API keys for each provider (encrypted with Fernet)</li> <li>Providers: Configure provider endpoints, models, and model discovery</li> <li>Judge: Configure the LLM Judge model, mode, temperature, and custom instructions</li> <li>Tuning: Configure param tuner defaults (search space, presets) and prompt tuner settings</li> </ul>"},{"location":"admin/settings/#settings-api","title":"Settings API","text":"<pre><code># Get Phase 10 settings\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/settings/phase10\n\n# Save Phase 10 settings\ncurl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"judge\": {\"enabled\": true, \"model_id\": \"gpt-4o\", \"mode\": \"post_eval\"},\n    \"param_tuner\": {\"max_combinations\": 50, \"temp_min\": 0.0, \"temp_max\": 1.0},\n    \"prompt_tuner\": {\"mode\": \"quick\", \"generations\": 3}\n  }' \\\n  http://localhost:8501/api/settings/phase10\n</code></pre>"},{"location":"admin/settings/#search-space-presets","title":"Search Space Presets","text":"<p>The param tuner supports saved search space presets (up to 20 per user). Presets are stored within the Phase 10 settings under <code>param_tuner.presets</code>. Built-in vendor presets (e.g., Qwen3-Coder, GLM-4.7) are also available and cannot be edited.</p>"},{"location":"admin/settings/#per-model-param-support","title":"Per-Model Param Support","text":"<p>The <code>param_support</code> section stores per-provider default parameters and per-model overrides, allowing fine-grained control over which parameters are sent to each model.</p>"},{"location":"admin/settings/#settings-backup-and-restore","title":"Settings Backup and Restore","text":"<p>Export your complete configuration:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/settings &gt; backup.json\n</code></pre> <p>Restore from backup:</p> <pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @backup.json \\\n  http://localhost:8501/api/import/settings\n</code></pre> <p>This exports and restores providers, models, prompt templates, and defaults. It does not include API keys or user data.</p>"},{"location":"admin/settings/#provider-health-check","title":"Provider Health Check","text":"<p>Check connectivity to all configured providers:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/health/providers\n</code></pre> <p>This tests API connectivity for each provider that has a key configured.</p>"},{"location":"admin/users/","title":"User Management","text":"<p>Admin users can manage all user accounts, roles, rate limits, and view system-wide statistics.</p>"},{"location":"admin/users/#user-roles","title":"User Roles","text":"Role Description <code>admin</code> Full access including user management, audit logs, system settings, and global API key management <code>user</code> Standard access to benchmarks, tool eval, history, and personal configuration"},{"location":"admin/users/#admin-promotion","title":"Admin Promotion","text":"<p>The admin role is assigned in three ways:</p> <ol> <li>First user: The first account registered is automatically promoted to admin</li> <li>ADMIN_EMAIL env var: Set <code>ADMIN_EMAIL</code> in <code>.env</code> to auto-promote that email on startup</li> <li>Manual promotion: An existing admin changes another user's role via the API</li> </ol>"},{"location":"admin/users/#user-list","title":"User List","text":"<p>View all registered users with their metadata:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/users\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"users\": [\n    {\n      \"id\": \"abc123\",\n      \"email\": \"admin@example.com\",\n      \"role\": \"admin\",\n      \"created_at\": \"2026-01-15 10:00:00\",\n      \"last_login\": \"2026-02-17 14:30:00\",\n      \"benchmark_count\": 42,\n      \"key_count\": 3\n    }\n  ]\n}\n</code></pre>"},{"location":"admin/users/#change-user-role","title":"Change User Role","text":"<pre><code>curl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"role\": \"admin\"}' \\\n  http://localhost:8501/api/admin/users/{user_id}/role\n</code></pre> <p>Warning</p> <p>You cannot change your own role. This prevents accidentally locking yourself out.</p>"},{"location":"admin/users/#delete-user","title":"Delete User","text":"<p>Deleting a user removes all their data (config, keys, benchmark runs, tool eval runs, schedules).</p> <pre><code>curl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/users/{user_id}\n</code></pre> <p>Warning</p> <p>You cannot delete your own account. Audit log entries for the deleted user are preserved but the <code>user_id</code> field is set to NULL.</p>"},{"location":"admin/users/#rate-limits","title":"Rate Limits","text":""},{"location":"admin/users/#set-per-user-rate-limits","title":"Set Per-User Rate Limits","text":"<pre><code>curl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"benchmarks_per_hour\": 20,\n    \"max_concurrent\": 1,\n    \"max_runs_per_benchmark\": 10\n  }' \\\n  http://localhost:8501/api/admin/users/{user_id}/rate-limit\n</code></pre>"},{"location":"admin/users/#get-per-user-rate-limits","title":"Get Per-User Rate Limits","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/users/{user_id}/rate-limit\n</code></pre> <p>Default limits (when none are set):</p> Limit Default Benchmarks per hour 20 Max concurrent 1 Max runs per benchmark 10"},{"location":"admin/users/#usage-statistics","title":"Usage Statistics","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/stats\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"benchmarks_24h\": 15,\n  \"benchmarks_7d\": 89,\n  \"benchmarks_30d\": 342,\n  \"total_users\": 5,\n  \"top_users\": [\n    { \"username\": \"user@example.com\", \"cnt\": 120 }\n  ],\n  \"keys_by_provider\": [\n    { \"provider\": \"openai\", \"user_count\": 4 }\n  ]\n}\n</code></pre>"},{"location":"admin/users/#jobs-management","title":"Jobs Management","text":"<p>The Admin Dashboard includes a Jobs tab showing all active and queued jobs across all users. Admins can view and cancel any running job.</p>"},{"location":"admin/users/#list-active-jobs","title":"List Active Jobs","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/jobs\n</code></pre>"},{"location":"admin/users/#cancel-a-job","title":"Cancel a Job","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/jobs/{job_id}/cancel\n</code></pre> <p>The admin page auto-refreshes the jobs list every 15 seconds and shows:</p> <ul> <li>Active Jobs tab with running and queued jobs</li> <li>Users tab with user management</li> <li>Audit Log tab with filterable event history</li> </ul>"},{"location":"admin/users/#system-health","title":"System Health","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/admin/system\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"db_size_mb\": 12.5,\n  \"results_size_mb\": 3.2,\n  \"results_count\": 89,\n  \"benchmark_active\": false,\n  \"active_jobs\": [],\n  \"total_active\": 0,\n  \"total_queued\": 0,\n  \"connected_ws_clients\": 2,\n  \"process_uptime_s\": 86400\n}\n</code></pre> <p>The system health panel in the Admin Dashboard displays DB size, results file count, uptime, active/queued job counts, and connected WebSocket clients.</p>"},{"location":"admin/users/#application-logs","title":"Application Logs","text":"<p>Admins can access application logs via the API. Authentication uses either an admin JWT or a static <code>LOG_ACCESS_TOKEN</code> query parameter.</p> <pre><code># Using admin JWT\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/admin/logs?lines=50\"\n\n# Using static token\ncurl \"http://localhost:8501/api/admin/logs?token=YOUR_LOG_TOKEN&amp;lines=50\"\n\n# Filter by level\ncurl \"http://localhost:8501/api/admin/logs?token=YOUR_LOG_TOKEN&amp;level=ERROR&amp;lines=20\"\n\n# Search by keyword\ncurl \"http://localhost:8501/api/admin/logs?token=YOUR_LOG_TOKEN&amp;search=benchmark&amp;lines=50\"\n</code></pre> <p>Logs are stored in an in-memory ring buffer (2000 entries max) and reset on container restart.</p>"},{"location":"admin/users/#audit-log","title":"Audit Log","text":"<p>The audit log records all significant actions (logins, benchmarks, admin changes):</p> <pre><code># Full log (paginated)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/admin/audit?limit=50&amp;offset=0\"\n\n# Filter by user\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/admin/audit?user=admin@example.com\"\n\n# Filter by action\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/admin/audit?action=benchmark_start\"\n\n# Filter by time\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/admin/audit?since=2026-02-01\"\n</code></pre>"},{"location":"admin/users/#audit-actions","title":"Audit Actions","text":"Action Description <code>user_register</code> New account created <code>user_login</code> Successful login <code>benchmark_start</code> Benchmark run started <code>benchmark_complete</code> Benchmark run completed <code>benchmark_cancel</code> Benchmark run cancelled <code>admin_user_update</code> Admin changed a user's role <code>admin_user_delete</code> Admin deleted a user <code>admin_rate_limit</code> Admin changed rate limits <p>Audit log entries older than 90 days are automatically cleaned up on server startup.</p>"},{"location":"api/config-schema/","title":"Configuration Schema","text":""},{"location":"api/config-schema/#provider-parameter-registry","title":"Provider Parameter Registry","text":"<p>The provider parameter registry defines the parameter support, ranges, and conflict rules for each LLM provider. It uses a three-tier architecture implemented in <code>provider_params.py</code>.</p>"},{"location":"api/config-schema/#three-tier-architecture","title":"Three-Tier Architecture","text":"<p>Tier 1 -- Universal Parameters</p> <p>Supported by all providers:</p> Parameter Type Description <code>temperature</code> float Sampling temperature <code>max_tokens</code> int Maximum output tokens <code>stop</code> string_array Stop sequences <p>Tier 2 -- Common Parameters</p> <p>Supported by most providers (with variations):</p> Parameter Type Description <code>top_p</code> float Nucleus sampling <code>top_k</code> int Top-k sampling <code>frequency_penalty</code> float Frequency penalty <code>presence_penalty</code> float Presence penalty <code>seed</code> int Random seed <code>reasoning_effort</code> enum Reasoning mode (none/low/medium/high) <p>Tier 3 -- Provider-Specific Parameters</p> <p>JSON passthrough for any LiteLLM-supported parameter. Bypasses validation. Passed via <code>provider_params.passthrough</code> in request bodies.</p>"},{"location":"api/config-schema/#provider-support-matrix","title":"Provider Support Matrix","text":"Provider Temp Range top_p top_k freq_penalty pres_penalty seed reasoning OpenAI 0-2 Yes No Yes (-2 to 2) Yes (-2 to 2) Deprecated Yes (o-series, GPT-5) Anthropic 0-1 Yes* Yes (1-500) No No No Yes (maps to thinking.budget_tokens) Gemini 0-2 Yes Yes (1-100) Yes (-2 to 2) Yes (-2 to 2) Yes Yes (maps to thinkingConfig) Ollama 0-2 Yes Yes (1-500) Yes (-2 to 2) Yes (-2 to 2) Yes No LM Studio 0-2 Yes Yes (1-500) Yes (-2 to 2) Yes (-2 to 2) Yes No Mistral 0-1.5 Yes Partial Yes (-2 to 2) Yes (-2 to 2) Yes No DeepSeek 0-2 Yes No Yes (-2 to 2) Yes (-2 to 2) No Yes (binary: on/off) Cohere 0-1 Yes (max 0.99) Yes (0-500) Yes (0-1) Yes (0-1) Yes No xAI (Grok) 0-2 Yes No Yes (-2 to 2) Yes (-2 to 2) Yes Partial vLLM 0-2 Yes Yes (1-500) Yes (-2 to 2) Yes (-2 to 2) Yes No <p>*Anthropic: Cannot use top_p and temperature simultaneously on newer models.</p>"},{"location":"api/config-schema/#tier-3-provider-specific-passthrough-examples","title":"Tier 3 Provider-Specific Passthrough Examples","text":"<p>Each provider has documented Tier 3 parameters that can be passed via the <code>passthrough</code> field:</p> Provider Parameters OpenAI <code>service_tier</code> (auto/default/flex/priority), <code>prediction</code>, <code>web_search_options</code> Anthropic <code>cache_control</code>, <code>inference_geo</code> Gemini <code>safety_settings</code> Ollama <code>mirostat</code> (0/1/2), <code>mirostat_eta</code>, <code>mirostat_tau</code>, <code>repetition_penalty</code>, <code>num_ctx</code>, <code>min_p</code>, <code>keep_alive</code> LM Studio <code>repetition_penalty</code>, <code>min_p</code> Mistral <code>safe_prompt</code> Cohere <code>safety_mode</code> (CONTEXTUAL/STRICT/OFF), <code>documents</code>, <code>citation_options</code> vLLM <code>repetition_penalty</code>, <code>min_p</code>, <code>typical_p</code>, <code>guided_json</code>, <code>guided_choice</code>, <code>best_of</code>, <code>ignore_eos</code> <p>Note: vLLM parameters must be passed as direct kwargs, not via <code>extra_body</code>.</p>"},{"location":"api/config-schema/#model-specific-overrides","title":"Model-Specific Overrides","text":"<p>Some models have locked or modified parameters:</p> <ul> <li>GPT-5: Temperature locked to 1.0</li> <li>O-series (o1, o3, o4): Temperature locked to 1.0; uses <code>max_completion_tokens</code> instead of <code>max_tokens</code>; stop sequences not supported</li> <li>Gemini 3: Temperature minimum clamped to 1.0</li> </ul>"},{"location":"api/config-schema/#conflict-resolution","title":"Conflict Resolution","text":"<p>The system resolves parameter conflicts using four action types:</p> Action Description <code>drop</code> Parameter removed from request (hard mutual exclusion) <code>warn</code> Parameter passed through unchanged with a warning <code>rename</code> Parameter renamed/remapped (value preserved) <code>clamp</code> Value adjusted to provider's valid range <p>Conflict rules by provider:</p> Conflict Action Resolution Anthropic: temperature + top_p both set drop Drops top_p, keeps temperature Anthropic: freq_penalty / pres_penalty / seed warn Passed through (provider may reject) OpenAI O-series: max_tokens rename Converts to max_completion_tokens OpenAI: top_k set warn Passed through (provider may reject) DeepSeek R1: thinking mode + sampling params warn Params have no effect in thinking mode DeepSeek: top_k or seed warn Passed through (provider may reject) xAI: reasoning + penalties/stop warn Provider may reject when reasoning active xAI: top_k set warn Passed through (provider may reject) Cohere: penalty &gt; 1.0 clamp Clamped to 0-1 range Cohere: top_p &gt; 0.99 clamp Clamped to 0.99 Mistral: top_k set warn Limited support"},{"location":"api/config-schema/#provider-identification","title":"Provider Identification","text":"<p>The system identifies which provider registry to use through a resolution chain:</p> <ol> <li>Explicit provider_key from config.yaml (if it matches a registry key)</li> <li>Model ID prefix detection (e.g., <code>anthropic/</code>, <code>gemini/</code>, <code>ollama/</code>)</li> <li>Fallback to <code>_unknown</code> (OpenAI-compatible defaults)</li> </ol> <p>Recognized prefixes: <code>anthropic/</code>, <code>gemini/</code>, <code>vertex_ai/</code>, <code>ollama/</code>, <code>ollama_chat/</code>, <code>lm_studio/</code>, <code>mistral/</code>, <code>deepseek/</code>, <code>cohere/</code>, <code>cohere_chat/</code>, <code>xai/</code>, <code>vllm/</code>, <code>openai/</code>.</p>"},{"location":"api/config-schema/#parameter-validation-api","title":"Parameter Validation API","text":"<p>Validate parameters before sending them to a provider:</p> <pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"provider_key\": \"openai\",\n    \"model_id\": \"gpt-4o\",\n    \"params\": {\n      \"temperature\": 1.5,\n      \"top_p\": 0.9,\n      \"top_k\": 50\n    }\n  }' \\\n  http://localhost:8501/api/provider-params/validate\n</code></pre> <p>Response:</p> <pre><code>{\n  \"valid\": false,\n  \"has_warnings\": true,\n  \"adjustments\": [\n    {\n      \"param\": \"top_k\",\n      \"original\": 50,\n      \"adjusted\": 50,\n      \"action\": \"warn\",\n      \"reason\": \"OpenAI may not support top_k -- passing through\"\n    }\n  ],\n  \"warnings\": [],\n  \"resolved_params\": {\n    \"temperature\": 1.5,\n    \"top_p\": 0.9,\n    \"top_k\": 50\n  }\n}\n</code></pre> <p>The <code>valid</code> field is <code>false</code> when any parameter was dropped or clamped. Warnings (pass-through with notice) do not invalidate the request.</p>"},{"location":"api/config-schema/#full-registry-api","title":"Full Registry API","text":"<p>Get the complete provider parameter registry:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/provider-params/registry\n</code></pre>"},{"location":"api/config-schema/#build_litellm_kwargs","title":"build_litellm_kwargs","text":"<p>The <code>build_litellm_kwargs()</code> function is the central pipeline for preparing parameters for LiteLLM calls. It:</p> <ol> <li>Merges explicit temperature/max_tokens with provider_params</li> <li>Runs validation and conflict resolution</li> <li>Applies model-specific <code>skip_params</code> from config</li> <li>Merges Tier 3 passthrough parameters (bypass validation)</li> <li>Returns a clean kwargs dict ready for <code>litellm.completion()</code></li> </ol>"},{"location":"api/config-schema/#search-space-presets","title":"Search Space Presets","text":"<p>The param tuner supports saved search space presets for reuse across tuning sessions. Presets are stored per-user in the Phase 10 settings.</p>"},{"location":"api/config-schema/#custom-presets","title":"Custom Presets","text":"<p>Users can save up to 20 custom presets via the Settings API:</p> <pre><code>curl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"param_tuner\": {\n      \"presets\": [\n        {\n          \"name\": \"Conservative Sampling\",\n          \"search_space\": {\n            \"temperature\": [0.0, 0.3, 0.5],\n            \"top_p\": [0.8, 0.95]\n          }\n        },\n        {\n          \"name\": \"Aggressive Exploration\",\n          \"search_space\": {\n            \"temperature\": [0.5, 0.8, 1.2, 1.5],\n            \"top_p\": [0.7, 0.8, 0.9, 1.0],\n            \"top_k\": [20, 50, 100]\n          }\n        }\n      ]\n    }\n  }' \\\n  http://localhost:8501/api/settings/phase10\n</code></pre>"},{"location":"api/config-schema/#built-in-vendor-presets","title":"Built-in Vendor Presets","text":"<p>The system includes vendor-recommended presets that are always available (returned from <code>POST /api/param-support/seed</code>):</p> <p>Qwen3 Coder 30B (Recommended):</p> <pre><code>{\n  \"name\": \"Qwen3 Coder 30B (Recommended)\",\n  \"builtin\": true,\n  \"search_space\": {\n    \"temperature\": [0.7],\n    \"top_p\": [0.8],\n    \"top_k\": [20]\n  },\n  \"system_prompt\": \"Greedy decoding (temp=0) worsens quality. Always use sampling.\"\n}\n</code></pre> <p>GLM-4.7 Flash (Z.AI Recommended):</p> <pre><code>{\n  \"name\": \"GLM-4.7 Flash (Z.AI Recommended)\",\n  \"builtin\": true,\n  \"search_space\": {\n    \"temperature\": [0.8],\n    \"top_p\": [0.6],\n    \"top_k\": [2]\n  },\n  \"system_prompt\": \"Very low top_k recommended for MoE architecture.\"\n}\n</code></pre> <p>Built-in presets have <code>\"builtin\": true</code> and cannot be deleted by users.</p>"},{"location":"api/config-schema/#per-model-search-spaces","title":"Per-Model Search Spaces","text":"<p>The param tuner supports <code>per_model_search_spaces</code> which allows different search spaces per model in a single tuning run:</p> <pre><code>{\n  \"suite_id\": \"suite-id\",\n  \"models\": [\"gpt-4o\", \"ollama/qwen3-coder\"],\n  \"search_space\": {\n    \"temperature\": [0.0, 0.5, 1.0]\n  },\n  \"per_model_search_spaces\": {\n    \"ollama/qwen3-coder\": {\n      \"temperature\": [0.7],\n      \"top_p\": [0.8],\n      \"top_k\": [20]\n    }\n  }\n}\n</code></pre> <p>Models listed in <code>per_model_search_spaces</code> use their custom space; all others fall back to the global <code>search_space</code>.</p>"},{"location":"api/config-schema/#configuration-yaml-schema","title":"Configuration YAML Schema","text":"<p>The configuration file follows this schema:</p> <pre><code>defaults:\n  max_tokens: &lt;int&gt;           # 1-128000\n  temperature: &lt;float&gt;        # 0.0-2.0\n  context_tiers: &lt;list[int]&gt;  # Token counts\n  prompt: &lt;string&gt;            # Default prompt\n\nprompt_templates:\n  &lt;template_key&gt;:\n    category: &lt;string&gt;        # reasoning, code, creative, short_qa, general\n    label: &lt;string&gt;           # Display name\n    prompt: &lt;string&gt;          # Prompt text\n\nproviders:\n  &lt;provider_key&gt;:\n    display_name: &lt;string&gt;\n    api_key_env: &lt;string&gt;     # Optional: env var name for API key\n    api_base: &lt;string&gt;        # Optional: custom API base URL\n    api_key: &lt;string&gt;         # Optional: inline API key\n    model_id_prefix: &lt;string&gt; # Optional: LiteLLM prefix (e.g., \"anthropic\")\n    models:\n      - id: &lt;string&gt;                    # LiteLLM model ID\n        display_name: &lt;string&gt;\n        context_window: &lt;int&gt;           # Optional, default 128000\n        max_output_tokens: &lt;int&gt;        # Optional\n        skip_params: &lt;list[string]&gt;     # Optional: params to omit (e.g., [\"temperature\"])\n        system_prompt: &lt;string&gt;         # Optional: per-model system prompt\n        input_cost_per_mtok: &lt;float&gt;    # Optional: input cost per million tokens\n        output_cost_per_mtok: &lt;float&gt;   # Optional: output cost per million tokens\n</code></pre> <p>Each user gets their own copy of the configuration stored in the <code>user_configs</code> table. The base configuration comes from <code>config.yaml</code> and is copied on first access.</p>"},{"location":"api/config-schema/#database-schema","title":"Database Schema","text":""},{"location":"api/config-schema/#users","title":"Users","text":"<pre><code>CREATE TABLE users (\n    id TEXT PRIMARY KEY,\n    email TEXT UNIQUE NOT NULL,\n    password_hash TEXT NOT NULL,\n    role TEXT NOT NULL DEFAULT 'user',  -- 'admin' or 'user'\n    created_at TEXT NOT NULL,\n    updated_at TEXT NOT NULL,\n    onboarding_completed INTEGER DEFAULT 0\n);\n</code></pre>"},{"location":"api/config-schema/#refresh-tokens","title":"Refresh Tokens","text":"<pre><code>CREATE TABLE refresh_tokens (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    token_hash TEXT NOT NULL,\n    expires_at TEXT NOT NULL,\n    created_at TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#user-api-keys","title":"User API Keys","text":"<pre><code>CREATE TABLE user_api_keys (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    provider_key TEXT NOT NULL,\n    key_name TEXT NOT NULL,\n    encrypted_value TEXT NOT NULL,  -- Fernet encrypted\n    updated_at TEXT,\n    UNIQUE(user_id, provider_key)\n);\n</code></pre>"},{"location":"api/config-schema/#user-configs","title":"User Configs","text":"<pre><code>CREATE TABLE user_configs (\n    user_id TEXT PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,\n    config_json TEXT NOT NULL,  -- Full provider/model config\n    updated_at TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#benchmark-runs","title":"Benchmark Runs","text":"<pre><code>CREATE TABLE benchmark_runs (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    timestamp TEXT NOT NULL,\n    prompt TEXT,\n    context_tiers TEXT,       -- JSON array\n    results_json TEXT NOT NULL, -- Full results\n    metadata TEXT              -- JSON (source, schedule_id, etc.)\n);\n</code></pre>"},{"location":"api/config-schema/#tool-suites","title":"Tool Suites","text":"<pre><code>CREATE TABLE tool_suites (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    name TEXT NOT NULL,\n    description TEXT DEFAULT '',\n    tools_json TEXT NOT NULL,   -- OpenAI function calling schema\n    system_prompt TEXT          -- Optional suite-level system prompt\n);\n</code></pre>"},{"location":"api/config-schema/#tool-test-cases","title":"Tool Test Cases","text":"<pre><code>CREATE TABLE tool_test_cases (\n    id TEXT PRIMARY KEY,\n    suite_id TEXT NOT NULL REFERENCES tool_suites(id) ON DELETE CASCADE,\n    prompt TEXT NOT NULL,\n    expected_tool TEXT,         -- string or JSON array\n    expected_params TEXT,       -- JSON object\n    param_scoring TEXT NOT NULL DEFAULT 'exact',\n    multi_turn_config TEXT,     -- JSON for multi-turn cases\n    scoring_config_json TEXT    -- JSON for fuzzy scoring rules\n);\n</code></pre>"},{"location":"api/config-schema/#tool-eval-runs","title":"Tool Eval Runs","text":"<pre><code>CREATE TABLE tool_eval_runs (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    suite_id TEXT NOT NULL,\n    suite_name TEXT,\n    models_json TEXT NOT NULL,\n    results_json TEXT,\n    summary_json TEXT,\n    config_json TEXT,           -- Eval configuration snapshot\n    experiment_id TEXT,\n    timestamp TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#experiments","title":"Experiments","text":"<pre><code>CREATE TABLE experiments (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    name TEXT NOT NULL,\n    description TEXT DEFAULT '',\n    suite_id TEXT NOT NULL,\n    status TEXT DEFAULT 'active',\n    baseline_eval_id TEXT,\n    baseline_score REAL,\n    best_score REAL,\n    best_source TEXT,\n    best_source_id TEXT,\n    best_config_json TEXT,\n    suite_snapshot_json TEXT,\n    created_at TEXT NOT NULL,\n    updated_at TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#param-tune-runs","title":"Param Tune Runs","text":"<pre><code>CREATE TABLE param_tune_runs (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    suite_id TEXT NOT NULL,\n    models_json TEXT,\n    search_space_json TEXT,\n    results_json TEXT,\n    best_config_json TEXT,\n    best_score REAL,\n    status TEXT,\n    experiment_id TEXT,\n    timestamp TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#prompt-tune-runs","title":"Prompt Tune Runs","text":"<pre><code>CREATE TABLE prompt_tune_runs (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    suite_id TEXT NOT NULL,\n    mode TEXT,\n    target_models_json TEXT,\n    meta_model TEXT,\n    results_json TEXT,\n    best_prompt TEXT,\n    best_score REAL,\n    status TEXT,\n    experiment_id TEXT,\n    timestamp TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#judge-reports","title":"Judge Reports","text":"<pre><code>CREATE TABLE judge_reports (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    eval_run_id TEXT,\n    judge_model TEXT,\n    mode TEXT,\n    verdicts_json TEXT,\n    report_json TEXT,\n    overall_grade TEXT,\n    overall_score REAL,\n    experiment_id TEXT,\n    status TEXT,\n    timestamp TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#jobs","title":"Jobs","text":"<pre><code>CREATE TABLE jobs (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    job_type TEXT NOT NULL,\n    status TEXT NOT NULL DEFAULT 'pending',\n    params_json TEXT,\n    progress_pct INTEGER DEFAULT 0,\n    progress_detail TEXT DEFAULT '',\n    result_ref TEXT,\n    error_msg TEXT,\n    timeout_seconds INTEGER DEFAULT 7200,\n    timeout_at TEXT,\n    created_at TEXT NOT NULL,\n    started_at TEXT,\n    completed_at TEXT\n);\n</code></pre>"},{"location":"api/config-schema/#schedules","title":"Schedules","text":"<pre><code>CREATE TABLE schedules (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    name TEXT NOT NULL,\n    prompt TEXT NOT NULL,\n    models_json TEXT NOT NULL,\n    max_tokens INTEGER DEFAULT 512,\n    temperature REAL DEFAULT 0.7,\n    interval_hours INTEGER NOT NULL,\n    enabled INTEGER DEFAULT 1,\n    last_run TEXT,\n    next_run TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/config-schema/#rate-limits","title":"Rate Limits","text":"<pre><code>CREATE TABLE rate_limits (\n    user_id TEXT PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,\n    benchmarks_per_hour INTEGER DEFAULT 20,\n    max_concurrent INTEGER DEFAULT 1,\n    max_runs_per_benchmark INTEGER DEFAULT 10,\n    updated_at TEXT,\n    updated_by TEXT\n);\n</code></pre>"},{"location":"api/config-schema/#audit-log","title":"Audit Log","text":"<pre><code>CREATE TABLE audit_log (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    user_id TEXT,              -- NULL-able (preserved when user deleted)\n    username TEXT,\n    action TEXT NOT NULL,\n    resource_type TEXT,\n    resource_id TEXT,\n    detail TEXT,               -- JSON\n    ip_address TEXT,\n    user_agent TEXT,\n    timestamp TEXT NOT NULL\n);\n</code></pre>"},{"location":"api/rest/","title":"REST API Reference","text":"<p>All API endpoints require authentication via JWT bearer token unless noted otherwise. Tokens are obtained from the authentication endpoints.</p> <p>Base URL: <code>http://localhost:8501</code></p> <p>All long-running operations (benchmarks, tool evals, param tuning, prompt tuning, judge) return a <code>job_id</code> immediately. Real-time progress is delivered via WebSocket, not the HTTP response.</p>"},{"location":"api/rest/#authentication","title":"Authentication","text":""},{"location":"api/rest/#register","title":"Register","text":"<p>Create a new user account. The first registered user is automatically promoted to admin. Users matching the <code>ADMIN_EMAIL</code> environment variable are also auto-promoted.</p> <pre><code>POST /api/auth/register\n</code></pre> <p>Request body:</p> <pre><code>{\n  \"email\": \"user@example.com\",\n  \"password\": \"minimum8chars\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"user\": { \"id\": \"abc123\", \"email\": \"user@example.com\", \"role\": \"user\" },\n  \"access_token\": \"eyJ...\",\n  \"token_type\": \"bearer\"\n}\n</code></pre> <p>A refresh token is set as an HttpOnly cookie. Access tokens expire after 24 hours.</p>"},{"location":"api/rest/#login","title":"Login","text":"<pre><code>POST /api/auth/login\n</code></pre> <p>Request body:</p> <pre><code>{\n  \"email\": \"user@example.com\",\n  \"password\": \"your-password\"\n}\n</code></pre> <p>Response: Same format as Register. Login is rate-limited (5 attempts per 5 minutes per IP, 15-minute lockout).</p>"},{"location":"api/rest/#refresh-token","title":"Refresh Token","text":"<pre><code>POST /api/auth/refresh\n</code></pre> <p>Uses the HttpOnly refresh token cookie to issue a new access token. No request body needed.</p>"},{"location":"api/rest/#logout","title":"Logout","text":"<pre><code>POST /api/auth/logout\n</code></pre> <p>Revokes the refresh token.</p>"},{"location":"api/rest/#get-current-user","title":"Get Current User","text":"<pre><code>GET /api/auth/me\n</code></pre> <p>Returns the authenticated user's profile.</p>"},{"location":"api/rest/#change-password","title":"Change Password","text":"<pre><code>POST /api/auth/change-password\n</code></pre> <p>Request body:</p> <pre><code>{\n  \"current_password\": \"old-password\",\n  \"new_password\": \"new-password-min-8\"\n}\n</code></pre>"},{"location":"api/rest/#generate-cli-token","title":"Generate CLI Token","text":"<pre><code>POST /api/auth/cli-token\n</code></pre> <p>Generates a long-lived JWT (30 days) for CLI usage. CLI tokens are accepted for both REST and WebSocket authentication.</p> <p>Response:</p> <pre><code>{\n  \"token\": \"eyJ...\",\n  \"expires_in_days\": 30\n}\n</code></pre>"},{"location":"api/rest/#health-seo","title":"Health &amp; SEO","text":""},{"location":"api/rest/#health-check","title":"Health Check","text":"<pre><code>GET /healthz\n</code></pre> <p>No authentication required.</p> <pre><code>{\"status\": \"ok\", \"version\": \"1.2.0\"}\n</code></pre>"},{"location":"api/rest/#robotstxt","title":"Robots.txt","text":"<pre><code>GET /robots.txt\n</code></pre>"},{"location":"api/rest/#sitemap","title":"Sitemap","text":"<pre><code>GET /sitemap.xml\n</code></pre>"},{"location":"api/rest/#configuration","title":"Configuration","text":""},{"location":"api/rest/#get-configuration","title":"Get Configuration","text":"<pre><code>GET /api/config\n</code></pre> <p>Returns the user's providers, models, and defaults. Each provider includes its models with <code>model_id</code>, <code>display_name</code>, <code>context_window</code>, <code>max_output_tokens</code>, <code>skip_params</code>, and any custom fields (costs, system_prompt, etc.).</p>"},{"location":"api/rest/#add-provider","title":"Add Provider","text":"<pre><code>POST /api/config/provider\n</code></pre> <pre><code>{\n  \"provider_key\": \"my_provider\",\n  \"display_name\": \"My Provider\",\n  \"api_base\": \"http://localhost:1234/v1\",\n  \"api_key_env\": \"MY_API_KEY\",\n  \"model_id_prefix\": \"my_provider\"\n}\n</code></pre>"},{"location":"api/rest/#update-provider","title":"Update Provider","text":"<pre><code>PUT /api/config/provider\n</code></pre> <pre><code>{\n  \"provider_key\": \"my_provider\",\n  \"display_name\": \"Updated Name\",\n  \"api_base\": \"http://new-url:1234/v1\"\n}\n</code></pre>"},{"location":"api/rest/#delete-provider","title":"Delete Provider","text":"<pre><code>DELETE /api/config/provider\n</code></pre> <pre><code>{ \"provider_key\": \"my_provider\" }\n</code></pre>"},{"location":"api/rest/#add-model","title":"Add Model","text":"<pre><code>POST /api/config/model\n</code></pre> <pre><code>{\n  \"provider_key\": \"openai\",\n  \"id\": \"gpt-4o-mini\",\n  \"display_name\": \"GPT-4o Mini\",\n  \"context_window\": 128000\n}\n</code></pre>"},{"location":"api/rest/#update-model","title":"Update Model","text":"<pre><code>PUT /api/config/model\n</code></pre> <p>Supports updating display_name, context_window, max_output_tokens, skip_params, system_prompt, and custom_fields. Can also rename the model via <code>new_model_id</code>.</p> <pre><code>{\n  \"model_id\": \"gpt-4o\",\n  \"provider_key\": \"openai\",\n  \"display_name\": \"GPT-4o Updated\",\n  \"context_window\": 128000,\n  \"skip_params\": [\"temperature\"],\n  \"system_prompt\": \"You are a helpful assistant.\",\n  \"input_cost_per_mtok\": 2.50,\n  \"output_cost_per_mtok\": 10.00\n}\n</code></pre>"},{"location":"api/rest/#delete-model","title":"Delete Model","text":"<pre><code>DELETE /api/config/model\n</code></pre> <pre><code>{ \"provider_key\": \"openai\", \"model_id\": \"gpt-4o-mini\" }\n</code></pre>"},{"location":"api/rest/#prompt-templates","title":"Prompt Templates","text":"<pre><code>GET /api/config/prompts           # List templates\nPOST /api/config/prompts          # Add template\n</code></pre> <p>Add template:</p> <pre><code>{\n  \"key\": \"my_template\",\n  \"label\": \"My Template\",\n  \"category\": \"reasoning\",\n  \"prompt\": \"Explain the concept of...\"\n}\n</code></pre>"},{"location":"api/rest/#api-keys","title":"API Keys","text":""},{"location":"api/rest/#per-user-keys","title":"Per-User Keys","text":"<p>Keys are Fernet-encrypted and stored per-user. They override global environment keys.</p> <pre><code>GET /api/keys                     # List user's key status per provider\nPUT /api/keys                     # Set/update a key\nDELETE /api/keys                  # Remove a key\n</code></pre> <p>Set a key:</p> <pre><code>{ \"provider_key\": \"openai\", \"value\": \"sk-...\" }\n</code></pre> <p>Response from GET: Returns per-provider status showing <code>has_user_key</code>, <code>has_global_key</code>, and display metadata. Never returns plaintext keys.</p>"},{"location":"api/rest/#global-environment-keys-admin-only","title":"Global Environment Keys (Admin Only)","text":"<pre><code>GET /api/env                      # List env keys (masked)\nPUT /api/env                      # Set/update env key\nDELETE /api/env                   # Remove env key\n</code></pre> <p>Only provider API key environment variables from the safe list can be modified: <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>MISTRAL_API_KEY</code>, <code>COHERE_API_KEY</code>, <code>GROQ_API_KEY</code>, <code>DEEPSEEK_API_KEY</code>, <code>TOGETHER_API_KEY</code>, <code>FIREWORKS_API_KEY</code>, <code>XAI_API_KEY</code>, <code>DEEPINFRA_API_KEY</code>, <code>CEREBRAS_API_KEY</code>, <code>SAMBANOVA_API_KEY</code>, <code>OPENROUTER_API_KEY</code>.</p>"},{"location":"api/rest/#benchmarks","title":"Benchmarks","text":""},{"location":"api/rest/#run-benchmark","title":"Run Benchmark","text":"<pre><code>POST /api/benchmark\n</code></pre> <p>Submits a benchmark job to the JobRegistry. Returns a <code>job_id</code> immediately. Progress and results are delivered via WebSocket.</p> <p>Request body:</p> <pre><code>{\n  \"models\": [\"gpt-4o\", \"anthropic/claude-sonnet-4-5\"],\n  \"runs\": 3,\n  \"max_tokens\": 512,\n  \"temperature\": 0.7,\n  \"prompt\": \"Explain recursion in programming\",\n  \"context_tiers\": [0, 5000],\n  \"warmup\": true,\n  \"provider_params\": {\n    \"top_p\": 0.9,\n    \"passthrough\": { \"service_tier\": \"flex\" }\n  }\n}\n</code></pre> <p>Alternatively, use precise <code>targets</code> instead of <code>models</code>:</p> <pre><code>{\n  \"targets\": [\n    { \"provider_key\": \"openai\", \"model_id\": \"gpt-4o\" },\n    { \"provider_key\": \"lm_studio\", \"model_id\": \"lm_studio/qwen3-coder\" }\n  ],\n  \"runs\": 1\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"job_id\": \"a1b2c3d4e5f6...\",\n  \"status\": \"submitted\"\n}\n</code></pre> <p>WebSocket events for benchmark jobs:</p> Event Type Description <code>job_created</code> Job registered (pending or queued) <code>job_started</code> Job execution began <code>benchmark_init</code> Target list, run count, context tiers <code>job_progress</code> Progress percentage and detail string <code>benchmark_result</code> Individual run metrics (model, TPS, TTFT, cost) <code>benchmark_skipped</code> Context tier skipped (exceeds model window) <code>job_completed</code> All runs finished, includes <code>result_ref</code> (run ID) <code>job_failed</code> Error occurred <code>job_cancelled</code> Benchmark was cancelled"},{"location":"api/rest/#cancel-benchmark","title":"Cancel Benchmark","text":"<pre><code>POST /api/benchmark/cancel\n</code></pre> <p>Request body (optional):</p> <pre><code>{ \"job_id\": \"a1b2c3d4...\" }\n</code></pre> <p>If <code>job_id</code> is omitted, cancels the most recent active benchmark for the current user (backward compatibility).</p>"},{"location":"api/rest/#rate-limit-status","title":"Rate Limit Status","text":"<pre><code>GET /api/user/rate-limit\n</code></pre> <pre><code>{ \"limit\": 20, \"remaining\": 15, \"window\": \"1 hour\" }\n</code></pre>"},{"location":"api/rest/#history","title":"History","text":"<pre><code>GET /api/history                  # List benchmark runs\nGET /api/history/{run_id}         # Get specific run with full results\nDELETE /api/history/{run_id}      # Delete a run\n</code></pre>"},{"location":"api/rest/#jobs","title":"Jobs","text":"<p>Job tracking endpoints for monitoring all background operations.</p>"},{"location":"api/rest/#list-jobs","title":"List Jobs","text":"<pre><code>GET /api/jobs\n</code></pre> <p>Query parameters:</p> Parameter Type Description <code>status</code> string Comma-separated status filter (e.g. <code>running,queued</code>) <code>limit</code> int Max results (default 20) <p>Response:</p> <pre><code>{\n  \"jobs\": [\n    {\n      \"id\": \"abc123\",\n      \"job_type\": \"benchmark\",\n      \"status\": \"running\",\n      \"progress_pct\": 45,\n      \"progress_detail\": \"Benchmark: 3 models, 2 runs each\",\n      \"created_at\": \"2026-02-20T10:00:00Z\",\n      \"started_at\": \"2026-02-20T10:00:01Z\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/rest/#get-job","title":"Get Job","text":"<pre><code>GET /api/jobs/{job_id}\n</code></pre> <p>Returns full job details including <code>params_json</code>, <code>result_ref</code>, <code>error_msg</code>, timing fields.</p>"},{"location":"api/rest/#cancel-job","title":"Cancel Job","text":"<pre><code>POST /api/jobs/{job_id}/cancel\n</code></pre> <p>Cancels a specific job. Works for pending, queued, and running jobs. Also cleans up orphaned linked tune runs if the job is already terminal.</p>"},{"location":"api/rest/#admin-list-all-active-jobs","title":"Admin: List All Active Jobs","text":"<pre><code>GET /api/admin/jobs\n</code></pre> <p>Admin only. Lists all active jobs across all users.</p>"},{"location":"api/rest/#admin-cancel-any-job","title":"Admin: Cancel Any Job","text":"<pre><code>POST /api/admin/jobs/{job_id}/cancel\n</code></pre> <p>Admin only. Can cancel any user's job.</p> <p>Job statuses:</p> Status Description <code>pending</code> Created, about to start <code>queued</code> Waiting for concurrency slot <code>running</code> Actively executing <code>done</code> Completed successfully <code>failed</code> Error occurred <code>cancelled</code> Cancelled by user or admin <code>interrupted</code> Server shutdown or timeout <p>Job types: <code>benchmark</code>, <code>tool_eval</code>, <code>param_tune</code>, <code>prompt_tune</code>, <code>judge</code>, <code>judge_compare</code>, <code>schedule</code></p>"},{"location":"api/rest/#tool-suites","title":"Tool Suites","text":"<pre><code>GET /api/tool-suites                              # List suites\nPOST /api/tool-suites                             # Create suite\nGET /api/tool-suites/{suite_id}                   # Get suite with tools and test cases\nPUT /api/tool-suites/{suite_id}                   # Update suite (name, description, tools, system_prompt)\nPATCH /api/tool-suites/{suite_id}                 # Patch individual fields (lighter than PUT)\nDELETE /api/tool-suites/{suite_id}                # Delete suite and all cases\nGET /api/tool-suites/{suite_id}/export            # Export suite as downloadable JSON\nGET /api/tool-suites/{suite_id}/cases             # List test cases\nPOST /api/tool-suites/{suite_id}/cases            # Add case(s) -- single or bulk via \"cases\" array\nPUT /api/tool-suites/{suite_id}/cases/{case_id}   # Update case\nDELETE /api/tool-suites/{suite_id}/cases/{case_id} # Delete case\n</code></pre>"},{"location":"api/rest/#import-suite-from-json","title":"Import Suite from JSON","text":"<pre><code>POST /api/tool-eval/import\n</code></pre> <p>See Tool Calling Evaluation for the JSON format.</p>"},{"location":"api/rest/#import-example","title":"Import Example","text":"<pre><code>GET /api/tool-eval/import/example\n</code></pre> <p>Returns a downloadable example JSON template showing the expected import format.</p>"},{"location":"api/rest/#import-from-mcp-server","title":"Import from MCP Server","text":"<pre><code>POST /api/mcp/discover            # Discover tools from MCP server\nPOST /api/mcp/import              # Import MCP tools as a suite\n</code></pre> <p>Discover request:</p> <pre><code>{ \"url\": \"http://localhost:3000/sse\" }\n</code></pre> <p>Import request:</p> <pre><code>{\n  \"tools\": [{ \"name\": \"...\", \"description\": \"...\", \"inputSchema\": {...} }],\n  \"suite_name\": \"My MCP Suite\",\n  \"suite_description\": \"Imported from MCP server\",\n  \"generate_test_cases\": true\n}\n</code></pre> <p>When <code>generate_test_cases</code> is true, one sample test case is auto-generated per tool using realistic placeholder values derived from parameter names.</p>"},{"location":"api/rest/#tool-eval","title":"Tool Eval","text":""},{"location":"api/rest/#run-eval","title":"Run Eval","text":"<pre><code>POST /api/tool-eval\n</code></pre> <p>Submits a tool calling evaluation via the JobRegistry. Returns <code>job_id</code> immediately. Progress via WebSocket.</p> <pre><code>{\n  \"suite_id\": \"suite-id\",\n  \"models\": [\"gpt-4o\"],\n  \"temperature\": 0.0,\n  \"tool_choice\": \"required\",\n  \"provider_params\": { \"top_p\": 0.9 },\n  \"system_prompt\": \"Always use tools when available.\",\n  \"experiment_id\": \"exp-id\",\n  \"judge\": {\n    \"enabled\": true,\n    \"mode\": \"live_inline\",\n    \"judge_model\": \"anthropic/claude-sonnet-4-5\",\n    \"custom_instructions\": \"Focus on parameter completeness\"\n  }\n}\n</code></pre> <p>Supports <code>targets</code> array for precise provider+model selection (same as benchmarks).</p> <p>Response:</p> <pre><code>{ \"job_id\": \"abc123\", \"status\": \"submitted\" }\n</code></pre>"},{"location":"api/rest/#cancel-eval","title":"Cancel Eval","text":"<pre><code>POST /api/tool-eval/cancel\n</code></pre> <pre><code>{ \"job_id\": \"abc123\" }\n</code></pre>"},{"location":"api/rest/#eval-history","title":"Eval History","text":"<pre><code>GET /api/tool-eval/history                 # List runs (includes summary per model)\nGET /api/tool-eval/history/{eval_id}       # Get full run details with per-case results\nDELETE /api/tool-eval/history/{eval_id}    # Delete run\n</code></pre>"},{"location":"api/rest/#param-tuner","title":"Param Tuner","text":"<p>Grid search over parameter combinations to find optimal settings for tool calling accuracy.</p>"},{"location":"api/rest/#run-param-tune","title":"Run Param Tune","text":"<pre><code>POST /api/tool-eval/param-tune\n</code></pre> <p>Submits via JobRegistry. Returns <code>job_id</code> immediately.</p> <pre><code>{\n  \"suite_id\": \"suite-id\",\n  \"models\": [\"gpt-4o\", \"anthropic/claude-sonnet-4-5\"],\n  \"search_space\": {\n    \"temperature\": [0.0, 0.3, 0.7],\n    \"top_p\": [0.8, 0.95],\n    \"top_k\": [20, 50]\n  },\n  \"per_model_search_spaces\": {\n    \"gpt-4o\": { \"temperature\": [0.0, 0.5] },\n    \"anthropic/claude-sonnet-4-5\": { \"temperature\": [0.0, 0.3] }\n  },\n  \"experiment_id\": \"exp-id\"\n}\n</code></pre> <p>The <code>per_model_search_spaces</code> field is optional and overrides the global <code>search_space</code> for specific models.</p> <p>Response:</p> <pre><code>{ \"job_id\": \"abc123\", \"status\": \"submitted\" }\n</code></pre>"},{"location":"api/rest/#cancel-param-tune","title":"Cancel Param Tune","text":"<pre><code>POST /api/tool-eval/param-tune/cancel\n</code></pre> <pre><code>{ \"job_id\": \"abc123\" }\n</code></pre>"},{"location":"api/rest/#param-tune-history","title":"Param Tune History","text":"<pre><code>GET /api/tool-eval/param-tune/history              # List runs\nGET /api/tool-eval/param-tune/history/{tune_id}    # Get details (all combos + best config)\nDELETE /api/tool-eval/param-tune/history/{tune_id} # Delete\n</code></pre>"},{"location":"api/rest/#prompt-tuner","title":"Prompt Tuner","text":"<p>AI-powered system prompt optimization using a meta-model to generate and evaluate prompt variations.</p>"},{"location":"api/rest/#run-prompt-tune","title":"Run Prompt Tune","text":"<pre><code>POST /api/tool-eval/prompt-tune\n</code></pre> <p>Submits via JobRegistry. Returns <code>job_id</code> immediately.</p> <pre><code>{\n  \"suite_id\": \"suite-id\",\n  \"mode\": \"quick\",\n  \"target_models\": [\"gpt-4o\"],\n  \"meta_model\": \"anthropic/claude-sonnet-4-5\",\n  \"meta_provider_key\": \"anthropic\",\n  \"base_prompt\": \"You are a helpful assistant that uses tools.\",\n  \"config\": {\n    \"population_size\": 5,\n    \"generations\": 1\n  },\n  \"experiment_id\": \"exp-id\"\n}\n</code></pre> Mode Description <code>quick</code> Single generation of prompt variations <code>evolutionary</code> Multiple generations with mutation of winning prompts <p>Response:</p> <pre><code>{ \"job_id\": \"abc123\", \"status\": \"submitted\" }\n</code></pre>"},{"location":"api/rest/#cancel-prompt-tune","title":"Cancel Prompt Tune","text":"<pre><code>POST /api/tool-eval/prompt-tune/cancel\n</code></pre> <pre><code>{ \"job_id\": \"abc123\" }\n</code></pre>"},{"location":"api/rest/#cost-estimate","title":"Cost Estimate","text":"<pre><code>GET /api/tool-eval/prompt-tune/estimate\n</code></pre> <p>Query parameters:</p> Parameter Type Default Description <code>suite_id</code> string Suite to estimate for <code>mode</code> string <code>quick</code> <code>quick</code> or <code>evolutionary</code> <code>population_size</code> int 5 Prompts per generation <code>generations</code> int 1/3 Generations to run <code>num_models</code> int 1 Number of target models <p>Response:</p> <pre><code>{\n  \"total_prompt_generations\": 5,\n  \"total_eval_calls\": 50,\n  \"total_api_calls\": 51,\n  \"estimated_duration_s\": 105,\n  \"warning\": null\n}\n</code></pre>"},{"location":"api/rest/#prompt-tune-history","title":"Prompt Tune History","text":"<pre><code>GET /api/tool-eval/prompt-tune/history              # List runs\nGET /api/tool-eval/prompt-tune/history/{tune_id}    # Get details (all prompts + best)\nDELETE /api/tool-eval/prompt-tune/history/{tune_id} # Delete\n</code></pre>"},{"location":"api/rest/#judge","title":"Judge","text":"<p>AI-powered evaluation quality assessment. Uses a judge model to grade tool calling results.</p>"},{"location":"api/rest/#run-post-eval-judge","title":"Run Post-Eval Judge","text":"<pre><code>POST /api/tool-eval/judge\n</code></pre> <p>Submits via JobRegistry. Returns <code>job_id</code> immediately.</p> <pre><code>{\n  \"eval_run_id\": \"eval-run-id\",\n  \"judge_model\": \"anthropic/claude-sonnet-4-5\",\n  \"judge_provider_key\": \"anthropic\",\n  \"mode\": \"post_eval\",\n  \"custom_instructions\": \"Focus on parameter completeness\",\n  \"concurrency\": 4,\n  \"experiment_id\": \"exp-id\"\n}\n</code></pre>"},{"location":"api/rest/#run-comparative-judge","title":"Run Comparative Judge","text":"<pre><code>POST /api/tool-eval/judge/compare\n</code></pre> <p>Compares two eval runs head-to-head. Requires common test cases between the runs.</p> <pre><code>{\n  \"eval_run_id_a\": \"run-a-id\",\n  \"eval_run_id_b\": \"run-b-id\",\n  \"judge_model\": \"anthropic/claude-sonnet-4-5\",\n  \"judge_provider_key\": \"anthropic\",\n  \"concurrency\": 4,\n  \"experiment_id\": \"exp-id\"\n}\n</code></pre>"},{"location":"api/rest/#cancel-judge","title":"Cancel Judge","text":"<pre><code>POST /api/tool-eval/judge/cancel\n</code></pre> <pre><code>{ \"job_id\": \"abc123\" }\n</code></pre>"},{"location":"api/rest/#judge-reports","title":"Judge Reports","text":"<pre><code>GET /api/tool-eval/judge/reports                   # List reports\nGET /api/tool-eval/judge/reports/{report_id}       # Get full report with verdicts\nDELETE /api/tool-eval/judge/reports/{report_id}    # Delete report\n</code></pre>"},{"location":"api/rest/#experiments","title":"Experiments","text":"<p>Experiments group related eval, tune, and judge runs for A/B testing workflows.</p>"},{"location":"api/rest/#list-experiments","title":"List Experiments","text":"<pre><code>GET /api/experiments\n</code></pre>"},{"location":"api/rest/#create-experiment","title":"Create Experiment","text":"<pre><code>POST /api/experiments\n</code></pre> <pre><code>{\n  \"name\": \"Temperature Optimization\",\n  \"description\": \"Testing temperature ranges for weather API\",\n  \"suite_id\": \"suite-id\",\n  \"baseline_eval_id\": \"eval-id\",\n  \"snapshot_suite\": true\n}\n</code></pre>"},{"location":"api/rest/#get-experiment","title":"Get Experiment","text":"<pre><code>GET /api/experiments/{experiment_id}\n</code></pre> <p>Returns experiment with parsed <code>best_config</code> and <code>suite_name</code>.</p>"},{"location":"api/rest/#update-experiment","title":"Update Experiment","text":"<pre><code>PUT /api/experiments/{experiment_id}\n</code></pre> <pre><code>{\n  \"name\": \"Updated Name\",\n  \"description\": \"Updated description\",\n  \"status\": \"archived\"\n}\n</code></pre>"},{"location":"api/rest/#delete-experiment","title":"Delete Experiment","text":"<pre><code>DELETE /api/experiments/{experiment_id}\n</code></pre>"},{"location":"api/rest/#pin-baseline","title":"Pin Baseline","text":"<pre><code>PUT /api/experiments/{experiment_id}/baseline\n</code></pre> <p>Pin or re-pin a baseline eval run for score comparison.</p> <pre><code>{ \"eval_run_id\": \"eval-id\" }\n</code></pre>"},{"location":"api/rest/#get-timeline","title":"Get Timeline","text":"<pre><code>GET /api/experiments/{experiment_id}/timeline\n</code></pre> <p>Returns ordered timeline of all linked runs (eval, param_tune, prompt_tune, judge) with scores and delta from baseline.</p>"},{"location":"api/rest/#run-best-config","title":"Run Best Config","text":"<pre><code>POST /api/experiments/{experiment_id}/run-best\n</code></pre> <p>Convenience endpoint: runs a tool eval using the experiment's best discovered configuration. Optionally override models via request body.</p>"},{"location":"api/rest/#analytics","title":"Analytics","text":""},{"location":"api/rest/#leaderboard","title":"Leaderboard","text":"<pre><code>GET /api/analytics/leaderboard\n</code></pre> <p>Query parameters:</p> Parameter Values Default Description <code>type</code> <code>benchmark</code>, <code>tool_eval</code> <code>benchmark</code> Data source <code>period</code> <code>7d</code>, <code>30d</code>, <code>90d</code>, <code>all</code> <code>all</code> Time window <p>Benchmark response: Models ranked by avg TPS, with avg TTFT, avg cost, total runs.</p> <p>Tool eval response: Models ranked by avg overall %, with avg tool %, avg param %, total evals.</p>"},{"location":"api/rest/#trends","title":"Trends","text":"<pre><code>GET /api/analytics/trends\n</code></pre> <p>Query parameters:</p> Parameter Type Description <code>models</code> string Comma-separated model names (required) <code>metric</code> string <code>tps</code> or <code>ttft</code> <code>period</code> string <code>7d</code>, <code>30d</code>, <code>90d</code>, <code>all</code> <p>Returns time-series data points for each model.</p>"},{"location":"api/rest/#compare","title":"Compare","text":"<pre><code>GET /api/analytics/compare\n</code></pre> <p>Query parameters:</p> Parameter Type Description <code>runs</code> string Comma-separated run IDs (2-4 required) <p>Side-by-side comparison of specific benchmark runs.</p>"},{"location":"api/rest/#schedules","title":"Schedules","text":"<pre><code>GET /api/schedules                           # List schedules\nPOST /api/schedules                          # Create schedule\nPUT /api/schedules/{schedule_id}             # Update schedule\nDELETE /api/schedules/{schedule_id}          # Delete schedule\nPOST /api/schedules/{schedule_id}/trigger    # Trigger immediately\n</code></pre> <p>Create schedule:</p> <pre><code>{\n  \"name\": \"Daily GPT-4o Check\",\n  \"prompt\": \"Explain recursion\",\n  \"models\": [\"gpt-4o\", \"anthropic/claude-sonnet-4-5\"],\n  \"max_tokens\": 512,\n  \"temperature\": 0.7,\n  \"interval_hours\": 24\n}\n</code></pre>"},{"location":"api/rest/#export-import","title":"Export / Import","text":""},{"location":"api/rest/#export-endpoints","title":"Export Endpoints","text":"<pre><code>GET /api/export/history            # Benchmark history as CSV\nGET /api/export/leaderboard        # Leaderboard as CSV (?type=benchmark|tool_eval&amp;period=all)\nGET /api/export/tool-eval          # Tool eval runs as CSV\nGET /api/export/eval/{eval_id}     # Single eval run as JSON (with raw request/response)\nGET /api/export/run/{run_id}       # Single benchmark run as CSV\nGET /api/export/settings           # Configuration backup as JSON\n</code></pre>"},{"location":"api/rest/#import-endpoints","title":"Import Endpoints","text":"<pre><code>POST /api/import/settings          # Restore configuration from JSON\n</code></pre> <p>The settings import merges providers into existing config (adds new, updates existing). Requires <code>export_version</code> field in the payload.</p>"},{"location":"api/rest/#provider-discovery-health","title":"Provider Discovery &amp; Health","text":""},{"location":"api/rest/#discover-models","title":"Discover Models","text":"<pre><code>GET /api/models/discover?provider_key=openai\n</code></pre> <p>Fetches available models from the provider's API. Supports OpenAI, Anthropic, Gemini, and OpenAI-compatible endpoints (LM Studio, Ollama, vLLM).</p>"},{"location":"api/rest/#detect-lm-studio-backend","title":"Detect LM Studio Backend","text":"<pre><code>GET /api/lm-studio/detect?provider_key=lm_studio\n</code></pre> <p>Detects whether LM Studio is running GGUF or MLX models via <code>/v1/models</code>. Returns <code>backend_type</code> (<code>gguf</code>, <code>mlx</code>, <code>mixed</code>, <code>unknown</code>) and lists unsupported parameters for MLX backends.</p>"},{"location":"api/rest/#provider-health-check","title":"Provider Health Check","text":"<pre><code>GET /api/health/providers\n</code></pre> <p>Sends a minimal completion request to one model per provider to verify connectivity, API key validity, and measure latency.</p> <p>Response:</p> <pre><code>{\n  \"providers\": [\n    { \"name\": \"OpenAI\", \"status\": \"ok\", \"latency_ms\": 245 },\n    { \"name\": \"Anthropic\", \"status\": \"error\", \"latency_ms\": 5012, \"error\": \"Authentication failed\" }\n  ]\n}\n</code></pre>"},{"location":"api/rest/#provider-parameters","title":"Provider Parameters","text":""},{"location":"api/rest/#full-registry","title":"Full Registry","text":"<pre><code>GET /api/provider-params/registry\n</code></pre> <p>Returns the complete 3-tier parameter registry for all 10 supported providers. See Configuration Schema for the full registry structure.</p>"},{"location":"api/rest/#validate-parameters","title":"Validate Parameters","text":"<pre><code>POST /api/provider-params/validate\n</code></pre> <p>Validates parameters against provider constraints before running a benchmark or eval.</p> <pre><code>{\n  \"provider_key\": \"openai\",\n  \"model_id\": \"gpt-4o\",\n  \"params\": {\n    \"temperature\": 1.5,\n    \"top_p\": 0.9,\n    \"top_k\": 50\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"valid\": false,\n  \"has_warnings\": true,\n  \"adjustments\": [\n    {\n      \"param\": \"top_k\",\n      \"original\": 50,\n      \"adjusted\": 50,\n      \"action\": \"warn\",\n      \"reason\": \"OpenAI may not support top_k -- passing through\"\n    }\n  ],\n  \"warnings\": [],\n  \"resolved_params\": {\n    \"temperature\": 1.5,\n    \"top_p\": 0.9,\n    \"top_k\": 50\n  }\n}\n</code></pre>"},{"location":"api/rest/#seed-param-support-config","title":"Seed Param Support Config","text":"<pre><code>POST /api/param-support/seed\n</code></pre> <p>Generates default parameter support configuration from the built-in provider registry. Used by the Settings page to initialize per-model parameter support UI.</p>"},{"location":"api/rest/#settings","title":"Settings","text":""},{"location":"api/rest/#phase-10-settings","title":"Phase 10 Settings","text":"<pre><code>GET /api/settings/phase10          # Get feature settings\nPUT /api/settings/phase10          # Update feature settings\n</code></pre> <p>Settings for judge, param tuner, prompt tuner, and per-model parameter support configuration.</p> <p>PUT body:</p> <pre><code>{\n  \"judge\": { \"default_model\": \"anthropic/claude-sonnet-4-5\", \"concurrency\": 4 },\n  \"param_tuner\": {\n    \"presets\": [\n      { \"name\": \"My Preset\", \"search_space\": { \"temperature\": [0.0, 0.5, 1.0] } }\n    ]\n  },\n  \"prompt_tuner\": { \"population_size\": 5, \"generations\": 3 },\n  \"param_support\": {\n    \"provider_defaults\": { \"openai\": { \"params\": {...} } },\n    \"model_overrides\": {}\n  }\n}\n</code></pre> <p>Maximum 20 custom presets allowed per user.</p>"},{"location":"api/rest/#onboarding","title":"Onboarding","text":"<pre><code>GET /api/onboarding/status         # Check onboarding completion\nPOST /api/onboarding/complete      # Mark onboarding as complete\n</code></pre>"},{"location":"api/rest/#admin-endpoints","title":"Admin Endpoints","text":"<p>All admin endpoints require the <code>admin</code> role.</p>"},{"location":"api/rest/#user-management","title":"User Management","text":"<pre><code>GET /api/admin/users                               # List all users (with stats)\nPUT /api/admin/users/{user_id}/role                # Change user role (admin/user)\nDELETE /api/admin/users/{user_id}                  # Delete user and all data\n</code></pre> <p>Cannot change your own role or delete your own account. Audit log entries are preserved (unlinked, not deleted) when a user is deleted.</p>"},{"location":"api/rest/#rate-limits","title":"Rate Limits","text":"<pre><code>PUT /api/admin/users/{user_id}/rate-limit          # Set rate limits\nGET /api/admin/users/{user_id}/rate-limit          # Get rate limits\n</code></pre> <p>Set rate limits:</p> <pre><code>{\n  \"benchmarks_per_hour\": 50,\n  \"max_concurrent\": 3,\n  \"max_runs_per_benchmark\": 20\n}\n</code></pre>"},{"location":"api/rest/#statistics","title":"Statistics","text":"<pre><code>GET /api/admin/stats\n</code></pre> <p>Returns benchmark counts by time window (24h, 7d, 30d), top users, keys by provider, total user count.</p>"},{"location":"api/rest/#system-health","title":"System Health","text":"<pre><code>GET /api/admin/system\n</code></pre> <p>Returns database size, results count/size, active/queued job counts, connected WebSocket clients, process uptime.</p>"},{"location":"api/rest/#audit-log","title":"Audit Log","text":"<pre><code>GET /api/admin/audit\n</code></pre> <p>Query parameters:</p> Parameter Type Description <code>user</code> string Filter by username (email) <code>action</code> string Filter by action type <code>since</code> string ISO timestamp cutoff <code>limit</code> int Max entries (default 100) <code>offset</code> int Pagination offset"},{"location":"api/rest/#application-logs","title":"Application Logs","text":"<pre><code>GET /api/admin/logs\n</code></pre> <p>Returns recent application log entries from the in-memory ring buffer (2000 entries max).</p> <p>Authentication: Either admin JWT or <code>LOG_ACCESS_TOKEN</code> query parameter.</p> <p>Query parameters:</p> Parameter Type Description <code>lines</code> int Number of log lines (1-2000, default 100) <code>level</code> string Filter by level (ERROR, WARNING, INFO, DEBUG) <code>search</code> string Case-insensitive text search <code>token</code> string Static LOG_ACCESS_TOKEN for non-JWT access"},{"location":"api/rest/#error-responses","title":"Error Responses","text":"<p>All error responses follow a consistent format:</p> <pre><code>{ \"error\": \"Description of what went wrong\" }\n</code></pre> <p>Or for Pydantic validation errors (422):</p> <pre><code>{ \"detail\": \"Validation error description\" }\n</code></pre> <p>Common HTTP status codes:</p> Code Meaning 400 Bad request (invalid input) 401 Not authenticated 403 Forbidden (insufficient role) 404 Resource not found 422 Validation error 429 Rate limit exceeded 502 Upstream provider error 504 Upstream provider timeout"},{"location":"api/websocket/","title":"WebSocket Protocol","text":""},{"location":"api/websocket/#overview","title":"Overview","text":"<p>WebSocket connections provide real-time status updates for all background jobs (benchmarks, tool evals, param tuning, prompt tuning, judge operations). The WebSocket receives push notifications for any job started by the authenticated user, regardless of which browser tab initiated it.</p>"},{"location":"api/websocket/#connection","title":"Connection","text":"<pre><code>ws://localhost:8501/ws?token=&lt;JWT_ACCESS_TOKEN&gt;\n</code></pre> <p>Or with TLS:</p> <pre><code>wss://your-domain.com/ws?token=&lt;JWT_ACCESS_TOKEN&gt;\n</code></pre>"},{"location":"api/websocket/#authentication","title":"Authentication","text":"<p>Authentication is performed via the <code>token</code> query parameter. Both access tokens and CLI tokens are accepted.</p> <pre><code>const token = localStorage.getItem(\"access_token\");\nconst ws = new WebSocket(`wss://example.com/ws?token=${token}`);\n</code></pre> <p>Close codes for auth failures:</p> Code Reason 4001 Missing token, invalid token, expired token, or user not found 4008 Too many connections (max per user exceeded)"},{"location":"api/websocket/#multi-tab-support","title":"Multi-Tab Support","text":"<p>Each user can have up to 5 simultaneous WebSocket connections. All tabs receive identical messages. This enables:</p> <ul> <li>Starting a benchmark in one tab and monitoring it in another</li> <li>Closing and reopening the browser without losing process visibility</li> <li>Multiple dashboard views receiving updates simultaneously</li> </ul> <p>If a 6th connection is attempted, it is rejected with close code <code>4008</code>.</p>"},{"location":"api/websocket/#connection-lifecycle","title":"Connection Lifecycle","text":"<pre><code>Client                          Server\n  |                                |\n  |--- WebSocket connect ---------&gt;|  (with ?token=JWT)\n  |                                |  Validate JWT\n  |                                |  Check connection limit\n  |&lt;-- accept / close(4001/4008) --|\n  |                                |\n  |&lt;-- sync (active + recent) -----|  Initial state sync\n  |&lt;-- *_init (per running job) ---|  Reconnect init events\n  |                                |\n  |--- ping ----------------------&gt;|  Client keepalive\n  |&lt;-- pong -----------------------|\n  |                                |\n  |&lt;-- job_created/started/... ----|  Job lifecycle events\n  |&lt;-- job_progress ---------------|  Progress updates\n  |&lt;-- job_completed/failed -------|  Terminal events\n  |                                |\n  |--- cancel --------------------&gt;|  Cancel a job\n  |&lt;-- job_cancelled --------------|\n  |                                |\n  |--- (silence for 90s) --------&gt;|\n  |&lt;-- close(4002, timeout) -------|  Server-side timeout\n</code></pre>"},{"location":"api/websocket/#keepalive","title":"Keepalive","text":"<p>The server enforces a 90-second receive timeout. If no message is received from the client within 90 seconds, the connection is closed with code <code>4002</code> (\"Receive timeout\"). This catches dead connections from unclean proxy disconnects (e.g., Cloudflare closing without sending a close frame).</p> <p>Clients should send a <code>ping</code> message at least every 60 seconds:</p> <pre><code>{\"type\": \"ping\"}\n</code></pre> <p>Server responds with:</p> <pre><code>{\"type\": \"pong\"}\n</code></pre>"},{"location":"api/websocket/#auto-reconnect","title":"Auto-Reconnect","text":"<p>When a connection drops, clients should implement exponential backoff reconnection:</p> <pre><code>let reconnectDelay = 1000; // Start at 1 second\nconst maxDelay = 30000;    // Cap at 30 seconds\n\nfunction reconnect() {\n  setTimeout(() =&gt; {\n    const ws = new WebSocket(`wss://example.com/ws?token=${getToken()}`);\n    ws.onopen = () =&gt; { reconnectDelay = 1000; };\n    ws.onclose = () =&gt; {\n      reconnectDelay = Math.min(reconnectDelay * 2, maxDelay);\n      reconnect();\n    };\n  }, reconnectDelay);\n}\n</code></pre> <p>On reconnect, the server automatically:</p> <ol> <li>Sends a <code>sync</code> message with current active and recent jobs</li> <li>Re-sends <code>*_init</code> events for any currently running jobs (so progress tracking can resume)</li> </ol>"},{"location":"api/websocket/#message-format","title":"Message Format","text":"<p>All messages are JSON objects with a <code>type</code> field.</p>"},{"location":"api/websocket/#server-to-client-messages","title":"Server-to-Client Messages","text":""},{"location":"api/websocket/#sync","title":"sync","text":"<p>Sent immediately after connection. Contains the user's active and recent jobs.</p> <pre><code>{\n  \"type\": \"sync\",\n  \"active_jobs\": [\n    {\n      \"id\": \"abc123\",\n      \"job_type\": \"benchmark\",\n      \"status\": \"running\",\n      \"progress_pct\": 45,\n      \"progress_detail\": \"Benchmark: 3 models, 2 runs each\",\n      \"created_at\": \"2026-02-20T10:00:00+00:00\",\n      \"started_at\": \"2026-02-20T10:00:01+00:00\"\n    }\n  ],\n  \"recent_jobs\": [\n    {\n      \"id\": \"def456\",\n      \"job_type\": \"tool_eval\",\n      \"status\": \"done\",\n      \"result_ref\": \"eval-run-id\",\n      \"completed_at\": \"2026-02-20T09:55:00+00:00\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/websocket/#job_created","title":"job_created","text":"<p>A new job has been registered.</p> <pre><code>{\n  \"type\": \"job_created\",\n  \"job_id\": \"abc123\",\n  \"job_type\": \"benchmark\",\n  \"status\": \"pending\",\n  \"progress_detail\": \"Benchmark: 3 models, 2 runs each\",\n  \"created_at\": \"2026-02-20T10:00:00+00:00\"\n}\n</code></pre> <p>If the user is at their concurrency limit, <code>status</code> will be <code>\"queued\"</code> instead of <code>\"pending\"</code>.</p>"},{"location":"api/websocket/#job_started","title":"job_started","text":"<p>Job execution has begun (transitioned from pending/queued to running).</p> <pre><code>{\n  \"type\": \"job_started\",\n  \"job_id\": \"abc123\",\n  \"job_type\": \"benchmark\"\n}\n</code></pre>"},{"location":"api/websocket/#job_progress","title":"job_progress","text":"<p>Periodic progress update for a running job.</p> <pre><code>{\n  \"type\": \"job_progress\",\n  \"job_id\": \"abc123\",\n  \"progress_pct\": 45,\n  \"progress_detail\": \"GPT-4o: Run 2/3, Context 5K\"\n}\n</code></pre>"},{"location":"api/websocket/#job_completed","title":"job_completed","text":"<p>Job finished successfully.</p> <pre><code>{\n  \"type\": \"job_completed\",\n  \"job_id\": \"abc123\",\n  \"result_ref\": \"benchmark-run-id\"\n}\n</code></pre> <p>The <code>result_ref</code> is the ID of the created resource (benchmark run, eval run, tune run, or judge report).</p>"},{"location":"api/websocket/#job_failed","title":"job_failed","text":"<p>Job encountered an error.</p> <pre><code>{\n  \"type\": \"job_failed\",\n  \"job_id\": \"abc123\",\n  \"error\": \"API key invalid for provider openai\"\n}\n</code></pre> <p>Error messages are truncated to 500 characters. API keys are sanitized from error messages.</p>"},{"location":"api/websocket/#job_cancelled","title":"job_cancelled","text":"<p>Job was cancelled by the user or admin.</p> <pre><code>{\n  \"type\": \"job_cancelled\",\n  \"job_id\": \"abc123\"\n}\n</code></pre>"},{"location":"api/websocket/#benchmark-specific-events","title":"Benchmark-Specific Events","text":"<p>These events are sent in addition to the generic job lifecycle events during benchmark execution.</p> <p>benchmark_init -- Sent when a benchmark job starts (and on reconnect for running benchmarks):</p> <pre><code>{\n  \"type\": \"benchmark_init\",\n  \"job_id\": \"abc123\",\n  \"reconnect\": false,\n  \"data\": {\n    \"targets\": [\n      { \"provider_key\": \"openai\", \"model_id\": \"gpt-4o\" },\n      { \"provider_key\": \"anthropic\", \"model_id\": \"anthropic/claude-sonnet-4-5\" }\n    ],\n    \"runs\": 3,\n    \"context_tiers\": [0, 5000],\n    \"max_tokens\": 512\n  }\n}\n</code></pre> <p>On reconnect, <code>reconnect</code> is <code>true</code> and <code>progress_pct</code> is included.</p> <p>benchmark_result -- Individual run result:</p> <pre><code>{\n  \"type\": \"benchmark_result\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"model\": \"gpt-4o\",\n    \"provider\": \"OpenAI\",\n    \"tokens_per_second\": 142.5,\n    \"ttft_ms\": 234,\n    \"cost\": 0.00125,\n    \"context_tokens\": 5000,\n    \"output_tokens\": 512,\n    \"success\": true\n  }\n}\n</code></pre> <p>benchmark_skipped -- Context tier too large for model:</p> <pre><code>{\n  \"type\": \"benchmark_skipped\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"model\": \"gpt-4o-mini\",\n    \"context_tier\": 200000,\n    \"reason\": \"Context tier exceeds model window\"\n  }\n}\n</code></pre>"},{"location":"api/websocket/#tool-eval-events","title":"Tool Eval Events","text":"<p>tool_eval_init -- Sent when a tool eval starts:</p> <pre><code>{\n  \"type\": \"tool_eval_init\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"targets\": [\n      { \"provider_key\": \"openai\", \"model_id\": \"gpt-4o\" }\n    ],\n    \"total_cases\": 12,\n    \"suite_name\": \"Weather API Suite\"\n  }\n}\n</code></pre> <p>tool_eval_result -- Per-model per-case result:</p> <pre><code>{\n  \"type\": \"tool_eval_result\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"model_id\": \"gpt-4o\",\n    \"test_case_id\": \"case-1\",\n    \"actual_tool\": \"get_weather\",\n    \"actual_params\": { \"city\": \"Paris\" },\n    \"tool_selection_score\": 1.0,\n    \"param_accuracy\": 1.0,\n    \"overall_score\": 1.0,\n    \"latency_ms\": 345\n  }\n}\n</code></pre>"},{"location":"api/websocket/#param-tune-events","title":"Param Tune Events","text":"<p>tune_start -- Sent when param/prompt tuning starts:</p> <pre><code>{\n  \"type\": \"tune_start\",\n  \"job_id\": \"abc123\",\n  \"tune_id\": \"tune-run-id\",\n  \"total_combos\": 12,\n  \"models\": [\"gpt-4o\"],\n  \"suite_name\": \"Weather API Suite\"\n}\n</code></pre> <p>tune_combo_result -- Result for one parameter combination:</p> <pre><code>{\n  \"type\": \"tune_combo_result\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"combo_index\": 3,\n    \"model_id\": \"gpt-4o\",\n    \"params\": { \"temperature\": 0.3, \"top_p\": 0.9 },\n    \"score\": 0.85,\n    \"case_results\": [\n      {\n        \"test_case_id\": \"case-1\",\n        \"tool_selection_score\": 1.0,\n        \"param_accuracy\": 0.8,\n        \"overall_score\": 0.9\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"api/websocket/#prompt-tune-events","title":"Prompt Tune Events","text":"<p>tune_start -- Same format as param tune.</p> <p>prompt_eval_result -- Result for one prompt variation:</p> <pre><code>{\n  \"type\": \"prompt_eval_result\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"prompt_index\": 2,\n    \"style\": \"structured\",\n    \"score\": 0.92,\n    \"prompt_preview\": \"You are a tool-calling assistant. Follow these rules...\"\n  }\n}\n</code></pre>"},{"location":"api/websocket/#judge-events","title":"Judge Events","text":"<p>judge_verdict -- Verdict for a single test case:</p> <pre><code>{\n  \"type\": \"judge_verdict\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"test_case_id\": \"case-1\",\n    \"quality_score\": 4,\n    \"verdict\": \"pass\",\n    \"summary\": \"Correct tool with accurate parameters\"\n  }\n}\n</code></pre> <p>judge_report -- Cross-case analysis complete:</p> <pre><code>{\n  \"type\": \"judge_report\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"overall_grade\": \"B+\",\n    \"overall_score\": 82,\n    \"strengths\": [\"Consistent tool selection\"],\n    \"weaknesses\": [\"Occasional parameter omission\"]\n  }\n}\n</code></pre> <p>compare_case -- Per-case comparison result (judge compare):</p> <pre><code>{\n  \"type\": \"compare_case\",\n  \"job_id\": \"abc123\",\n  \"data\": {\n    \"case_num\": 3,\n    \"winner\": \"model_a\",\n    \"confidence\": 0.85,\n    \"reasoning\": \"Model A provided more complete parameters\"\n  }\n}\n</code></pre>"},{"location":"api/websocket/#client-to-server-messages","title":"Client-to-Server Messages","text":""},{"location":"api/websocket/#ping","title":"ping","text":"<p>Keepalive message. Send at least every 60 seconds.</p> <pre><code>{\"type\": \"ping\"}\n</code></pre>"},{"location":"api/websocket/#cancel","title":"cancel","text":"<p>Request cancellation of a running job.</p> <pre><code>{\n  \"type\": \"cancel\",\n  \"job_id\": \"abc123\"\n}\n</code></pre> <p>The server delegates to the JobRegistry, which signals the cancel event to the running task. A <code>job_cancelled</code> message is sent when cancellation completes.</p>"},{"location":"api/websocket/#connection-close-codes","title":"Connection Close Codes","text":"Code Reason Action 1000 Normal closure No action needed 4001 Missing/invalid/expired token Re-authenticate and reconnect 4002 Receive timeout (90s silence) Reconnect with fresh token 4008 Too many connections (max 5) Close another tab or wait"},{"location":"api/websocket/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>The WebSocket endpoint is at <code>/ws</code> (not under <code>/api/</code>)</li> <li>Messages are JSON encoded via <code>ws.send_json()</code> / <code>ws.receive_json()</code></li> <li>Dead connections are automatically cleaned up when <code>send_json</code> fails</li> <li>The ConnectionManager tracks connections per user with an async lock for thread safety</li> <li>Admin users receive <code>broadcast_to_admins</code> messages for system-wide events</li> <li>All job status transitions are validated against a state machine (see JobRegistry)</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>LLM Benchmark Studio uses a YAML-based configuration system. Each user gets their own configuration stored in the database, initialized from a default template on first login.</p>"},{"location":"getting-started/configuration/#configuration-structure","title":"Configuration Structure","text":"<p>The configuration has three top-level sections:</p> <pre><code>defaults:\n  max_tokens: 512\n  temperature: 0.7\n  context_tiers: [0]\n  prompt: \"Explain the concept of recursion in programming...\"\n\nprompt_templates:\n  recursion:\n    category: reasoning\n    label: Explain Recursion\n    prompt: \"Explain the concept of recursion in programming...\"\n\nproviders:\n  openai:\n    display_name: OpenAI\n    api_key_env: OPENAI_API_KEY\n    models:\n      - id: gpt-4o\n        display_name: GPT-4o\n        context_window: 128000\n</code></pre>"},{"location":"getting-started/configuration/#defaults","title":"Defaults","text":"Field Type Description <code>max_tokens</code> int Default maximum output tokens (1-16384) <code>temperature</code> float Default sampling temperature (0.0-2.0) <code>context_tiers</code> list[int] Token counts for context window testing <code>prompt</code> string Default benchmark prompt"},{"location":"getting-started/configuration/#prompt-templates","title":"Prompt Templates","text":"<p>Named prompt templates organized by category:</p> <pre><code>prompt_templates:\n  my_template:\n    category: reasoning    # Category for grouping\n    label: My Template     # Display name in the UI\n    prompt: \"Your prompt text here...\"\n</code></pre> <p>Categories include: <code>reasoning</code>, <code>code</code>, <code>creative</code>, <code>short_qa</code>, <code>general</code>.</p>"},{"location":"getting-started/configuration/#providers","title":"Providers","text":"<p>Each provider configures an LLM API endpoint:</p> <pre><code>providers:\n  provider_key:\n    display_name: Provider Name    # Shown in the UI\n    api_key_env: API_KEY_ENV_VAR   # Environment variable for the API key\n    api_base: https://api.example.com/v1  # Optional: custom API base URL\n    api_key: literal-key           # Optional: direct API key (not recommended)\n    model_id_prefix: provider      # Optional: LiteLLM model prefix\n    models:\n      - id: provider/model-name\n        display_name: Model Name\n        context_window: 128000\n        max_output_tokens: 8000    # Optional: max output token limit\n        skip_params:               # Optional: params to omit from API calls\n          - temperature\n        input_cost_per_mtok: 3.0   # Optional: custom $/1M input tokens\n        output_cost_per_mtok: 15.0 # Optional: custom $/1M output tokens\n</code></pre>"},{"location":"getting-started/configuration/#provider-fields","title":"Provider Fields","text":"Field Required Description <code>display_name</code> Yes Human-readable name <code>api_key_env</code> No Environment variable name for the API key <code>api_base</code> No Custom API base URL (for local/self-hosted models) <code>api_key</code> No Direct API key value <code>model_id_prefix</code> No LiteLLM prefix (e.g., <code>anthropic</code>, <code>gemini</code>) <code>models</code> Yes List of model configurations"},{"location":"getting-started/configuration/#model-fields","title":"Model Fields","text":"Field Required Description <code>id</code> Yes LiteLLM model identifier <code>display_name</code> Yes Human-readable name <code>context_window</code> No Max context tokens (default: 128000) <code>max_output_tokens</code> No Max output tokens for this model <code>skip_params</code> No Parameters to exclude from API calls <code>input_cost_per_mtok</code> No Custom input cost per million tokens <code>output_cost_per_mtok</code> No Custom output cost per million tokens"},{"location":"getting-started/configuration/#litellm-model-id-conventions","title":"LiteLLM Model ID Conventions","text":"<p>All LLM calls go through LiteLLM. Model IDs follow LiteLLM conventions:</p> Provider Model ID Format Example OpenAI <code>model-name</code> (no prefix) <code>gpt-4o</code> Anthropic <code>anthropic/model-name</code> <code>anthropic/claude-sonnet-4-5</code> Google Gemini <code>gemini/model-name</code> <code>gemini/gemini-2.5-flash</code> LM Studio <code>lm_studio/model-name</code> <code>lm_studio/my-model</code> Ollama <code>ollama/model-name</code> <code>ollama/llama3</code> Custom OpenAI-compatible <code>openai/model-name</code> with <code>api_base</code> Any"},{"location":"getting-started/configuration/#managing-configuration-via-the-ui","title":"Managing Configuration via the UI","text":""},{"location":"getting-started/configuration/#adding-a-provider","title":"Adding a Provider","text":"<ol> <li>Navigate to Configuration</li> <li>Click Add Provider</li> <li>Fill in the provider key, display name, and API settings</li> <li>Save</li> </ol>"},{"location":"getting-started/configuration/#adding-a-model","title":"Adding a Model","text":"<ol> <li>Open the provider in the Configuration screen</li> <li>Click Add Model</li> <li>Enter the model ID (following LiteLLM conventions), display name, and context window</li> <li>Save</li> </ol>"},{"location":"getting-started/configuration/#model-discovery","title":"Model Discovery","text":"<p>For providers that support it, click Discover Models to fetch available models from the provider's API. This works with:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Google Gemini</li> <li>Any OpenAI-compatible endpoint (LM Studio, Ollama, vLLM)</li> </ul>"},{"location":"getting-started/configuration/#managing-configuration-via-the-api","title":"Managing Configuration via the API","text":"<p>All configuration changes are made through the REST API:</p> <pre><code># Get current configuration\ncurl -H \"Authorization: Bearer $TOKEN\" http://localhost:8501/api/config\n\n# Add a provider\ncurl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"provider_key\": \"my_provider\", \"display_name\": \"My Provider\", \"api_base\": \"http://localhost:1234/v1\"}' \\\n  http://localhost:8501/api/config/provider\n\n# Add a model to a provider\ncurl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"provider_key\": \"my_provider\", \"id\": \"my-model\", \"display_name\": \"My Model\", \"context_window\": 32000}' \\\n  http://localhost:8501/api/config/model\n</code></pre> <p>See the REST API Reference for all configuration endpoints.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>These environment variables control application behavior:</p> Variable Default Description <code>JWT_SECRET</code> Auto-generated Secret key for JWT token signing. Changes on restart if not set. <code>ADMIN_EMAIL</code> (none) Auto-promote this email to admin on startup (or create the account if it does not exist) <code>ADMIN_PASSWORD</code> (none) Password for auto-created admin account (requires <code>ADMIN_EMAIL</code>) <code>FERNET_KEY</code> Auto-generated Master encryption key for API key storage. Stored in <code>data/.fernet_key</code> if not set. <code>COOKIE_SECURE</code> <code>false</code> Set to <code>true</code> for HTTPS deployments (controls <code>Secure</code> flag on refresh token cookie) <code>CORS_ORIGINS</code> (none) Comma-separated list of allowed CORS origins <code>LOG_LEVEL</code> <code>warning</code> Uvicorn log level (<code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>) <code>LOG_ACCESS_TOKEN</code> (none) Static token for accessing <code>/api/admin/logs</code> without admin JWT <code>APP_VERSION</code> <code>dev</code> Application version string (set automatically by Docker build)"},{"location":"getting-started/configuration/#local-llm-configuration","title":"Local LLM Configuration","text":"<p>To benchmark local models running on LM Studio, Ollama, or vLLM:</p> <pre><code>providers:\n  lm_studio:\n    display_name: LM Studio (Local)\n    api_base: http://localhost:1234/v1\n    api_key: not-needed\n    model_id_prefix: lm_studio\n    models:\n      - id: lm_studio/my-local-model\n        display_name: My Local Model\n        context_window: 32000\n</code></pre> <p>Network Access</p> <p>If running the application in Docker and the local LLM server is on the host machine, use <code>host.docker.internal</code> instead of <code>localhost</code> for the <code>api_base</code> URL.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>LLM Benchmark Studio can be run locally for development or deployed with Docker for production.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher (3.13 recommended)</li> <li>uv package manager (recommended) or pip</li> <li>Node.js 18+ and npm (for building the frontend)</li> <li>At least one LLM provider API key (OpenAI, Anthropic, Google Gemini, etc.)</li> </ul>"},{"location":"getting-started/installation/#local-installation","title":"Local Installation","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/maheidem/llm-benchmark-studio.git\ncd llm-benchmark-studio\n</code></pre>"},{"location":"getting-started/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>Using uv (recommended):</p> <pre><code>uv sync\n</code></pre> <p>Using pip:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Dependencies</p> <p>The project uses <code>pyproject.toml</code> with uv lockfile. Key dependencies include:</p> <ul> <li><code>litellm</code> -- Unified LLM API</li> <li><code>fastapi</code> + <code>uvicorn</code> -- Web framework</li> <li><code>aiosqlite</code> -- Async SQLite</li> <li><code>bcrypt</code> + <code>python-jose</code> -- Authentication</li> <li><code>cryptography</code> -- API key encryption</li> <li><code>tiktoken</code> -- Token counting</li> <li><code>rich</code> -- CLI output</li> <li><code>httpx</code> -- HTTP client</li> <li><code>mcp</code> -- Model Context Protocol client</li> </ul>"},{"location":"getting-started/installation/#3-build-the-frontend","title":"3. Build the Frontend","text":"<p>The web dashboard is a Vue 3 SPA that must be compiled before running the server:</p> <pre><code>cd frontend &amp;&amp; npm ci &amp;&amp; npm run build\n</code></pre> <p>This compiles the frontend into the <code>static/</code> directory, which FastAPI serves in production.</p> <p>Docker handles this automatically</p> <p>If you use Docker Compose, the multi-stage Dockerfile builds the frontend automatically. You only need this step for local development without Docker.</p>"},{"location":"getting-started/installation/#4-configure-environment","title":"4. Configure Environment","text":"<pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code> with your API keys:</p> <pre><code># LLM Provider API Keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=AIza...\n\n# Authentication\nJWT_SECRET=change-this-to-a-random-string\n\n# Optional: Auto-create admin account on first startup\n# ADMIN_EMAIL=admin@example.com\n# ADMIN_PASSWORD=changeme123\n\n# Optional: Encryption key (auto-generated if not set)\n# FERNET_KEY=your-base64-fernet-key-here\n\n# Deployment\nCOOKIE_SECURE=false\nLOG_LEVEL=warning\n</code></pre>"},{"location":"getting-started/installation/#5-start-the-application","title":"5. Start the Application","text":"<pre><code>python app.py\n</code></pre> <p>The web dashboard is available at <code>http://localhost:8501</code>.</p> <p>Custom host and port:</p> <pre><code>python app.py --host 0.0.0.0 --port 3333\n</code></pre>"},{"location":"getting-started/installation/#frontend-development-mode","title":"Frontend Development Mode","text":"<p>For frontend development with hot-reload, run the Vite dev server alongside the backend:</p> <pre><code># Terminal 1: Backend\npython app.py\n\n# Terminal 2: Frontend dev server\ncd frontend &amp;&amp; npm install &amp;&amp; npm run dev\n</code></pre> <p>The Vite dev server proxies API requests to the backend.</p>"},{"location":"getting-started/installation/#docker-installation","title":"Docker Installation","text":"<p>See Docker Setup for the full Docker guide.</p> <p>Quick start with Docker Compose:</p> <pre><code># Configure environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# Start the application\ndocker compose up -d\n</code></pre> <p>Docker handles the frontend build automatically via a multi-stage Dockerfile (Node.js builds the Vue 3 SPA, then Python serves it).</p> <p>The application will be available at <code>http://localhost:8501</code>.</p>"},{"location":"getting-started/installation/#first-time-setup","title":"First-Time Setup","text":"<ol> <li>Open the dashboard at <code>http://localhost:8501</code></li> <li>Register a new account (the first user is automatically promoted to admin)</li> <li>Complete the onboarding wizard to add your API keys</li> <li>You are ready to run benchmarks</li> </ol> <p>Admin Account</p> <p>The first user to register always gets the <code>admin</code> role. Alternatively, set <code>ADMIN_EMAIL</code> and <code>ADMIN_PASSWORD</code> in your <code>.env</code> file to auto-create an admin account on startup.</p>"},{"location":"getting-started/installation/#cli-usage","title":"CLI Usage","text":"<p>The CLI tool (<code>benchmark.py</code>) can run benchmarks without the web dashboard:</p> <pre><code>python benchmark.py                          # All providers, all models\npython benchmark.py --provider openai        # Filter by provider\npython benchmark.py --model GPT              # Filter by model name\npython benchmark.py --runs 3                 # Average over 3 runs\npython benchmark.py --no-save                # Skip saving results\n</code></pre> <p>See Running Benchmarks for full CLI documentation.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that the application is running:</p> <pre><code>curl http://localhost:8501/healthz\n</code></pre> <p>Expected response:</p> <pre><code>{\"status\": \"ok\", \"version\": \"dev\"}\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through running your first benchmark after installation.</p>"},{"location":"getting-started/quickstart/#1-register-and-log-in","title":"1. Register and Log In","text":"<p>Open <code>http://localhost:8501</code> in your browser. You will see the login screen.</p> <ol> <li>Click Register to create a new account</li> <li>Enter your email and a password (minimum 8 characters)</li> <li>The first user is automatically promoted to admin</li> </ol>"},{"location":"getting-started/quickstart/#2-add-api-keys","title":"2. Add API Keys","text":"<p>After registering, the onboarding wizard guides you through adding API keys.</p> <p>You can add keys for any provider in your configuration:</p> <ol> <li>Navigate to the API Keys section</li> <li>Select a provider (e.g., OpenAI, Anthropic, Google Gemini)</li> <li>Paste your API key</li> <li>Click Save</li> </ol> <p>Keys are encrypted with Fernet symmetric encryption and stored per-user in the database. Each user manages their own keys independently.</p> <p>Key Priority</p> <p>Per-user keys take priority over global environment variables. Admin-set <code>.env</code> keys serve as fallbacks.</p>"},{"location":"getting-started/quickstart/#3-run-a-benchmark","title":"3. Run a Benchmark","text":"<ol> <li>Navigate to the Benchmark screen</li> <li>Select one or more models from the provider list</li> <li>Configure parameters:<ul> <li>Prompt: The text to send to each model (or select a template)</li> <li>Runs: Number of iterations per model (1-20, default 3)</li> <li>Max Tokens: Maximum output tokens (1-16384, default 512)</li> <li>Temperature: Sampling temperature (0.0-2.0, default 0.7)</li> <li>Context Tiers: Token counts for context window testing (default: [0])</li> <li>Warmup: Enable/disable warmup run (discarded, reduces cold-start variance)</li> </ul> </li> <li>Click Run Benchmark</li> </ol> <p>Results stream in real-time via WebSocket:</p> <ul> <li>Progress: Shows current run number, model, and context tier</li> <li>Results: Tokens/sec, TTFT, total time, cost per individual run</li> <li>Summary: Aggregated statistics across all runs</li> <li>Process Tracker: Active jobs appear in the notification area, so you can navigate away and return without losing progress</li> </ul>"},{"location":"getting-started/quickstart/#4-view-results","title":"4. View Results","text":"<p>After the benchmark completes:</p> <ul> <li>Results Table: Shows per-model averages for tokens/sec, TTFT, cost</li> <li>Charts: Visual comparison across models (bar charts, scatter plots)</li> <li>History: All runs are saved and accessible from the History page</li> </ul> <p>SPA Navigation</p> <p>LLM Benchmark Studio is a Vue 3 single-page application. Navigation between pages (Benchmark, History, Analytics, etc.) is instant -- there are no full page reloads.</p>"},{"location":"getting-started/quickstart/#5-run-a-tool-eval","title":"5. Run a Tool Eval","text":"<p>Tool calling evaluation tests whether models correctly use function calling.</p> <ol> <li>Navigate to the Tool Eval screen</li> <li>Create a tool suite or import one from JSON</li> <li>Define tools using OpenAI function calling schema</li> <li>Add test cases (prompt + expected tool + expected parameters)</li> <li>Select models and click Run Eval</li> </ol> <p>Each test case scores:</p> <ul> <li>Tool Selection: Did the model call the correct tool? (0% or 100%)</li> <li>Parameter Accuracy: Did the model pass the correct parameters? (0-100%)</li> <li>Overall Score: Weighted combination (60% tool + 40% params)</li> </ul> <p>See Tool Calling Evaluation for the full guide.</p>"},{"location":"getting-started/quickstart/#6-explore-analytics","title":"6. Explore Analytics","text":"<p>The Analytics page has three tabs:</p> <ul> <li>Leaderboard: Models ranked by tokens/sec (benchmark) or overall score (tool eval), filterable by time period (7d, 30d, 90d, all)</li> <li>Compare: Select 2-4 benchmark runs for side-by-side comparison with charts</li> <li>Trends: Track model performance (tokens/sec and TTFT) over time with multi-model selection</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration -- Customize providers, models, and defaults</li> <li>Tool Calling Evaluation -- Deep dive into tool eval</li> <li>Judge System -- Use LLMs to evaluate other LLMs</li> <li>API Reference -- Integrate via REST API</li> </ul>"},{"location":"guide/analytics/","title":"Analytics &amp; History","text":"<p>LLM Benchmark Studio provides analytics features for comparing models, tracking performance over time, and exporting data.</p>"},{"location":"guide/analytics/#benchmark-history","title":"Benchmark History","text":"<p>All benchmark runs are saved per-user in the database. The History screen shows:</p> <ul> <li>Run timestamp</li> <li>Prompt used</li> <li>Models benchmarked</li> <li>Context tiers</li> <li>Per-model results (tokens/sec, TTFT, cost)</li> </ul>"},{"location":"guide/analytics/#viewing-a-run","title":"Viewing a Run","text":"<p>Click any run to see full details including individual iteration results, aggregated statistics, and charts.</p>"},{"location":"guide/analytics/#deleting-a-run","title":"Deleting a Run","text":"<p>Runs can be deleted individually from the history view or via API:</p> <pre><code>curl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/history/{run_id}\n</code></pre>"},{"location":"guide/analytics/#leaderboard","title":"Leaderboard","text":"<p>The leaderboard ranks models across all your benchmark or tool eval runs. It supports two leaderboard types and four time periods.</p> <pre><code># Benchmark leaderboard (default, ranked by avg tokens/sec)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/analytics/leaderboard?type=benchmark&amp;period=all\"\n\n# Tool eval leaderboard (ranked by avg overall score)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/analytics/leaderboard?type=tool_eval&amp;period=30d\"\n</code></pre> <p>Benchmark leaderboard returns per-model:</p> <ul> <li><code>avg_tps</code>: Average tokens per second</li> <li><code>avg_ttft_ms</code>: Average time to first token (ms)</li> <li><code>avg_cost</code>: Average cost per run</li> <li><code>total_runs</code>: Number of runs included</li> </ul> <p>Tool eval leaderboard returns per-model:</p> <ul> <li><code>avg_tool_pct</code>: Average tool selection accuracy (%)</li> <li><code>avg_param_pct</code>: Average parameter accuracy (%)</li> <li><code>avg_overall_pct</code>: Average overall score (%)</li> <li><code>total_evals</code>: Number of evals included</li> </ul> <p>Period filter: <code>7d</code>, <code>30d</code>, <code>90d</code>, <code>all</code></p>"},{"location":"guide/analytics/#trend-analysis","title":"Trend Analysis","text":"<p>Track model performance over time by selecting one or more models:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/analytics/trends?models=GPT-4o,Claude+Sonnet+4.5&amp;metric=tps&amp;period=30d\"\n</code></pre> <p>Parameters:</p> Parameter Required Description <code>models</code> Yes Comma-separated model display names <code>metric</code> No <code>tps</code> (default) or <code>ttft</code> <code>period</code> No <code>7d</code>, <code>30d</code>, <code>90d</code>, or <code>all</code> (default) <p>Returns time-series data points per model, displayed as line charts in the Trends tab.</p>"},{"location":"guide/analytics/#run-comparison","title":"Run Comparison","text":"<p>Compare 2-4 specific benchmark runs side by side:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/analytics/compare?runs=run_id_1,run_id_2\"\n</code></pre> <p>Select runs from your history (up to 4). Returns per-run model results including avg tokens/sec, avg TTFT, context tokens, and cost. The Compare tab displays this data as side-by-side bar charts.</p>"},{"location":"guide/analytics/#tool-eval-history","title":"Tool Eval History","text":"<p>Tool eval runs are saved separately:</p> <pre><code># List eval runs\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/history\n\n# Get a specific eval run with full results\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/history/{eval_id}\n</code></pre> <p>Each eval run includes:</p> <ul> <li>Suite name and ID</li> <li>Models tested</li> <li>Per-case results with scores</li> <li>Per-model summaries (tool accuracy, param accuracy, overall)</li> <li>Judge verdicts (if Judge was enabled)</li> </ul>"},{"location":"guide/analytics/#export","title":"Export","text":""},{"location":"guide/analytics/#csv-export","title":"CSV Export","text":"<p>Export benchmark history as CSV:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/history &gt; benchmarks.csv\n</code></pre> <p>Export tool eval runs as CSV:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/tool-eval &gt; evals.csv\n</code></pre> <p>Export leaderboard as CSV:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/leaderboard &gt; leaderboard.csv\n</code></pre>"},{"location":"guide/analytics/#json-export","title":"JSON Export","text":"<p>Export a specific benchmark run:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/run/{run_id} &gt; run.json\n</code></pre> <p>Export a specific eval run (includes raw request/response data):</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/eval/{eval_id} &gt; eval.json\n</code></pre>"},{"location":"guide/analytics/#settings-backup","title":"Settings Backup","text":"<p>Export your complete configuration (providers, models, prompts):</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/export/settings &gt; settings.json\n</code></pre> <p>Restore from backup:</p> <pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @settings.json \\\n  http://localhost:8501/api/import/settings\n</code></pre>"},{"location":"guide/benchmarks/","title":"Running Benchmarks","text":"<p>LLM Benchmark Studio measures token throughput (tokens/sec), time to first token (TTFT), and cost across multiple LLM providers. Benchmarks can be run through the web dashboard or the CLI.</p>"},{"location":"guide/benchmarks/#web-dashboard","title":"Web Dashboard","text":""},{"location":"guide/benchmarks/#selecting-models","title":"Selecting Models","text":"<ol> <li>Navigate to the Benchmark screen</li> <li>Models are grouped by provider (OpenAI, Anthropic, Google Gemini, etc.)</li> <li>Check the models you want to benchmark</li> <li>Models from different providers run in parallel; models within the same provider run sequentially to avoid self-contention</li> </ol>"},{"location":"guide/benchmarks/#configuring-parameters","title":"Configuring Parameters","text":"Parameter Range Default Description Prompt Text Recursion explanation The prompt sent to each model Runs 1-20 3 Number of iterations per model Max Tokens 1-16384 512 Maximum output tokens Temperature 0.0-2.0 0.7 Sampling temperature Context Tiers List of ints [0] Token counts for context testing Warmup Boolean true Run one discarded warmup iteration <p>Prompt Templates: Select from pre-defined prompts or create your own in Configuration. Templates are organized by category (reasoning, code, creative, Q&amp;A).</p> <p>Context Tiers: Test how models perform with different input sizes. The engine generates filler text (code snippets, prose, JSON, documentation) to pad the system prompt to the target token count. Models whose context window is too small for a tier automatically skip it.</p>"},{"location":"guide/benchmarks/#benchmark-execution-flow","title":"Benchmark Execution Flow","text":"<p>When you click Run Benchmark, the following happens:</p> <ol> <li>The frontend sends a <code>POST /api/benchmark</code> request with the selected models, parameters, and options</li> <li>The server validates the request, checks rate limits, and submits the job to the JobRegistry</li> <li>The API returns a <code>job_id</code> immediately (the benchmark runs in the background)</li> <li>Real-time progress and results are delivered via WebSocket events</li> </ol> <pre><code>// POST /api/benchmark response\n{\"job_id\": \"a1b2c3d4...\", \"status\": \"submitted\"}\n</code></pre>"},{"location":"guide/benchmarks/#real-time-results","title":"Real-Time Results","text":"<p>Results stream via WebSocket as each run completes. The frontend receives the following event sequence:</p> <ol> <li><code>job_created</code>: Job has been submitted to the registry</li> <li><code>benchmark_init</code>: Sent before execution begins; contains the list of targets, run count, context tiers, and max tokens so the frontend can set up per-provider progress tracking</li> <li><code>benchmark_progress</code>: Emitted before each individual run starts; includes provider, model, run number, and context tier</li> <li><code>benchmark_result</code>: Individual run metrics (tokens/sec, TTFT, total time, cost, input/output tokens)</li> <li><code>job_progress</code>: Overall progress percentage and detail string (e.g., \"GPT-4o, Run 2/3\")</li> <li><code>job_completed</code>: All runs finished and results saved; includes <code>result_ref</code> (the benchmark_run ID)</li> </ol> <p>If a benchmark fails, a <code>job_failed</code> event is sent with the error message.</p>"},{"location":"guide/benchmarks/#understanding-metrics","title":"Understanding Metrics","text":"Metric Unit Description Tokens/sec tok/s Output token generation speed Input Tokens/sec tok/s Input processing speed TTFT ms Time to first token (latency) Total Time seconds End-to-end request duration Output Tokens count Number of tokens generated Input Tokens count Number of tokens in the prompt Cost USD Estimated cost per run"},{"location":"guide/benchmarks/#cancelling-a-benchmark","title":"Cancelling a Benchmark","text":"<p>Benchmarks can be cancelled through several methods:</p> <ul> <li>Cancel button: Click Cancel in the benchmark UI. This sends <code>POST /api/benchmark/cancel</code> with the <code>job_id</code></li> <li>Job tracker: Cancel from the notification widget dropdown</li> <li>WebSocket: Send <code>{\"type\": \"cancel\", \"job_id\": \"...\"}</code> over the WebSocket connection</li> </ul> <p>The system signals the cancel event, remaining provider tasks are stopped, and partial results are not saved. A <code>job_cancelled</code> WebSocket event confirms the cancellation.</p>"},{"location":"guide/benchmarks/#cli-usage","title":"CLI Usage","text":"<p>The CLI tool runs benchmarks from the terminal with Rich-formatted output:</p> <pre><code># Run all providers and models\npython benchmark.py\n\n# Filter by provider\npython benchmark.py --provider openai\n\n# Filter by model name (substring match)\npython benchmark.py --model GPT\n\n# Custom number of runs\npython benchmark.py --runs 5\n\n# Custom prompt\npython benchmark.py --prompt \"Write a haiku about programming\"\n\n# Custom context tiers\npython benchmark.py --context-tiers 0,5000,50000\n\n# Adjust output parameters\npython benchmark.py --max-tokens 1024 --temperature 0.5\n\n# Skip saving results to JSON file\npython benchmark.py --no-save\n\n# Enable LiteLLM debug logging\npython benchmark.py --verbose\n</code></pre> <p>CLI results are saved as timestamped JSON files in the <code>results/</code> directory.</p>"},{"location":"guide/benchmarks/#concurrency-model","title":"Concurrency Model","text":"<p>The benchmark engine uses an asyncio-based concurrency model managed by the JobRegistry:</p> <ol> <li>Provider groups execute in parallel via <code>asyncio.create_task()</code></li> <li>Models within a provider run sequentially (avoids API self-contention)</li> <li>Results flow through an <code>asyncio.Queue</code> to the handler, which broadcasts them via WebSocket</li> <li>Per-user concurrency is managed by the JobRegistry (configurable limit, default 1). Additional benchmark submissions are queued rather than rejected</li> <li>Job progress updates are persisted to the database and broadcast to all connected tabs</li> </ol>"},{"location":"guide/benchmarks/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Default: 2000 benchmark executions per user per hour</li> <li>Configurable via <code>BENCHMARK_RATE_LIMIT</code> environment variable</li> <li>Per-user rate limits can be set by admins</li> </ul>"},{"location":"guide/benchmarks/#results-storage","title":"Results Storage","text":"<p>Benchmark results are saved in two places:</p> <ol> <li>Database: Per-user <code>benchmark_runs</code> table with full results JSON</li> <li>JSON files: Timestamped files in the <code>results/</code> directory (for backward compatibility)</li> </ol> <p>Results include all individual run data plus aggregated statistics (averages, standard deviation, min/max, percentiles).</p>"},{"location":"guide/benchmarks/#context-tier-testing","title":"Context Tier Testing","text":"<p>Context tiers test model performance across different input sizes:</p> <pre><code>context_tiers: [0, 1000, 5000, 10000, 50000, 100000]\n</code></pre> <p>For each tier, the engine:</p> <ol> <li>Generates filler text to pad the input to the target token count</li> <li>Checks if the tier fits within the model's context window (with headroom for max_tokens + 100)</li> <li>Skips the tier if it exceeds the model's capacity</li> <li>Runs the specified number of iterations</li> </ol> <p>The filler text alternates between diverse content types (Python code, prose, JSON data, technical documentation, networking concepts) to simulate realistic workloads.</p>"},{"location":"guide/benchmarks/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>The Provider Parameter Registry handles provider-specific parameter rules:</p> <ul> <li>Temperature clamping: GPT-5 locks to 1.0; Gemini 3 clamps minimum to 1.0</li> <li>Skip params: Some models (like Anthropic's Claude) skip the temperature parameter</li> <li>Conflict resolution: Automatic handling of parameter conflicts (e.g., Anthropic's temperature + top_p restriction)</li> </ul>"},{"location":"guide/judge/","title":"Judge System","text":"<p>The Judge System uses one LLM to evaluate another LLM's tool calling performance. It provides qualitative analysis beyond the numerical scoring of the eval engine.</p>"},{"location":"guide/judge/#overview","title":"Overview","text":"<p>After running a tool evaluation, the Judge reviews each test case result and provides:</p> <ul> <li>A quality score (1-5)</li> <li>A verdict (pass, marginal, or fail)</li> <li>Tool selection assessment</li> <li>Parameter assessment</li> <li>Detailed reasoning</li> </ul> <p>The Judge also generates cross-case analysis reports with overall grades and recommendations.</p>"},{"location":"guide/judge/#judge-modes","title":"Judge Modes","text":""},{"location":"guide/judge/#live-inline","title":"Live Inline","text":"<p>The Judge evaluates each result concurrently as the eval runs. Verdicts appear in real-time alongside eval results.</p> <ul> <li>Best for: Interactive evaluation sessions</li> <li>Trade-off: Adds latency to the overall eval run</li> <li>Concurrency: Controlled via <code>judge_concurrency</code> (default 4); auto-capped to 1 when the judge shares an endpoint with eval models</li> </ul>"},{"location":"guide/judge/#post-eval","title":"Post-Eval","text":"<p>The Judge reviews all results after the eval completes. This runs as a separate pass over all results.</p> <ul> <li>Best for: Batch analysis, when you want eval results first</li> <li>Trade-off: Additional time after eval finishes</li> <li>Can also be run as a standalone operation on any existing eval run</li> </ul>"},{"location":"guide/judge/#configuring-the-judge","title":"Configuring the Judge","text":""},{"location":"guide/judge/#during-eval-integrated","title":"During Eval (Integrated)","text":"<p>When running a tool eval, enable the Judge via the <code>judge</code> config:</p> <pre><code>{\n  \"suite_id\": \"...\",\n  \"models\": [\"gpt-4o\"],\n  \"judge\": {\n    \"enabled\": true,\n    \"mode\": \"post_eval\",\n    \"judge_model\": \"anthropic/claude-sonnet-4-5\",\n    \"judge_provider_key\": \"anthropic\",\n    \"custom_instructions\": \"Focus on parameter completeness.\"\n  },\n  \"judge_concurrency\": 4\n}\n</code></pre>"},{"location":"guide/judge/#standalone-post-eval-judge","title":"Standalone Post-Eval Judge","text":"<p>Run the Judge independently on an existing eval run:</p> <pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"eval_run_id\": \"eval-123\",\n    \"judge_model\": \"gpt-4o\",\n    \"judge_provider_key\": \"openai\",\n    \"custom_instructions\": \"\",\n    \"concurrency\": 4\n  }' \\\n  http://localhost:8501/api/tool-eval/judge\n</code></pre>"},{"location":"guide/judge/#custom-instructions","title":"Custom Instructions","text":"<p>You can provide custom instructions to tailor the Judge's evaluation:</p> <pre><code>Focus on parameter completeness. A tool call should be considered\na failure if any required parameter is missing, even if the tool\nselection is correct. Pay special attention to date formats.\n</code></pre>"},{"location":"guide/judge/#execution-via-jobregistry","title":"Execution via JobRegistry","text":"<p>All judge operations execute through the JobRegistry, providing background execution, cancellation, and WebSocket progress updates.</p> <p>The API returns <code>{\"job_id\": \"...\", \"status\": \"submitted\"}</code>. Progress and results are delivered via WebSocket.</p>"},{"location":"guide/judge/#websocket-event-flow-post-eval-judge","title":"WebSocket Event Flow (Post-Eval Judge)","text":"<pre><code>POST /api/tool-eval/judge  --&gt;  { job_id, status: \"submitted\" }\n                                      |\n                             WebSocket events:\n                                      |\n    judge_start  ---&gt;  judge_verdict (per case, concurrent)\n                              |\n                       job_progress (percentage)\n                              |\n                       judge_report (per model, cross-case analysis)\n                              |\n                       judge_complete\n</code></pre> <p>Key WebSocket event types:</p> Event Description <code>judge_start</code> Judge session started, includes mode, judge_model, cases_to_review <code>judge_verdict</code> Individual verdict for one test case <code>judge_report</code> Cross-case analysis report for one model <code>judge_complete</code> All verdicts and reports finished <code>job_progress</code> Progress percentage (Judge: N/M)"},{"location":"guide/judge/#judge-verdicts","title":"Judge Verdicts","text":"<p>For each test case, the Judge produces:</p> Field Type Description <code>quality_score</code> int (1-5) Overall quality rating <code>verdict</code> string <code>pass</code>, <code>marginal</code>, or <code>fail</code> <code>summary</code> string One-line summary (max 100 chars) <code>reasoning</code> string Detailed 2-3 sentence explanation <code>tool_selection_assessment</code> string <code>correct</code>, <code>acceptable_alternative</code>, or <code>wrong</code> <code>param_assessment</code> string <code>exact</code>, <code>close</code>, <code>partial</code>, or <code>wrong</code>"},{"location":"guide/judge/#cross-case-reports","title":"Cross-Case Reports","text":"<p>After evaluating all cases for a model, the Judge generates a cross-case report:</p> Field Description <code>overall_grade</code> Letter grade (A-F with +/-) <code>overall_score</code> Numeric score (0-100) <code>strengths</code> List of identified strengths <code>weaknesses</code> List of identified weaknesses <code>cross_case_analysis</code> Paragraph of pattern analysis <code>recommendations</code> List of specific improvement recommendations"},{"location":"guide/judge/#judge-compare","title":"Judge Compare","text":"<p>Compare two tool eval runs side by side using the Judge.</p>"},{"location":"guide/judge/#when-to-use","title":"When to Use","text":"<ul> <li>Comparing the same model with different parameters</li> <li>Comparing different models on the same test suite</li> <li>Before/after prompt changes</li> <li>Validating improvements from param tuning or prompt tuning</li> </ul>"},{"location":"guide/judge/#running-a-comparison","title":"Running a Comparison","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"eval_run_id_a\": \"eval-run-A\",\n    \"eval_run_id_b\": \"eval-run-B\",\n    \"judge_model\": \"gpt-4o\",\n    \"judge_provider_key\": \"openai\",\n    \"concurrency\": 4\n  }' \\\n  http://localhost:8501/api/tool-eval/judge/compare\n</code></pre> <p>Both eval runs must share at least one common test case. The Judge evaluates only the intersection of test cases.</p>"},{"location":"guide/judge/#compare-websocket-events","title":"Compare WebSocket Events","text":"Event Description <code>compare_start</code> Comparison started, includes model names and case count <code>compare_case</code> Per-case comparison result with winner, confidence, reasoning <code>compare_complete</code> Summary with overall_winner, score_a, score_b <code>job_progress</code> Progress percentage (Compare: N/M)"},{"location":"guide/judge/#compare-summary","title":"Compare Summary","text":"<p>The final summary includes:</p> <pre><code>{\n  \"overall_winner\": \"model_a\",\n  \"score_a\": 78,\n  \"score_b\": 65,\n  \"summary\": \"Model A outperformed Model B on parameter accuracy...\",\n  \"tie_cases\": 1\n}\n</code></pre>"},{"location":"guide/judge/#cancellation","title":"Cancellation","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"job_id\": \"abc123\"}' \\\n  http://localhost:8501/api/tool-eval/judge/cancel\n</code></pre> <p>Cancellation is cooperative and partial results are saved.</p>"},{"location":"guide/judge/#retry-and-error-handling","title":"Retry and Error Handling","text":"<p>The Judge uses exponential backoff for transient errors:</p> <ul> <li>Retries on 502, 503, 500, connection errors, and timeouts</li> <li>Up to 3 attempts with delays of 2s, 4s, 8s</li> <li>Non-transient errors (auth, 400, 404, rate-limit) propagate immediately</li> <li>Judge uses temperature 0.0 for reproducible assessments</li> <li>max_tokens set to 2048 for detailed reasoning</li> </ul>"},{"location":"guide/judge/#judge-reports-api","title":"Judge Reports API","text":"<pre><code># List judge reports\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/judge/reports\n\n# Get a specific report\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/judge/reports/{report_id}\n\n# Delete a report\ncurl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/judge/reports/{report_id}\n</code></pre>"},{"location":"guide/judge/#api-reference","title":"API Reference","text":"Method Endpoint Description <code>POST</code> <code>/api/tool-eval/judge</code> Run post-eval judge (returns job_id) <code>POST</code> <code>/api/tool-eval/judge/compare</code> Run comparative judge (returns job_id) <code>POST</code> <code>/api/tool-eval/judge/cancel</code> Cancel running judge <code>GET</code> <code>/api/tool-eval/judge/reports</code> List judge reports <code>GET</code> <code>/api/tool-eval/judge/reports/{id}</code> Get report details <code>DELETE</code> <code>/api/tool-eval/judge/reports/{id}</code> Delete report"},{"location":"guide/judge/#best-practices","title":"Best Practices","text":"<ul> <li>Use a capable model as the Judge (e.g., Claude Sonnet 4.5, GPT-4o)</li> <li>Avoid using the same model as both the test subject and the Judge</li> <li>Custom instructions help focus the Judge on what matters for your use case</li> <li>Post-eval mode is more cost-effective for large eval runs</li> <li>When the judge model shares an endpoint with eval models (common with local LLMs), concurrency is auto-capped to 1 to prevent overload</li> <li>Use comparative judge to validate improvements after param tuning or prompt tuning</li> <li>Link judge runs to experiments for unified timeline tracking</li> </ul>"},{"location":"guide/param-tuner/","title":"Param Tuner","text":"<p>The Parameter Tuner performs a grid search across parameter combinations to find the optimal settings for tool calling accuracy. Think of it as GridSearchCV for LLM tool calling.</p>"},{"location":"guide/param-tuner/#how-it-works","title":"How It Works","text":"<ol> <li>Define a search space with parameter ranges</li> <li>The tuner generates all combinations (Cartesian product)</li> <li>Each combination runs the full tool eval suite against selected models</li> <li>Parameters are validated and clamped per-provider via the 3-tier param registry</li> <li>Results are ranked by overall accuracy, with per-test-case drill-down available</li> </ol>"},{"location":"guide/param-tuner/#search-space-configuration","title":"Search Space Configuration","text":"<p>Define parameter ranges as numeric ranges or categorical values:</p> <pre><code>{\n  \"temperature\": { \"min\": 0.0, \"max\": 1.0, \"step\": 0.2 },\n  \"tool_choice\": [\"auto\", \"required\"],\n  \"top_p\": { \"min\": 0.5, \"max\": 1.0, \"step\": 0.25 }\n}\n</code></pre> <p>This produces combinations like:</p> temperature tool_choice top_p 0.0 auto 0.5 0.0 auto 0.75 0.0 auto 1.0 0.0 required 0.5 ... ... ..."},{"location":"guide/param-tuner/#numeric-ranges","title":"Numeric Ranges","text":"<pre><code>{\n  \"param_name\": { \"min\": 0.0, \"max\": 1.0, \"step\": 0.1 }\n}\n</code></pre> <p>Generates: <code>[0.0, 0.1, 0.2, ..., 1.0]</code></p>"},{"location":"guide/param-tuner/#categorical-values","title":"Categorical Values","text":"<pre><code>{\n  \"tool_choice\": [\"auto\", \"required\"]\n}\n</code></pre>"},{"location":"guide/param-tuner/#phase-2-custom-passthrough-params","title":"Phase 2: Custom Passthrough Params","text":"<p>Beyond the standard <code>temperature</code>, <code>top_p</code>, and <code>tool_choice</code>, the param tuner supports provider-specific parameters in the search space. These are validated and clamped through the 3-tier param registry.</p>"},{"location":"guide/param-tuner/#tier-2-common-parameters","title":"Tier 2 (Common) Parameters","text":"<p>Parameters supported by most providers with per-provider validation:</p> Parameter Type Description <code>top_p</code> float Nucleus sampling threshold <code>top_k</code> int Top-K sampling cutoff <code>frequency_penalty</code> float Penalize repeated tokens <code>presence_penalty</code> float Penalize tokens already seen <code>seed</code> int Reproducibility seed <code>reasoning_effort</code> enum Reasoning depth (<code>none</code>, <code>low</code>, <code>medium</code>, <code>high</code>)"},{"location":"guide/param-tuner/#tier-3-provider-specific-parameters","title":"Tier 3 (Provider-Specific) Parameters","text":"<p>Passed through to the LLM provider without validation. Examples:</p> Provider Parameter Description Ollama / LM Studio <code>repetition_penalty</code> Multiplicative penalty (not same as presence_penalty) Ollama / LM Studio <code>min_p</code> Minimum probability threshold Ollama <code>mirostat</code> Mirostat sampling mode (0, 1, 2) Ollama <code>num_ctx</code> Context window size override Ollama <code>keep_alive</code> Model memory duration vLLM <code>guided_json</code> JSON schema for constrained decoding vLLM <code>best_of</code> Generate N sequences, return best OpenAI <code>service_tier</code> Priority tier (auto, default, flex, priority) <p>Include these in the search space like any other parameter:</p> <pre><code>{\n  \"temperature\": [0.0, 0.5, 1.0],\n  \"repetition_penalty\": [1.0, 1.1, 1.2],\n  \"min_p\": { \"min\": 0.0, \"max\": 0.1, \"step\": 0.05 }\n}\n</code></pre>"},{"location":"guide/param-tuner/#per-model-search-spaces","title":"Per-Model Search Spaces","text":"<p>Different models may benefit from different parameter ranges. The tuner supports per-model search spaces via <code>per_model_search_spaces</code>:</p> <pre><code>{\n  \"suite_id\": \"...\",\n  \"models\": [\"gpt-4o\", \"ollama/qwen3-coder-30b\"],\n  \"search_space\": {},\n  \"per_model_search_spaces\": {\n    \"gpt-4o\": {\n      \"temperature\": [0.0, 0.5, 1.0],\n      \"top_p\": [0.8, 1.0]\n    },\n    \"ollama/qwen3-coder-30b\": {\n      \"temperature\": [0.7],\n      \"top_p\": [0.8],\n      \"top_k\": [20],\n      \"repetition_penalty\": [1.0, 1.1]\n    }\n  }\n}\n</code></pre> <p>When <code>per_model_search_spaces</code> is provided, each model uses its own search space. Models not listed in <code>per_model_search_spaces</code> fall back to the global <code>search_space</code>.</p>"},{"location":"guide/param-tuner/#phase-3-search-space-presets","title":"Phase 3: Search Space Presets","text":"<p>Save and reuse search space configurations as presets.</p>"},{"location":"guide/param-tuner/#built-in-presets","title":"Built-in Presets","text":"<p>The system includes vendor-recommended presets:</p> Preset Parameters Notes Qwen3 Coder 30B (Recommended) temp=0.7, top_p=0.8, top_k=20 Greedy decoding (temp=0) worsens quality GLM-4.7 Flash (Z.AI Recommended) temp=0.8, top_p=0.6, top_k=2 Very low top_k for MoE architecture"},{"location":"guide/param-tuner/#user-presets","title":"User Presets","text":"<p>Save your own presets via the Settings page:</p> <ol> <li>Navigate to Settings &gt; Param Tuner section</li> <li>Configure a search space</li> <li>Click Save Preset</li> <li>Name the preset for later use</li> </ol> <p>Presets are stored in the Phase 10 settings (<code>param_tuner.presets</code> array). They can be loaded, updated, or deleted from the Settings page or directly from the Param Tuner UI before starting a tune run.</p>"},{"location":"guide/param-tuner/#phase-4-settings-page-per-model-param-config","title":"Phase 4: Settings Page Per-Model Param Config","text":"<p>The Settings page provides a dedicated section for configuring parameter support per model:</p> <ul> <li>Param Support Seed: <code>POST /api/param-support/seed</code> auto-detects which parameters each model supports by querying the provider registry</li> <li>GGUF/MLX Auto-Detection: <code>GET /api/lm-studio/detect?provider_key=...</code> probes local LM Studio instances to detect whether they serve GGUF or MLX models, enabling accurate parameter range selection</li> </ul>"},{"location":"guide/param-tuner/#compatibility-matrix","title":"Compatibility Matrix","text":"<p>Before running a tune, the UI shows a compatibility matrix that displays which parameters are supported by each selected model. This grid helps you:</p> <ul> <li>Identify which parameters will be dropped or clamped for specific models</li> <li>Understand provider-specific restrictions before committing to a long-running tune</li> <li>Spot unsupported parameters early (marked as \"warn\" or \"drop\")</li> </ul> <p>The matrix is built client-side from the 3-tier parameter registry (<code>provider_params.py</code>).</p>"},{"location":"guide/param-tuner/#running-a-tune","title":"Running a Tune","text":""},{"location":"guide/param-tuner/#execution-via-jobregistry","title":"Execution via JobRegistry","text":"<p>All param tune runs execute through the JobRegistry, providing background execution, cancellation, and WebSocket progress updates.</p>"},{"location":"guide/param-tuner/#steps","title":"Steps","text":"<ol> <li>Navigate to Tool Eval and select a suite</li> <li>Click Param Tuner</li> <li>Select models to tune</li> <li>Define the search space (or load a preset)</li> <li>Review the compatibility matrix</li> <li>Click Run Tune</li> </ol> <p>The API returns <code>{\"job_id\": \"...\", \"status\": \"submitted\"}</code>.</p>"},{"location":"guide/param-tuner/#websocket-event-flow","title":"WebSocket Event Flow","text":"<pre><code>POST /api/tool-eval/param-tune  --&gt;  { job_id, status: \"submitted\" }\n                                          |\n                                 WebSocket events:\n                                          |\n    job_created  ---&gt;  tune_start  ---&gt;  combo_result (per combination)\n                                                |\n                                         job_progress (percentage + detail)\n                                                |\n                                         tune_complete (best_config, best_score)\n</code></pre> <p>Key WebSocket event types:</p> Event Payload Description <code>tune_start</code> <code>tune_id</code>, <code>total_combos</code>, <code>models</code>, <code>suite_name</code> Tuning session started <code>combo_result</code> Full result object (see below) One combination completed <code>job_progress</code> <code>progress_pct</code>, <code>progress_detail</code> Progress percentage <code>tune_complete</code> <code>best_config</code>, <code>best_score</code>, <code>duration_s</code> Tuning finished"},{"location":"guide/param-tuner/#combo_result-payload","title":"combo_result Payload","text":"<p>Each <code>combo_result</code> includes:</p> <pre><code>{\n  \"combo_index\": 3,\n  \"model_id\": \"gpt-4o\",\n  \"provider_key\": \"openai\",\n  \"model_name\": \"GPT-4o\",\n  \"config\": { \"temperature\": 0.5, \"top_p\": 0.8 },\n  \"overall_score\": 0.85,\n  \"tool_accuracy\": 90.0,\n  \"param_accuracy\": 80.0,\n  \"latency_avg_ms\": 1200,\n  \"cases_passed\": 8,\n  \"cases_total\": 10,\n  \"adjustments\": [],\n  \"case_results\": [...]\n}\n</code></pre>"},{"location":"guide/param-tuner/#drill-down-modal","title":"Drill-Down Modal","text":"<p>Click any result row in the UI to see the drill-down modal with per-test-case details. The <code>case_results</code> array in each combo result contains:</p> Field Description <code>test_case_id</code> ID of the test case <code>prompt</code> The user prompt <code>expected_tool</code> Expected tool name <code>actual_tool</code> Tool the model actually called <code>expected_params</code> Expected parameters <code>actual_params</code> Parameters the model provided <code>tool_selection_score</code> 0.0 or 1.0 <code>param_accuracy</code> 0.0-1.0 or null <code>overall_score</code> Weighted composite <code>success</code> Whether the API call succeeded <code>error</code> Error message if failed <code>latency_ms</code> Response time in ms"},{"location":"guide/param-tuner/#droppedclamped-param-badges","title":"Dropped/Clamped Param Badges","text":"<p>When the 3-tier registry modifies a parameter, the UI displays badges with tooltips:</p> <ul> <li>Clamped (yellow): Parameter value was adjusted to fit the provider's valid range</li> <li>Dropped (red): Parameter was removed due to a hard conflict (e.g., Anthropic temp+top_p)</li> <li>Warn (orange): Parameter was passed through but the provider may not support it</li> </ul> <p>Each badge shows the original value, the adjusted value, and the reason.</p>"},{"location":"guide/param-tuner/#estimating-run-count","title":"Estimating Run Count","text":"<p>Before starting, review the total combinations:</p> <pre><code>total_combinations = product_of_all_param_values * num_models\ntotal_api_calls = total_combinations * num_test_cases\n</code></pre> <p>Duplicate combinations (after provider validation and clamping) are automatically deduplicated per model.</p> <p>Cost Awareness: A search space with many parameters and fine steps can produce thousands of combinations. Each combination runs the full test suite against each model. Monitor costs carefully.</p>"},{"location":"guide/param-tuner/#the-3-tier-parameter-registry","title":"The 3-Tier Parameter Registry","text":"<p>All parameter validation flows through <code>provider_params.py</code>, which defines a 3-tier architecture:</p>"},{"location":"guide/param-tuner/#tier-1-universal","title":"Tier 1 -- Universal","text":"<p><code>temperature</code>, <code>max_tokens</code>, <code>stop</code> -- supported by all providers. Ranges and defaults are provider-specific.</p>"},{"location":"guide/param-tuner/#tier-2-common","title":"Tier 2 -- Common","text":"<p><code>top_p</code>, <code>top_k</code>, <code>frequency_penalty</code>, <code>presence_penalty</code>, <code>seed</code>, <code>reasoning_effort</code> -- supported by most providers with per-provider validation rules.</p>"},{"location":"guide/param-tuner/#tier-3-provider-specific","title":"Tier 3 -- Provider-Specific","text":"<p>Passthrough parameters sent directly to the LLM via LiteLLM. No validation, no clamping.</p>"},{"location":"guide/param-tuner/#supported-providers-10","title":"Supported Providers (10)","text":"Provider Key Notes OpenAI <code>openai</code> GPT-5 locks temp to 1.0, O-series uses max_completion_tokens Anthropic <code>anthropic</code> max_tokens required, temp+top_p mutual exclusion Google Gemini <code>gemini</code> Gemini 3 degrades below temp 1.0 Ollama <code>ollama</code> Full local param support (mirostat, num_ctx, etc.) LM Studio <code>lm_studio</code> Similar to Ollama, supports repetition_penalty, min_p Mistral <code>mistral</code> Limited top_k, seed sent as random_seed DeepSeek <code>deepseek</code> R1 ignores sampling params in thinking mode Cohere <code>cohere</code> Penalty range 0-1 only, top_p max 0.99 xAI (Grok) <code>xai</code> Reasoning models reject penalties and stop vLLM <code>vllm</code> Direct kwargs (no extra_body), guided_json support"},{"location":"guide/param-tuner/#validation-pipeline","title":"Validation Pipeline","text":"<pre><code>User params  --&gt;  identify_provider()\n                        |\n                        v\n              validate_params()\n                        |\n                        v\n              clamp_temperature() (model-specific overrides)\n                        |\n                        v\n              resolve_conflicts() (provider-specific rules)\n                        |\n                        v\n              Apply skip_params (from config.yaml)\n                        |\n                        v\n              Merge passthrough (Tier 3)\n                        |\n                        v\n              Final kwargs for litellm.acompletion()\n</code></pre>"},{"location":"guide/param-tuner/#results","title":"Results","text":"<p>The tuner produces:</p> <ul> <li>Per-combination scores: Overall accuracy for each parameter set</li> <li>Best combination: The parameters that achieved the highest accuracy</li> <li>Comparison table: Side-by-side results for all combinations (sortable by any column)</li> <li>Provider-specific adjustments: Notes on parameter clamping, drops, or warnings</li> <li>Per-test-case drill-down: Click any result row to see individual case results</li> <li>ETA tracking: Real-time estimate of remaining time based on completion rate</li> </ul> <p>Results are incrementally saved to the database, so partial results survive disconnections.</p>"},{"location":"guide/param-tuner/#experiment-integration","title":"Experiment Integration","text":"<p>When a param tune is linked to an experiment:</p> <ol> <li>The best result is auto-promoted as a tool eval run in the experiment timeline</li> <li>The experiment's best config is updated if the tune achieves a higher score</li> <li>The promoted eval run includes a <code>promoted_from: \"param_tune:{tune_id}\"</code> marker</li> </ol> <p>This allows the experiment timeline to show a continuous improvement curve across eval runs, param tunes, and prompt tunes.</p>"},{"location":"guide/param-tuner/#tune-history","title":"Tune History","text":"<pre><code># List tune runs\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/param-tune/history\n\n# Get a specific tune run (includes full results)\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/param-tune/history/{tune_id}\n\n# Delete a tune run\ncurl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/param-tune/history/{tune_id}\n</code></pre>"},{"location":"guide/param-tuner/#api-reference","title":"API Reference","text":"Method Endpoint Description <code>POST</code> <code>/api/tool-eval/param-tune</code> Start param tuning (returns job_id) <code>POST</code> <code>/api/tool-eval/param-tune/cancel</code> Cancel running tune <code>GET</code> <code>/api/tool-eval/param-tune/history</code> List tune runs <code>GET</code> <code>/api/tool-eval/param-tune/history/{id}</code> Get tune run details <code>DELETE</code> <code>/api/tool-eval/param-tune/history/{id}</code> Delete tune run"},{"location":"guide/param-tuner/#request-body-for-post-apitool-evalparam-tune","title":"Request Body for POST /api/tool-eval/param-tune","text":"<pre><code>{\n  \"suite_id\": \"required - tool suite ID\",\n  \"models\": [\"model_id_1\", \"model_id_2\"],\n  \"targets\": [{\"provider_key\": \"openai\", \"model_id\": \"gpt-4o\"}],\n  \"search_space\": {\n    \"temperature\": { \"min\": 0.0, \"max\": 1.0, \"step\": 0.5 },\n    \"top_p\": [0.8, 1.0]\n  },\n  \"per_model_search_spaces\": {},\n  \"experiment_id\": \"optional - link to experiment\"\n}\n</code></pre> <p>Either <code>models</code> (legacy model_id list) or <code>targets</code> (precise provider_key+model_id pairs) must be provided. <code>targets</code> format is preferred for avoiding ambiguity when the same model ID exists under multiple providers.</p>"},{"location":"guide/param-tuner/#best-practices","title":"Best Practices","text":"<ul> <li>Start with a coarse grid (large steps) to identify promising regions</li> <li>Then refine with a fine grid around the best-performing area</li> <li>Use <code>temperature</code> and <code>tool_choice</code> as primary axes -- they have the most impact</li> <li>Keep the number of test cases manageable (5-10) for faster iteration</li> <li>Review the compatibility matrix before starting -- unsupported params waste combinations</li> <li>Use per-model search spaces when tuning across very different providers (e.g., OpenAI vs Ollama)</li> <li>Load vendor presets for local models to start with recommended settings</li> <li>Link tune runs to an experiment for automatic best-config tracking</li> </ul>"},{"location":"guide/process-tracker/","title":"Process Tracker","text":""},{"location":"guide/process-tracker/#overview","title":"Overview","text":"<p>The Process Tracker provides persistent server-side tracking for all background jobs. Jobs run as asyncio tasks, persist their state to SQLite, and broadcast real-time status updates to connected clients via WebSocket. If you close your browser or navigate away, you can reconnect and see the current status of any running job.</p> <p>The system is implemented primarily in two modules:</p> <ul> <li><code>job_registry.py</code> -- The <code>JobRegistry</code> singleton that manages job lifecycle, concurrency, and persistence</li> <li><code>job_handlers.py</code> -- Handler functions for each job type (one handler per type)</li> </ul>"},{"location":"guide/process-tracker/#job-types","title":"Job Types","text":"<p>The registry supports seven job types, each with its own handler:</p> Job Type Handler Description <code>benchmark</code> <code>benchmark_handler</code> Speed benchmark runs across multiple LLM providers <code>tool_eval</code> <code>tool_eval_handler</code> Tool-calling evaluation runs against test suites <code>param_tune</code> <code>param_tune_handler</code> Parameter grid search for optimal configurations <code>prompt_tune</code> <code>prompt_tune_handler</code> Prompt variation testing (quick or evolutionary) <code>judge</code> <code>judge_handler</code> LLM-as-Judge quality evaluation of eval results <code>judge_compare</code> <code>judge_compare_handler</code> Side-by-side comparative judge between two eval runs <code>scheduled_benchmark</code> (via scheduler) Automated recurring benchmark runs <p>Handlers are registered at startup in <code>app.py</code> by calling <code>register_all_handlers()</code> from <code>job_handlers.py</code>.</p>"},{"location":"guide/process-tracker/#job-statuses","title":"Job Statuses","text":"<p>Every job moves through a defined set of statuses with validated transitions:</p> Status Description <code>pending</code> Created, about to start (under concurrency limit) <code>queued</code> Waiting for a concurrency slot to open <code>running</code> Actively executing <code>done</code> Completed successfully <code>failed</code> Completed with an error (including timeout) <code>cancelled</code> Cancelled by user or admin <code>interrupted</code> Terminated due to server restart or orphaned state"},{"location":"guide/process-tracker/#state-transitions","title":"State Transitions","text":"<pre><code>pending -----&gt; running -----&gt; done\n   |              |---------&gt; failed\n   |              |---------&gt; cancelled\n   |              |---------&gt; interrupted\n   |\n   +-----------&gt; queued -----&gt; running (when a slot opens)\n   |                |--------&gt; cancelled\n   |\n   +-----------&gt; cancelled\n</code></pre> <p>Invalid transitions are logged as warnings but do not block execution.</p>"},{"location":"guide/process-tracker/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"guide/process-tracker/#submit","title":"Submit","text":"<p>When a job is submitted via <code>registry.submit()</code>:</p> <ol> <li>A unique <code>job_id</code> (hex UUID) is generated</li> <li>The registry checks the user's concurrency limit</li> <li>If under the limit, the job starts immediately (status: <code>pending</code> then <code>running</code>)</li> <li>If at the limit, the job is placed in a queue (status: <code>queued</code>)</li> <li>The job is persisted to the <code>jobs</code> table in SQLite</li> <li>A <code>job_created</code> WebSocket event is broadcast to the user</li> </ol>"},{"location":"guide/process-tracker/#execution","title":"Execution","text":"<p>While running:</p> <ol> <li>The handler receives a <code>cancel_event</code> (asyncio.Event) and a <code>progress_cb</code> callback</li> <li>The handler calls <code>progress_cb(pct, detail)</code> to report progress (0-100%)</li> <li>Progress updates are persisted to the database and broadcast via WebSocket</li> <li>The handler returns a <code>result_ref</code> string on success (e.g., a benchmark_run ID)</li> </ol>"},{"location":"guide/process-tracker/#completion","title":"Completion","text":"<p>When a job finishes:</p> <ul> <li>Success: Status set to <code>done</code>, <code>result_ref</code> stored, <code>job_completed</code> event broadcast</li> <li>Error: Status set to <code>failed</code>, <code>error_msg</code> stored (truncated to 500 chars), <code>job_failed</code> event broadcast</li> <li>Cancelled: Status set to <code>cancelled</code>, <code>job_cancelled</code> event broadcast</li> <li>Interrupted: Status set to <code>interrupted</code> (server shutdown or asyncio task cancelled)</li> </ul> <p>After any terminal state, the user's concurrency slot is released and the queue is checked for the next eligible job.</p>"},{"location":"guide/process-tracker/#cancel","title":"Cancel","text":"<p>Jobs can be cancelled through multiple paths:</p> <ul> <li>REST API: <code>POST /api/jobs/{job_id}/cancel</code></li> <li>WebSocket: Send <code>{\"type\": \"cancel\", \"job_id\": \"...\"}</code> over the WebSocket connection</li> <li>Admin: <code>POST /api/admin/jobs/{job_id}/cancel</code></li> </ul> <p>For pending/queued jobs, cancellation is immediate. For running jobs, the cancel event is signaled and the handler checks it at safe points.</p>"},{"location":"guide/process-tracker/#concurrency-control","title":"Concurrency Control","text":"<p>Each user has a configurable maximum number of concurrent jobs (default: 1). This limit is stored in the <code>rate_limits</code> table and can be adjusted by admins.</p> <ul> <li>When a user submits a job and is at their limit, the job is queued</li> <li>When a running job finishes, the registry automatically starts the next queued job for that user</li> <li>A <code>_slot_lock</code> (asyncio.Lock) prevents race conditions in slot accounting</li> </ul>"},{"location":"guide/process-tracker/#queue-processing","title":"Queue Processing","text":"<p>The queue is processed in FIFO order per user:</p> <ol> <li>When a job completes (any terminal state), <code>_process_queue(user_id)</code> is called</li> <li>The registry checks if the user has available slots</li> <li>If so, the oldest queued job is retrieved from the database and started</li> <li>This repeats until either the limit is reached or no queued jobs remain</li> </ol>"},{"location":"guide/process-tracker/#timeout-and-watchdog","title":"Timeout and Watchdog","text":"<ul> <li>Each job has a configurable timeout (default: 7200 seconds / 2 hours)</li> <li>A watchdog task runs every 60 seconds, checking for jobs that have exceeded their timeout</li> <li>Timed-out jobs are marked as <code>failed</code> with the error message \"Timeout exceeded\"</li> <li>The corresponding asyncio task is cancelled</li> </ul>"},{"location":"guide/process-tracker/#server-restart-recovery","title":"Server Restart Recovery","text":"<p>On application startup, <code>_startup_recovery()</code> runs:</p> <ul> <li>All jobs with status <code>running</code>, <code>pending</code>, or <code>queued</code> are marked as <code>interrupted</code></li> <li>This prevents ghost jobs from blocking concurrency slots after a restart</li> <li>The count of affected jobs is logged as a warning</li> </ul>"},{"location":"guide/process-tracker/#websocket-integration","title":"WebSocket Integration","text":"<p>The JobRegistry broadcasts events to users via the <code>ConnectionManager</code> (set during startup with <code>set_ws_manager()</code>). All events include the <code>job_id</code> field.</p>"},{"location":"guide/process-tracker/#core-job-events","title":"Core Job Events","text":"Event Type When Key Fields <code>job_created</code> Job submitted <code>job_type</code>, <code>status</code>, <code>progress_detail</code>, <code>created_at</code> <code>job_started</code> Job begins executing <code>job_type</code> <code>job_progress</code> Handler reports progress <code>progress_pct</code>, <code>progress_detail</code> <code>job_completed</code> Job finished successfully <code>result_ref</code> <code>job_failed</code> Job errored or timed out <code>error</code> <code>job_cancelled</code> Job was cancelled --"},{"location":"guide/process-tracker/#job-type-specific-events","title":"Job-Type-Specific Events","text":"<p>Handlers send additional events for granular progress:</p> <p>Benchmarks: <code>benchmark_init</code>, <code>benchmark_progress</code>, <code>benchmark_result</code></p> <p>Tool Eval: <code>tool_eval_init</code>, <code>tool_eval_progress</code>, <code>tool_eval_result</code>, <code>tool_eval_summary</code>, <code>tool_eval_complete</code></p> <p>Param Tune: <code>tune_start</code>, <code>combo_result</code>, <code>tune_complete</code></p> <p>Prompt Tune: <code>tune_start</code>, <code>generation_start</code>, <code>prompt_generated</code>, <code>prompt_eval_start</code>, <code>prompt_eval_result</code>, <code>generation_complete</code>, <code>tune_complete</code></p> <p>Judge: <code>judge_start</code>, <code>judge_verdict</code>, <code>judge_report</code>, <code>judge_complete</code></p> <p>Judge Compare: <code>compare_start</code>, <code>compare_case</code>, <code>compare_complete</code></p>"},{"location":"guide/process-tracker/#reconnection","title":"Reconnection","text":"<p>When a WebSocket client reconnects (e.g., after a page refresh):</p> <ol> <li>The server sends a <code>sync</code> message with active and recent jobs</li> <li>For each running job, a reconstructed init event is sent so the client can resume progress tracking</li> <li>Reconnect init events include <code>reconnect: true</code> and the current <code>progress_pct</code></li> </ol>"},{"location":"guide/process-tracker/#database-schema","title":"Database Schema","text":"<p>The <code>jobs</code> table stores all job state:</p> <pre><code>CREATE TABLE jobs (\n    id TEXT PRIMARY KEY,\n    user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n    job_type TEXT NOT NULL,\n    status TEXT NOT NULL DEFAULT 'pending',\n    progress_pct INTEGER DEFAULT 0,\n    progress_detail TEXT DEFAULT '',\n    params_json TEXT NOT NULL DEFAULT '{}',\n    result_ref TEXT,\n    result_type TEXT,\n    error_msg TEXT,\n    created_at TEXT NOT NULL DEFAULT (datetime('now')),\n    started_at TEXT,\n    completed_at TEXT,\n    timeout_at TEXT,\n    timeout_seconds INTEGER NOT NULL DEFAULT 7200\n);\n</code></pre> <p>Indexes exist on <code>(user_id, status)</code>, <code>(user_id, created_at DESC)</code>, <code>(status)</code>, and <code>(status, timeout_at)</code> for efficient queries.</p>"},{"location":"guide/process-tracker/#rest-api-endpoints","title":"REST API Endpoints","text":""},{"location":"guide/process-tracker/#user-endpoints","title":"User Endpoints","text":"Method Path Description <code>GET</code> <code>/api/jobs</code> List current user's jobs. Query params: <code>?status=running,queued&amp;limit=20</code> <code>GET</code> <code>/api/jobs/{job_id}</code> Get a single job's details <code>POST</code> <code>/api/jobs/{job_id}/cancel</code> Cancel a specific job"},{"location":"guide/process-tracker/#admin-endpoints","title":"Admin Endpoints","text":"Method Path Description <code>GET</code> <code>/api/admin/jobs</code> List all active jobs across all users (includes user email) <code>POST</code> <code>/api/admin/jobs/{job_id}/cancel</code> Cancel any user's job"},{"location":"guide/process-tracker/#example-list-active-jobs","title":"Example: List Active Jobs","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/jobs?status=running,queued\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"jobs\": [\n    {\n      \"id\": \"a1b2c3d4...\",\n      \"job_type\": \"benchmark\",\n      \"status\": \"running\",\n      \"progress_pct\": 45,\n      \"progress_detail\": \"GPT-4o, Run 2/3\",\n      \"created_at\": \"2026-02-22T10:30:00\",\n      \"started_at\": \"2026-02-22T10:30:01\",\n      \"timeout_at\": \"2026-02-22T12:30:01\"\n    }\n  ]\n}\n</code></pre>"},{"location":"guide/process-tracker/#example-cancel-a-job","title":"Example: Cancel a Job","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/jobs/a1b2c3d4.../cancel\"\n</code></pre> <p>Response:</p> <pre><code>{\"status\": \"ok\", \"message\": \"Cancellation requested\"}\n</code></pre>"},{"location":"guide/process-tracker/#notification-widget","title":"Notification Widget","text":"<p>The frontend includes a notification widget in the top navigation bar:</p> <ul> <li>Badge: Displays the count of active (pending/queued/running) jobs</li> <li>Dropdown: Lists active jobs with progress bars and status details</li> <li>History: Shows the most recent completed jobs (approximately the last 10)</li> <li>Cancel: Each running job can be cancelled directly from the dropdown</li> <li>Multi-tab sync: All browser tabs receive the same WebSocket events and stay synchronized</li> </ul>"},{"location":"guide/process-tracker/#orphan-cleanup","title":"Orphan Cleanup","text":"<p>The system includes safeguards for orphaned state:</p> <ul> <li>Ghost jobs: If a running job has no in-memory cancel event (e.g., after a partial restart), it is detected and marked as <code>interrupted</code></li> <li>Orphaned tune runs: When a job is terminal but its linked param_tune or prompt_tune run still shows <code>running</code>, the cancel endpoint automatically cleans up the linked run</li> <li>Startup recovery: On server start, all non-terminal jobs are marked <code>interrupted</code></li> </ul>"},{"location":"guide/prompt-tuner/","title":"Prompt Tuner","text":"<p>The Prompt Tuner uses a meta-model to generate and evaluate system prompt variations against a tool eval suite. It supports two modes: Quick (single generation) and Evolutionary (multi-generation with selection pressure).</p>"},{"location":"guide/prompt-tuner/#how-it-works","title":"How It Works","text":"<ol> <li>A meta-model generates multiple system prompt variations</li> <li>Each variation is injected as a system message before the test case prompt</li> <li>The full tool eval suite runs for each variation and each target model</li> <li>Results are ranked by overall accuracy</li> <li>In evolutionary mode, top performers survive to seed the next generation</li> </ol>"},{"location":"guide/prompt-tuner/#modes","title":"Modes","text":""},{"location":"guide/prompt-tuner/#quick-mode","title":"Quick Mode","text":"<p>Single generation of prompt variations. Best for rapid exploration.</p> <ul> <li>One meta-model call to generate <code>population_size</code> prompts</li> <li>Each prompt evaluated against all models and test cases</li> <li>Results ranked, best prompt identified</li> </ul>"},{"location":"guide/prompt-tuner/#evolutionary-mode","title":"Evolutionary Mode","text":"<p>Multi-generation optimization with selection pressure. Best for finding high-quality prompts.</p> <ul> <li>Generation 1: Same as Quick mode</li> <li>Selection: Top performers (based on <code>selection_ratio</code>) survive</li> <li>Subsequent generations: Meta-model mutates surviving prompts</li> <li>Supports bold mutations (significantly different approach) and conservative mutations (small refinements)</li> </ul>"},{"location":"guide/prompt-tuner/#configuration","title":"Configuration","text":""},{"location":"guide/prompt-tuner/#request-body","title":"Request Body","text":"<pre><code>{\n  \"suite_id\": \"your-suite-id\",\n  \"mode\": \"quick\",\n  \"target_models\": [\"gpt-4o\", \"anthropic/claude-sonnet-4-5\"],\n  \"target_targets\": [\n    {\"provider_key\": \"openai\", \"model_id\": \"gpt-4o\"},\n    {\"provider_key\": \"anthropic\", \"model_id\": \"anthropic/claude-sonnet-4-5\"}\n  ],\n  \"meta_model\": \"gpt-4o\",\n  \"meta_provider_key\": \"openai\",\n  \"base_prompt\": \"You are a helpful assistant that uses tools to answer questions.\",\n  \"config\": {\n    \"population_size\": 5,\n    \"generations\": 3,\n    \"selection_ratio\": 0.4,\n    \"temperature\": 0.0,\n    \"tool_choice\": \"required\"\n  },\n  \"experiment_id\": \"optional-experiment-id\"\n}\n</code></pre>"},{"location":"guide/prompt-tuner/#config-parameters","title":"Config Parameters","text":"Parameter Default Range Description <code>population_size</code> 5 3-20 Number of prompts per generation <code>generations</code> 1 (quick) / 3 (evo) 1-10 Number of generations (forced to 1 in quick mode) <code>selection_ratio</code> 0.4 0.2-0.8 Fraction of prompts that survive each generation <code>temperature</code> 0.0 0.0-2.0 Temperature for eval calls (not meta-model) <code>tool_choice</code> required auto/required/none Tool choice for eval calls"},{"location":"guide/prompt-tuner/#meta-model-requirements","title":"Meta-Model Requirements","text":"<p>The meta-model generates prompt variations. Choose a capable model:</p> <ul> <li>Must support text generation (any LLM in your config)</li> <li>The meta-model is called with temperature 0.9 and max_tokens 4096</li> <li>If the model supports structured output (JSON schema), it is automatically used</li> <li>If a model rejects structured output, the tuner retries without it</li> <li>Transient errors (502, 503, timeout) are retried with exponential backoff</li> </ul>"},{"location":"guide/prompt-tuner/#running-a-prompt-tune","title":"Running a Prompt Tune","text":""},{"location":"guide/prompt-tuner/#execution-via-jobregistry","title":"Execution via JobRegistry","text":"<p>All prompt tune runs execute through the JobRegistry, providing background execution, cancellation, and WebSocket progress updates.</p>"},{"location":"guide/prompt-tuner/#steps","title":"Steps","text":"<ol> <li>Navigate to Tool Eval and select a suite</li> <li>Click Prompt Tuner</li> <li>Select target models to test against</li> <li>Select a meta-model for prompt generation</li> <li>Optionally set a base prompt to improve upon</li> <li>Configure mode (Quick or Evolutionary) and parameters</li> <li>Optionally link to an Experiment for tracking</li> <li>Click Run Tune</li> </ol> <p>The API returns <code>{\"job_id\": \"...\", \"status\": \"submitted\"}</code>.</p>"},{"location":"guide/prompt-tuner/#estimating-cost","title":"Estimating Cost","text":"<p>Before starting, use the estimate endpoint:</p> <pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  \"http://localhost:8501/api/tool-eval/prompt-tune/estimate?suite_id=ID&amp;mode=quick&amp;population_size=5&amp;num_models=2\"\n</code></pre> <p>Returns:</p> <pre><code>{\n  \"total_prompt_generations\": 5,\n  \"total_eval_calls\": 50,\n  \"total_api_calls\": 51,\n  \"estimated_duration_s\": 105,\n  \"warning\": null\n}\n</code></pre> <p>Total API calls = <code>generations * 1 (meta call) + total_prompts * num_cases * num_models</code></p>"},{"location":"guide/prompt-tuner/#websocket-event-flow","title":"WebSocket Event Flow","text":"<pre><code>POST /api/tool-eval/prompt-tune  --&gt;  { job_id, status: \"submitted\" }\n                                           |\n                                  WebSocket events:\n                                           |\n    tune_start  ---&gt;  generation_start  ---&gt;  prompt_generated (per prompt)\n                                                      |\n                                               prompt_eval_start (per model)\n                                                      |\n                                               prompt_eval_result (per model)\n                                                      |\n                                              generation_complete (survivors)\n                                                      |\n                                               [next generation or...]\n                                                      |\n                                               tune_complete\n</code></pre> <p>Key WebSocket event types:</p> Event Description <code>tune_start</code> Tuning session started, includes mode, total_prompts, total_eval_calls <code>generation_start</code> New generation beginning <code>prompt_generated</code> Meta-model produced a prompt variation (includes text, style, parent_index) <code>prompt_eval_start</code> Starting eval of a prompt on a specific model <code>prompt_eval_result</code> Eval result for one prompt on one model (overall_score, tool_accuracy, param_accuracy) <code>generation_complete</code> Generation finished, includes best_score and survivor indices <code>generation_error</code> Meta-model returned no prompts for this generation <code>tune_complete</code> Tuning finished, includes best_prompt and best_score"},{"location":"guide/prompt-tuner/#results","title":"Results","text":"<p>The tuner produces:</p> <ul> <li>Per-prompt scores: Overall accuracy for each prompt variation across all models</li> <li>Per-model breakdown: How each model performed with each prompt</li> <li>Best prompt: The variation that achieved the highest cross-model accuracy</li> <li>Generation history: In evolutionary mode, tracks improvement across generations</li> <li>Survivor tracking: Which prompts survived selection in each generation</li> </ul>"},{"location":"guide/prompt-tuner/#generation-result-structure","title":"Generation Result Structure","text":"<p>Each generation contains:</p> <pre><code>{\n  \"generation\": 1,\n  \"prompts\": [\n    {\n      \"index\": 0,\n      \"style\": \"concise\",\n      \"text\": \"Use the available tools...\",\n      \"parent_index\": null,\n      \"mutation_type\": null,\n      \"scores\": {\n        \"gpt-4o\": { \"overall\": 0.85, \"tool_acc\": 90.0, \"param_acc\": 80.0 },\n        \"claude-sonnet-4-5\": { \"overall\": 0.92, \"tool_acc\": 100.0, \"param_acc\": 85.0 }\n      },\n      \"avg_score\": 0.885,\n      \"survived\": true\n    }\n  ],\n  \"best_index\": 0,\n  \"best_score\": 0.885\n}\n</code></pre>"},{"location":"guide/prompt-tuner/#experiment-integration","title":"Experiment Integration","text":"<p>When a prompt tune is linked to an experiment:</p> <ol> <li>The best prompt's score is compared against the experiment's current best</li> <li>If it improves, the experiment's best config is updated with the winning system prompt</li> <li>A <code>promoted_from: \"prompt_tune:{tune_id}\"</code> marker is added</li> <li>The experiment timeline shows prompt tune entries with score and delta from baseline</li> </ol>"},{"location":"guide/prompt-tuner/#prompt-tune-history","title":"Prompt Tune History","text":"<pre><code># List prompt tune runs\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/prompt-tune/history\n\n# Get a specific prompt tune run\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/prompt-tune/history/{tune_id}\n\n# Delete a prompt tune run\ncurl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/tool-eval/prompt-tune/history/{tune_id}\n</code></pre>"},{"location":"guide/prompt-tuner/#api-reference","title":"API Reference","text":"Method Endpoint Description <code>POST</code> <code>/api/tool-eval/prompt-tune</code> Start prompt tuning (returns job_id) <code>POST</code> <code>/api/tool-eval/prompt-tune/cancel</code> Cancel running tune <code>GET</code> <code>/api/tool-eval/prompt-tune/estimate</code> Get cost/time estimate <code>GET</code> <code>/api/tool-eval/prompt-tune/history</code> List tune runs <code>GET</code> <code>/api/tool-eval/prompt-tune/history/{id}</code> Get tune run details <code>DELETE</code> <code>/api/tool-eval/prompt-tune/history/{id}</code> Delete tune run"},{"location":"guide/prompt-tuner/#cancellation","title":"Cancellation","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"job_id\": \"abc123\"}' \\\n  http://localhost:8501/api/tool-eval/prompt-tune/cancel\n</code></pre> <p>Cancellation is cooperative: the handler checks a cancel event between eval calls and stops gracefully. Partial results are saved to the database.</p>"},{"location":"guide/prompt-tuner/#best-practices","title":"Best Practices","text":"<ul> <li>Always include a \"no system prompt\" baseline for comparison (set <code>base_prompt</code> to an empty string)</li> <li>Use a capable meta-model (GPT-4o, Claude Sonnet 4.5) for better prompt generation</li> <li>Avoid using the same model as both the meta-model and the target model</li> <li>Use low temperature (0.0) for consistent eval results</li> <li>Use <code>tool_choice: required</code> to ensure models attempt tool calls</li> <li>Start with Quick mode to get a sense of what works</li> <li>Then switch to Evolutionary mode (3-5 generations) to refine the best approaches</li> <li>Keep test suites small (5-10 cases) for faster iteration during prompt development</li> <li>Test the winning prompt with a larger suite for validation</li> <li>Link tune runs to an experiment to track improvement over time</li> <li>The meta-model uses temperature 0.9 for creative diversity in prompt generation</li> </ul>"},{"location":"guide/scheduling/","title":"Scheduling","text":"<p>LLM Benchmark Studio supports automated, recurring benchmark runs. Schedules are managed per-user and execute in a background task on the server.</p>"},{"location":"guide/scheduling/#how-it-works","title":"How It Works","text":"<ol> <li>Create a schedule with a name, models, prompt, and interval</li> <li>The background scheduler checks for due schedules every 60 seconds</li> <li>When a schedule is due, it runs the specified benchmarks via the JobRegistry</li> <li>Results are saved to benchmark history with metadata indicating the source</li> <li>Real-time status updates for triggered schedules are broadcast via WebSocket</li> </ol>"},{"location":"guide/scheduling/#creating-a-schedule","title":"Creating a Schedule","text":""},{"location":"guide/scheduling/#via-the-ui","title":"Via the UI","text":"<ol> <li>Navigate to the Scheduling screen</li> <li>Click New Schedule</li> <li>Configure:<ul> <li>Name: A descriptive name for the schedule</li> <li>Models: Select which models to benchmark</li> <li>Prompt: The benchmark prompt</li> <li>Max Tokens: Output token limit (default: 512)</li> <li>Temperature: Sampling temperature (default: 0.7)</li> <li>Interval: Hours between runs</li> </ul> </li> <li>Click Create</li> </ol>"},{"location":"guide/scheduling/#via-the-api","title":"Via the API","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Nightly GPT-4o benchmark\",\n    \"models\": [\"gpt-4o\", \"gpt-4o-mini\"],\n    \"prompt\": \"Explain recursion in programming with a Python example.\",\n    \"max_tokens\": 512,\n    \"temperature\": 0.7,\n    \"interval_hours\": 24\n  }' \\\n  http://localhost:8501/api/schedules\n</code></pre>"},{"location":"guide/scheduling/#managing-schedules","title":"Managing Schedules","text":""},{"location":"guide/scheduling/#list-schedules","title":"List Schedules","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/schedules\n</code></pre>"},{"location":"guide/scheduling/#update-a-schedule","title":"Update a Schedule","text":"<pre><code>curl -X PUT -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"enabled\": false}' \\\n  http://localhost:8501/api/schedules/{schedule_id}\n</code></pre>"},{"location":"guide/scheduling/#delete-a-schedule","title":"Delete a Schedule","text":"<pre><code>curl -X DELETE -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/schedules/{schedule_id}\n</code></pre>"},{"location":"guide/scheduling/#trigger-immediately","title":"Trigger Immediately","text":"<p>Run a schedule immediately without waiting for the next scheduled time:</p> <pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/schedules/{schedule_id}/trigger\n</code></pre> <p>The trigger endpoint runs the benchmark as a background asyncio task and returns immediately. After execution, the schedule's <code>last_run</code> and <code>next_run</code> timestamps are updated.</p>"},{"location":"guide/scheduling/#schedule-fields","title":"Schedule Fields","text":"Field Type Description <code>name</code> string Schedule name <code>prompt</code> string Benchmark prompt <code>models_json</code> list Model IDs to benchmark <code>max_tokens</code> int Max output tokens <code>temperature</code> float Sampling temperature <code>interval_hours</code> int Hours between runs <code>enabled</code> boolean Whether the schedule is active <code>last_run</code> datetime When it last ran <code>next_run</code> datetime When it will next run"},{"location":"guide/scheduling/#how-results-are-stored","title":"How Results Are Stored","text":"<p>Scheduled benchmark results are saved to the same <code>benchmark_runs</code> table as manual benchmarks. The metadata field identifies them as scheduled:</p> <pre><code>{\n  \"source\": \"schedule\",\n  \"schedule_id\": \"abc123\",\n  \"schedule_name\": \"Nightly GPT-4o benchmark\"\n}\n</code></pre> <p>Results appear in the History screen alongside manual runs.</p>"},{"location":"guide/scheduling/#integration-with-job-tracking","title":"Integration with Job Tracking","text":"<p>Scheduled benchmarks are tracked by the JobRegistry like any other job type (<code>scheduled_benchmark</code>). This means:</p> <ul> <li>Scheduled runs appear in the notification widget when active</li> <li>Progress is visible via WebSocket events</li> <li>Active scheduled runs count toward the user's concurrency limit</li> <li>If the server restarts during a scheduled run, the job is marked as <code>interrupted</code></li> </ul>"},{"location":"guide/scheduling/#notes","title":"Notes","text":"<ul> <li>Scheduled runs execute single-run per model (no multi-run averaging)</li> <li>Warmup runs are not performed for scheduled benchmarks</li> <li>Context tier is fixed at <code>[0]</code> (no context scaling)</li> <li>The scheduler uses per-user API keys, so each user's keys are used for their own schedules</li> <li>If the server restarts, the scheduler resumes checking on the next 60-second cycle</li> </ul>"},{"location":"guide/tool-eval/","title":"Tool Calling Evaluation","text":"<p>The Tool Calling Evaluation framework tests whether LLMs correctly use function calling (tool use). You define tool suites with test cases and run models against them to measure accuracy.</p>"},{"location":"guide/tool-eval/#core-concepts","title":"Core Concepts","text":""},{"location":"guide/tool-eval/#tool-suite","title":"Tool Suite","text":"<p>A collection of tools and test cases -- like a test suite for function calling.</p> <ul> <li>Tools: Defined in OpenAI function calling JSON schema format</li> <li>Test Cases: Each has a prompt, an expected tool, and optionally expected parameters</li> <li>System Prompt: Optional per-suite system prompt injected before test case prompts</li> </ul>"},{"location":"guide/tool-eval/#test-case","title":"Test Case","text":"<p>A single evaluation scenario:</p> <pre><code>{\n  \"prompt\": \"What's the weather in Paris?\",\n  \"expected_tool\": \"get_weather\",\n  \"expected_params\": { \"city\": \"Paris\" },\n  \"param_scoring\": \"exact\"\n}\n</code></pre> Field Type Description <code>prompt</code> string The user message sent to the model <code>expected_tool</code> string or list The tool name(s) the model should call <code>expected_params</code> object or null Expected parameter values (null = not scored) <code>param_scoring</code> string Scoring mode: <code>exact</code> (default), <code>fuzzy</code>, <code>contains</code>, <code>semantic</code> <code>scoring_config</code> object or null Advanced scoring configuration for fuzzy matching <p>The <code>expected_tool</code> field supports multiple acceptable answers:</p> <pre><code>{\n  \"expected_tool\": [\"search\", \"web_search\"],\n  \"prompt\": \"Find information about quantum computing\"\n}\n</code></pre>"},{"location":"guide/tool-eval/#scoring-configuration","title":"Scoring Configuration","text":"<p>For advanced parameter matching, use <code>scoring_config</code>:</p> <pre><code>{\n  \"scoring_config\": {\n    \"mode\": \"case_insensitive\",\n    \"epsilon\": 0.01\n  }\n}\n</code></pre> <p>Available scoring modes:</p> Mode Behavior <code>exact</code> Case-insensitive string, exact numeric (default) <code>case_insensitive</code> Case-insensitive string comparison <code>contains</code> Substring match (either direction) <code>numeric_tolerance</code> Float comparison within epsilon <code>regex</code> Regular expression match"},{"location":"guide/tool-eval/#tool_choice-parameter","title":"tool_choice Parameter","text":"<p>Controls whether the model must call a tool:</p> Value Behavior <code>required</code> Model MUST call a tool (recommended for testing) <code>auto</code> Model can respond with text instead of calling a tool <code>none</code> Model cannot use tools (control test) <p>If a provider does not support <code>required</code>, the engine automatically falls back to <code>auto</code>.</p>"},{"location":"guide/tool-eval/#scoring","title":"Scoring","text":""},{"location":"guide/tool-eval/#tool-selection-score-00-or-10","title":"Tool Selection Score (0.0 or 1.0)","text":"<ul> <li>Did the model call the expected tool?</li> <li>Case-insensitive comparison</li> <li>Supports multiple acceptable tools</li> <li>If <code>expected_tool</code> is null, scores 1.0 only if the model also called nothing</li> </ul>"},{"location":"guide/tool-eval/#parameter-accuracy-00-10-or-null","title":"Parameter Accuracy (0.0 - 1.0 or null)","text":"<ul> <li>Per-key comparison: <code>correct_params / total_expected_params</code></li> <li>Strings are compared case-insensitively</li> <li>Numbers use float comparison</li> <li>If <code>expected_params</code> is null, returns null (not scored)</li> <li>If <code>expected_params</code> is <code>{}</code>, returns 1.0</li> </ul>"},{"location":"guide/tool-eval/#overall-score-weighted","title":"Overall Score (weighted)","text":"<pre><code>overall = 0.6 * tool_score + 0.4 * param_score\n</code></pre> <p>If parameters are not scored (null): <code>overall = tool_score</code></p>"},{"location":"guide/tool-eval/#creating-a-tool-suite","title":"Creating a Tool Suite","text":""},{"location":"guide/tool-eval/#via-the-ui","title":"Via the UI","text":"<ol> <li>Navigate to Tool Eval</li> <li>Click New Suite</li> <li>Enter a name and description</li> <li>Add tool definitions in JSON format</li> <li>Optionally set a system prompt for the suite</li> <li>Add test cases with prompts and expected results</li> </ol>"},{"location":"guide/tool-eval/#via-json-import","title":"Via JSON Import","text":"<p>Import a complete suite from a JSON file:</p> <pre><code>{\n  \"name\": \"Weather API Suite\",\n  \"description\": \"Tests weather tool calling\",\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current weather for a city\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"city\": { \"type\": \"string\", \"description\": \"City name\" },\n            \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n          },\n          \"required\": [\"city\"]\n        }\n      }\n    }\n  ],\n  \"test_cases\": [\n    {\n      \"prompt\": \"What's the weather in Paris?\",\n      \"expected_tool\": \"get_weather\",\n      \"expected_params\": { \"city\": \"Paris\" }\n    },\n    {\n      \"prompt\": \"Temperature in Tokyo in Fahrenheit\",\n      \"expected_tool\": \"get_weather\",\n      \"expected_params\": { \"city\": \"Tokyo\", \"units\": \"fahrenheit\" }\n    }\n  ]\n}\n</code></pre> <p>Import via API:</p> <pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @suite.json \\\n  http://localhost:8501/api/tool-eval/import\n</code></pre>"},{"location":"guide/tool-eval/#via-mcp-server","title":"Via MCP Server","text":"<p>Import tools from an MCP (Model Context Protocol) server:</p> <ol> <li>Click Import from MCP</li> <li>Enter the MCP server SSE endpoint URL</li> <li>The system connects, discovers tools, and creates a new suite</li> <li>Add test cases manually for the discovered tools</li> </ol>"},{"location":"guide/tool-eval/#system-prompt-per-suite","title":"System Prompt Per Suite","text":"<p>Each tool suite can have its own system prompt that is injected before every test case prompt. This is useful for:</p> <ul> <li>Guiding the model's tool calling behavior for the entire suite</li> <li>Setting context that applies to all test cases</li> <li>Testing how different base instructions affect tool use accuracy</li> </ul> <p>Set the system prompt via the UI when editing a suite, or via the API:</p> <pre><code>curl -X PATCH -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"system_prompt\": \"Always use the available tools. Never respond with plain text.\"}' \\\n  http://localhost:8501/api/tool-suites/{suite_id}\n</code></pre> <p>Additionally, when running an eval you can pass per-model system prompts via the <code>system_prompt</code> field (as a string for all models, or a dict keyed by <code>provider_key::model_id</code> for per-model prompts).</p>"},{"location":"guide/tool-eval/#running-an-eval","title":"Running an Eval","text":""},{"location":"guide/tool-eval/#execution-via-jobregistry","title":"Execution via JobRegistry","text":"<p>All eval runs execute through the JobRegistry, which provides:</p> <ul> <li>Background execution: The API returns a <code>job_id</code> immediately</li> <li>Per-user concurrency limits: Jobs queue if a user exceeds their limit</li> <li>Cancellation: Cancel running jobs via <code>job_id</code></li> <li>Persistence: Job state is saved to SQLite, surviving server restarts</li> <li>WebSocket progress: Real-time updates pushed to the frontend</li> </ul>"},{"location":"guide/tool-eval/#steps","title":"Steps","text":"<ol> <li>Select a tool suite</li> <li>Choose one or more models</li> <li>Set temperature (default: 0.0 for deterministic results)</li> <li>Set tool_choice (<code>required</code> recommended)</li> <li>Optionally link to an Experiment for tracking</li> <li>Click Run Eval</li> </ol> <p>The API returns <code>{\"job_id\": \"...\", \"status\": \"submitted\"}</code>. Progress and results are delivered via WebSocket.</p>"},{"location":"guide/tool-eval/#websocket-event-flow","title":"WebSocket Event Flow","text":"<pre><code>POST /api/tool-eval  --&gt;  { job_id, status: \"submitted\" }\n                              |\n                     WebSocket events:\n                              |\n    job_created  ---&gt;  tool_eval_init  ---&gt;  tool_eval_progress (per case)\n                                                     |\n                                              tool_eval_result (per case)\n                                                     |\n                                              tool_eval_summary (per model)\n                                                     |\n                                              tool_eval_complete\n</code></pre> <p>Key WebSocket event types:</p> Event Description <code>job_created</code> Job submitted to registry <code>tool_eval_init</code> Eval starting, includes target list and total cases <code>tool_eval_progress</code> Per-case progress update (current/total) <code>tool_eval_result</code> Individual test case result with scores <code>tool_eval_summary</code> Per-model aggregate scores <code>tool_eval_complete</code> Eval finished, includes <code>eval_id</code> and optional <code>delta</code> <code>job_progress</code> Generic progress update (percentage, detail text) <code>job_completed</code> Job finished successfully <code>job_failed</code> Job encountered an error <code>job_cancelled</code> Job was cancelled by user"},{"location":"guide/tool-eval/#cancellation","title":"Cancellation","text":"<pre><code>curl -X POST -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"job_id\": \"abc123\"}' \\\n  http://localhost:8501/api/tool-eval/cancel\n</code></pre>"},{"location":"guide/tool-eval/#multi-turn-evaluation","title":"Multi-Turn Evaluation","text":"<p>Multi-turn test cases simulate tool calling chains where the model must call prerequisite tools before reaching the final expected tool.</p> <p>Configure a multi-turn test case:</p> <pre><code>{\n  \"prompt\": \"Book a flight from NYC to London for next Friday\",\n  \"expected_tool\": \"book_flight\",\n  \"expected_params\": { \"origin\": \"NYC\", \"destination\": \"London\" },\n  \"multi_turn\": true,\n  \"max_rounds\": 5,\n  \"optimal_hops\": 2,\n  \"valid_prerequisites\": [\"search_flights\", \"get_availability\"],\n  \"mock_responses\": {\n    \"search_flights\": { \"flights\": [{\"id\": \"FL123\", \"price\": 450}] },\n    \"get_availability\": { \"available\": true, \"seats\": 12 }\n  }\n}\n</code></pre>"},{"location":"guide/tool-eval/#multi-turn-scoring","title":"Multi-Turn Scoring","text":"Metric Description Completion Did the model reach the expected final tool? (weighted tool + param score) Efficiency <code>optimal_hops / actual_hops</code> (capped at 1.0) Redundancy Penalty -10% per consecutive identical tool call Detour Penalty -10% per call not in valid_prerequisites Overall <code>completion * efficiency - redundancy - detour</code> (clamped 0-1)"},{"location":"guide/tool-eval/#experiment-tracking","title":"Experiment Tracking","text":"<p>Experiments group related eval runs, param tunes, prompt tunes, and judge reports together for tracking improvement over time.</p>"},{"location":"guide/tool-eval/#creating-an-experiment","title":"Creating an Experiment","text":"<ol> <li>Click New Experiment in the Tool Eval page</li> <li>Name the experiment and select a suite</li> <li>Optionally pin a baseline eval run</li> </ol>"},{"location":"guide/tool-eval/#experiment-features","title":"Experiment Features","text":"<ul> <li>Baseline: Pin an eval run as the reference point. All subsequent runs show a delta from baseline.</li> <li>Timeline: View all linked runs chronologically with scores and deltas.</li> <li>Best Config: The experiment automatically tracks the best-performing configuration across all linked runs (eval, param tune, prompt tune).</li> <li>Run Best: One-click to re-run the eval using the experiment's best config.</li> </ul>"},{"location":"guide/tool-eval/#linking-runs-to-experiments","title":"Linking Runs to Experiments","text":"<p>When running an eval, param tune, or prompt tune, select an experiment from the dropdown. The run is automatically linked:</p> <ul> <li>Eval runs compute <code>delta = avg_score - baseline_score</code></li> <li>Param tune runs auto-promote the best result as an eval run</li> <li>Prompt tune runs auto-promote the best prompt</li> </ul>"},{"location":"guide/tool-eval/#experiment-timeline-api","title":"Experiment Timeline API","text":"<pre><code>curl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8501/api/experiments/{experiment_id}/timeline\n</code></pre> <p>Returns entries ordered by timestamp, each with type (<code>eval</code>, <code>param_tune</code>, <code>prompt_tune</code>, <code>judge</code>), score, delta from baseline, and configuration summary.</p>"},{"location":"guide/tool-eval/#execution-flow","title":"Execution Flow","text":"<pre><code>POST /api/tool-eval (with suite_id, models, temperature, tool_choice)\n        |\n        v\nJobRegistry.submit(job_type=\"tool_eval\")\n        |\n        v\ntool_eval_handler() in job_handlers.py\n        |\n        v\nFor each provider group (in parallel):\n  For each model (sequentially within provider):\n    For each test case:\n        |\n        v\n    litellm.acompletion() (non-streaming)\n        |\n        v\n    response.choices[0].message.tool_calls\n        |\n        v\n    score_tool_selection() + score_params()\n        |\n        v\n    compute_overall_score()\n        |\n        v\n    WebSocket: tool_eval_result\n        |\n        v\nPer-model summaries computed\n        |\n        v\nResults saved to DB (tool_eval_runs table)\n        |\n        v\nWebSocket: tool_eval_complete\n</code></pre> <p>Provider groups execute in parallel via <code>asyncio.create_task()</code>. Models within a provider run sequentially to avoid self-contention on rate-limited APIs.</p>"},{"location":"guide/tool-eval/#api-reference","title":"API Reference","text":""},{"location":"guide/tool-eval/#tool-suite-endpoints","title":"Tool Suite Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/api/tool-suites</code> List user's tool suites <code>POST</code> <code>/api/tool-suites</code> Create a new suite <code>GET</code> <code>/api/tool-suites/{id}</code> Get suite with tools and test cases <code>PUT</code> <code>/api/tool-suites/{id}</code> Update suite (name, description, tools) <code>PATCH</code> <code>/api/tool-suites/{id}</code> Patch suite fields (e.g., system_prompt) <code>DELETE</code> <code>/api/tool-suites/{id}</code> Delete suite and its test cases <code>GET</code> <code>/api/tool-suites/{id}/export</code> Export suite as JSON <code>POST</code> <code>/api/tool-eval/import</code> Import suite from JSON <code>GET</code> <code>/api/tool-eval/import/example</code> Download example import template"},{"location":"guide/tool-eval/#test-case-endpoints","title":"Test Case Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/api/tool-suites/{id}/cases</code> List test cases for a suite <code>POST</code> <code>/api/tool-suites/{id}/cases</code> Create test case(s) (single or bulk) <code>PUT</code> <code>/api/tool-suites/{id}/cases/{cid}</code> Update a test case <code>DELETE</code> <code>/api/tool-suites/{id}/cases/{cid}</code> Delete a test case"},{"location":"guide/tool-eval/#eval-endpoints","title":"Eval Endpoints","text":"Method Endpoint Description <code>POST</code> <code>/api/tool-eval</code> Run eval (returns job_id) <code>POST</code> <code>/api/tool-eval/cancel</code> Cancel running eval <code>GET</code> <code>/api/tool-eval/history</code> List eval runs <code>GET</code> <code>/api/tool-eval/history/{id}</code> Get eval run details <code>DELETE</code> <code>/api/tool-eval/history/{id}</code> Delete eval run"},{"location":"guide/tool-eval/#experiment-endpoints","title":"Experiment Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/api/experiments</code> List experiments <code>POST</code> <code>/api/experiments</code> Create experiment <code>GET</code> <code>/api/experiments/{id}</code> Get experiment details <code>PUT</code> <code>/api/experiments/{id}</code> Update experiment <code>DELETE</code> <code>/api/experiments/{id}</code> Delete experiment <code>PUT</code> <code>/api/experiments/{id}/baseline</code> Pin baseline eval run <code>GET</code> <code>/api/experiments/{id}/timeline</code> Get experiment timeline <code>POST</code> <code>/api/experiments/{id}/run-best</code> Run eval with best config"},{"location":"guide/tool-eval/#common-issues","title":"Common Issues","text":"<p>100% Tool Selection, 0% Parameter Accuracy: The model picks the right tool but hallucinates parameter values. Ensure your <code>expected_params</code> match realistic model output patterns.</p> <p>0% everything with <code>auto</code>: The model responds with text instead of calling tools. Switch to <code>required</code> for tool_choice.</p> <p>MCP suites failing: Auto-generated prompts from MCP tool descriptions may be too vague. Use <code>required</code> for tool_choice and write more specific prompts.</p> <p>Local LLMs returning JSON in function name: The engine automatically detects when a local LLM stuffs the entire tool call JSON into the function name field and extracts the actual tool name and parameters.</p> <p>Reconnection during eval: If the WebSocket disconnects and reconnects, the frontend restores the active job from session storage. Completed results can be fetched from the DB via the <code>result_ref</code> stored on the job record.</p>"},{"location":"hosting/deployment/","title":"Deployment","text":"<p>LLM Benchmark Studio uses a CI/CD pipeline with GitHub Actions for automated builds and deployments.</p>"},{"location":"hosting/deployment/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>The pipeline is defined in <code>.github/workflows/ci.yml</code> and consists of four jobs:</p>"},{"location":"hosting/deployment/#1-test","title":"1. Test","text":"<p>Triggers on:</p> <ul> <li>Push to <code>main</code> branch</li> <li>Push of version tags (<code>v*.*.*</code>)</li> <li>Pull requests to <code>main</code></li> </ul> <p>Steps:</p> <ol> <li>Checkout code, install <code>uv</code> and Python 3.12</li> <li>Install dependencies with <code>uv sync</code></li> <li>Level 1 tests: Run all tests except E2E smoke (API contracts, no secrets needed)</li> <li>Level 2 tests (main branch only): Run E2E smoke tests requiring <code>ZAI_API_KEY</code></li> </ol>"},{"location":"hosting/deployment/#2-build","title":"2. Build","text":"<p>Runs after tests pass.</p> <p>Steps:</p> <ol> <li>Set up Docker Buildx</li> <li>Log in to GitHub Container Registry (GHCR)</li> <li>Determine app version from git ref (tag or <code>main-&lt;sha&gt;</code>)</li> <li>Build Docker image with layer caching (two-stage: Node.js frontend + Python backend)</li> <li>Push to GHCR (except for PRs)</li> <li>Run smoke test: start the container and verify <code>/healthz</code> responds within 60 seconds</li> </ol>"},{"location":"hosting/deployment/#3-deploy-to-staging","title":"3. Deploy to Staging","text":"<p>Triggers on push to <code>main</code> branch (after successful build).</p> <ul> <li>Image tag: <code>:main</code></li> <li>Deploys via Portainer REST API</li> </ul>"},{"location":"hosting/deployment/#4-deploy-to-production","title":"4. Deploy to Production","text":"<p>Triggers on version tags (<code>v*.*.*</code>) (after successful build).</p> <ul> <li>Image tag: <code>:&lt;major&gt;.&lt;minor&gt;</code></li> <li>Deploys via Portainer REST API</li> </ul>"},{"location":"hosting/deployment/#container-registry","title":"Container Registry","text":"<p>Images are published to GitHub Container Registry:</p> <pre><code>ghcr.io/maheidem/llm-benchmark-studio\n</code></pre> <p>Tags:</p> Tag Pattern Example Trigger <code>:main</code> <code>:main</code> Push to main branch <code>:sha-&lt;hash&gt;</code> <code>:sha-abc1234</code> Every push <code>:&lt;version&gt;</code> <code>:1.2.0</code> Version tag <code>v1.2.0</code> <code>:&lt;major&gt;.&lt;minor&gt;</code> <code>:1.2</code> Version tag <code>v1.2.x</code> <code>:latest</code> <code>:latest</code> Push to main branch"},{"location":"hosting/deployment/#deployment-method","title":"Deployment Method","text":"<p>The application deploys to a Portainer-managed Docker environment:</p> <ol> <li>Pull: Latest image from GHCR</li> <li>Stop: Existing stack</li> <li>Remove: Old containers</li> <li>Start: Stack (recreates containers with new image)</li> </ol>"},{"location":"hosting/deployment/#required-secrets","title":"Required Secrets","text":"<p>Configure these in your GitHub repository settings:</p> Secret Description <code>PORTAINER_URL</code> Portainer API base URL <code>PORTAINER_API_KEY</code> Portainer API authentication key <code>PORTAINER_ENDPOINT_ID</code> Docker endpoint ID in Portainer <code>PORTAINER_STACK_ID_STAGING</code> Staging stack ID <code>PORTAINER_STACK_ID_PROD</code> Production stack ID <code>ZAI_API_KEY</code> API key for Level 2 E2E smoke tests"},{"location":"hosting/deployment/#staging-vs-production","title":"Staging vs Production","text":"Setting Staging Production Port 8502 8501 Image tag <code>:main</code> <code>:&lt;major&gt;.&lt;minor&gt;</code> Auto-deploy On push to main On version tag"},{"location":"hosting/deployment/#manual-deployment","title":"Manual Deployment","text":"<p>To deploy manually without the CI/CD pipeline:</p> <pre><code># Build the image\ndocker build --build-arg APP_VERSION=1.2.0 -t llm-benchmark-studio:1.2.0 .\n\n# Run the container\ndocker run -d \\\n  -p 8501:8501 \\\n  -v ./data:/app/data \\\n  -v ./.env:/app/.env:ro \\\n  -e JWT_SECRET=your-secret \\\n  -e COOKIE_SECURE=true \\\n  --name benchmark-studio \\\n  --restart unless-stopped \\\n  llm-benchmark-studio:1.2.0\n</code></pre>"},{"location":"hosting/deployment/#reverse-proxy","title":"Reverse Proxy","text":"<p>LLM Benchmark Studio uses WebSocket connections for real-time job status updates. Your reverse proxy must be configured to handle both standard HTTP requests and WebSocket upgrades.</p>"},{"location":"hosting/deployment/#nginx","title":"Nginx","text":"<pre><code>server {\n    listen 443 ssl;\n    server_name benchmark.example.com;\n\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/key.pem;\n\n    location / {\n        proxy_pass http://localhost:8501;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # WebSocket support\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n\n        # Timeouts: WebSocket connections are long-lived\n        proxy_read_timeout 3600s;\n        proxy_send_timeout 3600s;\n    }\n}\n</code></pre> <p>The key WebSocket directives:</p> <ul> <li><code>proxy_http_version 1.1</code> -- Required for WebSocket upgrade</li> <li><code>proxy_set_header Upgrade $http_upgrade</code> -- Passes the Upgrade header from the client</li> <li><code>proxy_set_header Connection \"upgrade\"</code> -- Signals the upstream to switch protocols</li> <li><code>proxy_read_timeout 3600s</code> -- Keeps the WebSocket connection alive for long-running jobs (the client sends pings every 60 seconds to maintain the connection)</li> </ul>"},{"location":"hosting/deployment/#caddy","title":"Caddy","text":"<p>Caddy handles WebSocket proxying automatically with no extra configuration:</p> <pre><code>benchmark.example.com {\n    reverse_proxy localhost:8501\n}\n</code></pre>"},{"location":"hosting/deployment/#traefik","title":"Traefik","text":"<p>For Traefik, WebSocket support is enabled by default when using the standard HTTP router. No special middleware is needed.</p>"},{"location":"hosting/deployment/#cloudflare","title":"Cloudflare","text":"<p>If using Cloudflare as a proxy:</p> <ul> <li>WebSocket connections are supported on all plans</li> <li>Set the WebSockets toggle to \"On\" in the Cloudflare dashboard (Network tab)</li> <li>Cloudflare imposes a 100-second idle timeout; the application's 60-second ping interval keeps connections alive within this limit</li> <li>For the <code>/ws</code> endpoint, ensure the Cloudflare proxy is enabled (orange cloud)</li> </ul>"},{"location":"hosting/deployment/#health-check","title":"Health Check","text":"<p>The application exposes a health endpoint used by Docker, CI smoke tests, and monitoring:</p> <pre><code>curl http://localhost:8501/healthz\n# {\"status\": \"ok\", \"version\": \"1.2.0\"}\n</code></pre>"},{"location":"hosting/deployment/#docker-health-check","title":"Docker Health Check","text":"<p>The Docker image includes a built-in health check that runs every 30 seconds against <code>/healthz</code>:</p> <pre><code>HEALTHCHECK --interval=30s --timeout=5s --retries=3 \\\n  CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8501/healthz')\" || exit 1\n</code></pre> <p>The Docker Compose file includes an identical health check definition. After 3 consecutive failures (90 seconds), Docker marks the container as unhealthy.</p>"},{"location":"hosting/deployment/#application-metrics","title":"Application Metrics","text":"<p>Use the admin system endpoint for monitoring:</p> <pre><code>curl -H \"Authorization: Bearer $ADMIN_TOKEN\" \\\n  http://localhost:8501/api/admin/system\n</code></pre> <p>Returns database size, results count, active benchmarks, and uptime.</p>"},{"location":"hosting/deployment/#websocket-endpoint","title":"WebSocket Endpoint","text":"<p>The WebSocket endpoint is at <code>/ws</code> and requires a JWT access token passed as a query parameter:</p> <pre><code>ws://localhost:8501/ws?token=&lt;jwt_access_token&gt;\n</code></pre> <p>Or over TLS:</p> <pre><code>wss://benchmark.example.com/ws?token=&lt;jwt_access_token&gt;\n</code></pre> <p>Key behaviors:</p> <ul> <li>Supports up to 5 concurrent connections per user (multi-tab)</li> <li>On connect, sends a <code>sync</code> message with active and recent jobs</li> <li>Clients should send a <code>{\"type\": \"ping\"}</code> every 60 seconds to keep the connection alive</li> <li>The server closes connections that are idle for 90 seconds (no messages received)</li> <li>Clients can send <code>{\"type\": \"cancel\", \"job_id\": \"...\"}</code> to cancel a running job</li> </ul>"},{"location":"hosting/docker/","title":"Docker Setup","text":"<p>LLM Benchmark Studio provides a Docker image for production deployments. The image uses a two-stage build: the Vue 3 frontend is compiled with Node.js, and the resulting static assets are served by the Python backend.</p>"},{"location":"hosting/docker/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/maheidem/llm-benchmark-studio.git\ncd llm-benchmark-studio\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your API keys and JWT_SECRET\n\n# Start with Docker Compose\ndocker compose up -d\n</code></pre> <p>The application is available at <code>http://localhost:8501</code>.</p>"},{"location":"hosting/docker/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<p>The default <code>docker-compose.yml</code>:</p> <pre><code>services:\n  benchmark:\n    build: .\n    ports:\n      - \"8501:8501\"\n    volumes:\n      - ./data:/app/data        # Persist SQLite DB and Fernet key\n      - ./.env:/app/.env:ro     # API keys (read-only)\n    environment:\n      - JWT_SECRET=${JWT_SECRET:-change-me-in-production}\n      - BENCHMARK_RATE_LIMIT=${BENCHMARK_RATE_LIMIT:-2000}\n      - COOKIE_SECURE=${COOKIE_SECURE:-false}\n      - LOG_LEVEL=${LOG_LEVEL:-warning}\n      - ADMIN_EMAIL=${ADMIN_EMAIL:-}\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"python\", \"-c\", \"import urllib.request; urllib.request.urlopen('http://localhost:8501/healthz')\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n</code></pre>"},{"location":"hosting/docker/#volumes","title":"Volumes","text":"Volume Container Path Description <code>./data</code> <code>/app/data</code> SQLite database and Fernet encryption key <code>./.env</code> <code>/app/.env</code> Environment variables (read-only mount) <p>Back Up the Data Volume</p> <p>The <code>data/</code> directory contains:</p> <ul> <li><code>benchmark_studio.db</code> -- All user data, benchmark history, configurations</li> <li><code>.fernet_key</code> -- Encryption key for stored API keys</li> </ul> <p>Both must be backed up. If the Fernet key is lost, user API keys cannot be recovered.</p>"},{"location":"hosting/docker/#dockerfile","title":"Dockerfile","text":"<p>The Dockerfile uses a two-stage build to produce a single production image:</p> <pre><code># Stage 1: Build Vue 3 frontend\nFROM node:22-alpine AS frontend-build\nWORKDIR /build\nCOPY frontend/package.json frontend/package-lock.json* ./\nRUN npm ci\nCOPY frontend/ ./\nRUN npm run build\n\n# Stage 2: Python backend\nFROM python:3.13-slim\n\n# Install uv package manager\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv\n\nARG APP_VERSION=dev\nENV APP_VERSION=${APP_VERSION}\n\nWORKDIR /app\n\n# Install dependencies (cached layer)\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\n# Copy application code\nCOPY app.py benchmark.py auth.py db.py keyvault.py \\\n     provider_params.py job_registry.py job_handlers.py \\\n     schemas.py ws_manager.py config.yaml \\\n     migrate_to_multiuser.py ./\nCOPY routers/ routers/\n\n# Copy built frontend assets from stage 1\nCOPY --from=frontend-build /static/ static/\n\n# Create data directory\nRUN mkdir -p data\n\n# Non-root user\nRUN useradd -m -s /bin/bash bench &amp;&amp; chown -R bench:bench /app\nUSER bench\n\nEXPOSE 8501\n\nHEALTHCHECK --interval=30s --timeout=5s --retries=3 \\\n  CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8501/healthz')\" || exit 1\n\nCMD [\"uv\", \"run\", \"python\", \"app.py\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"hosting/docker/#build-stages","title":"Build Stages","text":"<p>Stage 1 -- Frontend (Node.js 22 Alpine):</p> <ol> <li>Copies <code>frontend/package.json</code> and <code>frontend/package-lock.json</code></li> <li>Runs <code>npm ci</code> for deterministic dependency installation</li> <li>Copies the full <code>frontend/</code> directory</li> <li>Runs <code>npm run build</code> (Vite builds the Vue 3 SPA to <code>/static/</code>)</li> </ol> <p>Stage 2 -- Backend (Python 3.13 Slim):</p> <ol> <li>Installs the <code>uv</code> package manager</li> <li>Copies <code>pyproject.toml</code> and <code>uv.lock</code>, then runs <code>uv sync --frozen --no-dev</code> (cached layer)</li> <li>Copies all Python application files and the <code>routers/</code> directory</li> <li>Copies the built frontend assets from stage 1 into <code>static/</code></li> <li>Creates the <code>data/</code> directory, sets up a non-root user, and configures the health check</li> </ol>"},{"location":"hosting/docker/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Two-stage build keeps the final image small (no Node.js runtime in production)</li> <li>Uses <code>uv</code> for fast, deterministic Python dependency installation</li> <li>Runs as non-root user <code>bench</code></li> <li>Built-in health check on <code>/healthz</code></li> <li>Build arg <code>APP_VERSION</code> sets the reported application version</li> </ul>"},{"location":"hosting/docker/#building-the-image","title":"Building the Image","text":"<pre><code># Build locally\ndocker build -t llm-benchmark-studio .\n\n# Build with version tag\ndocker build --build-arg APP_VERSION=1.2.0 -t llm-benchmark-studio:1.2.0 .\n</code></pre>"},{"location":"hosting/docker/#environment-variables","title":"Environment Variables","text":"<p>Pass these as environment variables or in the <code>.env</code> file:</p> Variable Required Default Description <code>JWT_SECRET</code> Yes Auto-generated JWT signing key <code>ADMIN_EMAIL</code> No (none) Auto-promote to admin <code>ADMIN_PASSWORD</code> No (none) Auto-create admin account <code>FERNET_KEY</code> No Auto-generated Encryption master key <code>BENCHMARK_RATE_LIMIT</code> No <code>2000</code> Rate limit per hour <code>COOKIE_SECURE</code> No <code>false</code> <code>true</code> for HTTPS <code>CORS_ORIGINS</code> No (none) Allowed CORS origins <code>LOG_LEVEL</code> No <code>warning</code> Log verbosity <p>Plus any LLM provider API keys (<code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, etc.).</p>"},{"location":"hosting/docker/#connecting-to-local-llm-servers","title":"Connecting to Local LLM Servers","text":"<p>To benchmark local models (LM Studio, Ollama, vLLM) from within the Docker container:</p> <pre><code># In the user's config or config.yaml\nproviders:\n  lm_studio:\n    display_name: LM Studio\n    api_base: http://host.docker.internal:1234/v1\n    api_key: not-needed\n    model_id_prefix: lm_studio\n    models:\n      - id: lm_studio/my-model\n        display_name: My Local Model\n</code></pre> <p>Use <code>host.docker.internal</code> to access services running on the Docker host machine.</p>"},{"location":"hosting/docker/#production-recommendations","title":"Production Recommendations","text":"<ul> <li>Set a strong, persistent <code>JWT_SECRET</code></li> <li>Set <code>FERNET_KEY</code> explicitly (do not rely on auto-generation)</li> <li>Use <code>COOKIE_SECURE=true</code> behind HTTPS</li> <li>Mount <code>data/</code> as a persistent volume</li> <li>Set up regular backups of the SQLite database</li> <li>Use a reverse proxy (nginx, Caddy, Traefik) for TLS termination and WebSocket proxying</li> <li>Set <code>CORS_ORIGINS</code> if the frontend is served from a different domain</li> </ul>"}]}